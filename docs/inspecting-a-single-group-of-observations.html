<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Inspecting a single group of observations | Bayesian multilevel models in R: A conceptual and practical introduction for linguists</title>
  <meta name="description" content="Bayesian Models for Linguists" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Inspecting a single group of observations | Bayesian multilevel models in R: A conceptual and practical introduction for linguists" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://santiagobarreda.com" />
  
  <meta property="og:description" content="Bayesian Models for Linguists" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Inspecting a single group of observations | Bayesian multilevel models in R: A conceptual and practical introduction for linguists" />
  
  <meta name="twitter:description" content="Bayesian Models for Linguists" />
  

<meta name="author" content="Santiago Bareda" />


<meta name="date" content="2021-07-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Models for Linguists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html"><i class="fa fa-check"></i><b>1</b> Inspecting a single group of observations</a>
<ul>
<li class="chapter" data-level="1.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#data-and-research-questions"><i class="fa fa-check"></i><b>1.1</b> Data and research questions</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#inspecting-the-central-location-and-spread-of-values"><i class="fa fa-check"></i><b>1.1.1</b> Inspecting the central location and spread of values</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#probability-distributions"><i class="fa fa-check"></i><b>1.2</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#the-normal-distribution"><i class="fa fa-check"></i><b>1.2.1</b> The normal distribution</a></li>
<li class="chapter" data-level="1.2.2" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#referring-to-the-normal-distribution-to-make-inferences"><i class="fa fa-check"></i><b>1.2.2</b> Referring to the normal distribution to make inferences</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#probabilities-of-events-and-likelihoods-of-parameters"><i class="fa fa-check"></i><b>1.3</b> Probabilities of events and likelihoods of parameters</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#making-inferences-using-likelihoods"><i class="fa fa-check"></i><b>1.3.1</b> Making inferences using likelihoods</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#bayesian-models"><i class="fa fa-check"></i><b>1.4</b> Bayesian models</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#what-are-regression-models"><i class="fa fa-check"></i><b>1.4.1</b> What are regression models?</a></li>
<li class="chapter" data-level="1.4.2" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#whats-bayesian-about-these-models"><i class="fa fa-check"></i><b>1.4.2</b> What’s ‘Bayesian’ about these models?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#posterior-distributions"><i class="fa fa-check"></i><b>1.5</b> Posterior distributions</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#sampling-from-the-posterior"><i class="fa fa-check"></i><b>1.5.1</b> Sampling from the posterior</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><i class="fa fa-check"></i><b>2</b> Inspecting a ‘single group’ of observations using a Bayesian multilevel model</a>
<ul>
<li class="chapter" data-level="2.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#data-and-research-questions-1"><i class="fa fa-check"></i><b>2.1</b> Data and research questions</a></li>
<li class="chapter" data-level="2.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#estimating-a-single-mean-with-the-brms-package"><i class="fa fa-check"></i><b>2.2</b> Estimating a single mean with the <code>brms</code> package</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#description-of-the-model"><i class="fa fa-check"></i><b>2.2.1</b> Description of the model</a></li>
<li class="chapter" data-level="2.2.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#the-model-formula"><i class="fa fa-check"></i><b>2.2.2</b> The model formula</a></li>
<li class="chapter" data-level="2.2.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#fitting-the-model-calling-the-brm-function"><i class="fa fa-check"></i><b>2.2.3</b> Fitting the model: Calling the <code>brm</code> function</a></li>
<li class="chapter" data-level="2.2.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#interpreting-the-model-the-print-statement"><i class="fa fa-check"></i><b>2.2.4</b> Interpreting the model: the print statement</a></li>
<li class="chapter" data-level="2.2.5" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#seeing-the-samples"><i class="fa fa-check"></i><b>2.2.5</b> Seeing the samples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#repeated-measures-data"><i class="fa fa-check"></i><b>2.3</b> Repeated measures data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#multilevel-models"><i class="fa fa-check"></i><b>2.3.1</b> Multilevel models</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#estimating-a-multilevel-model-with-brms"><i class="fa fa-check"></i><b>2.4</b> Estimating a multilevel model with <code>brms</code></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#description-of-the-model-1"><i class="fa fa-check"></i><b>2.4.1</b> Description of the model</a></li>
<li class="chapter" data-level="2.4.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#fitting-the-model"><i class="fa fa-check"></i><b>2.4.2</b> Fitting the model</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#checking-model-convergence"><i class="fa fa-check"></i><b>2.5</b> Checking model convergence</a></li>
<li class="chapter" data-level="2.6" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#specifying-prior-probabilities"><i class="fa fa-check"></i><b>2.6</b> Specifying prior probabilities</a></li>
<li class="chapter" data-level="2.7" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#answering-our-research-questions"><i class="fa fa-check"></i><b>2.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="2.8" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#simulating-data-using-our-model-parameters"><i class="fa fa-check"></i><b>2.8</b> Simulating data using our model parameters</a></li>
<li class="chapter" data-level="2.9" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#frequentist-corner"><i class="fa fa-check"></i><b>2.9</b> Frequentist corner</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#bayesian-multilevel-modesl-vs.-lmer"><i class="fa fa-check"></i><b>2.9.1</b> Bayesian multilevel modesl vs. lmer</a></li>
<li class="chapter" data-level="2.9.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#bayesian-multilevel-modesl-vs.-the-one-sample-t-test"><i class="fa fa-check"></i><b>2.9.2</b> Bayesian multilevel modesl vs. the one-sample t-test</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#exercises-1"><i class="fa fa-check"></i><b>2.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html"><i class="fa fa-check"></i><b>3</b> Comparing two groups of observations</a>
<ul>
<li class="chapter" data-level="3.1" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#data-and-research-questions-2"><i class="fa fa-check"></i><b>3.1</b> Data and research questions</a></li>
<li class="chapter" data-level="3.2" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#estimating-the-difference-between-two-means-with-brms"><i class="fa fa-check"></i><b>3.2</b> Estimating the difference between two means with ‘brms’</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#fitting-the-model-1"><i class="fa fa-check"></i><b>3.2.1</b> Fitting the model</a></li>
<li class="chapter" data-level="3.2.2" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#interpreting-the-model"><i class="fa fa-check"></i><b>3.2.2</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#contrasts"><i class="fa fa-check"></i><b>3.3</b> Contrasts</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#treatment-coding"><i class="fa fa-check"></i><b>3.3.1</b> Treatment coding</a></li>
<li class="chapter" data-level="3.3.2" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#sum-coding"><i class="fa fa-check"></i><b>3.3.2</b> Sum coding</a></li>
<li class="chapter" data-level="3.3.3" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#comparison-of-sum-and-treatment-coding"><i class="fa fa-check"></i><b>3.3.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#refitting-the-model-with-sum-coding"><i class="fa fa-check"></i><b>3.4</b> Refitting the model with sum coding</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#fitting-the-model-2"><i class="fa fa-check"></i><b>3.4.1</b> Fitting the model</a></li>
<li class="chapter" data-level="3.4.2" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#description-of-the-model-2"><i class="fa fa-check"></i><b>3.4.2</b> Description of the model</a></li>
<li class="chapter" data-level="3.4.3" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#interpreting-the-model-1"><i class="fa fa-check"></i><b>3.4.3</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#random-effects"><i class="fa fa-check"></i><b>3.5</b> ‘Random’ Effects</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#random-effects-priors-and-pooling"><i class="fa fa-check"></i><b>3.5.1</b> Random effects, priors and pooling</a></li>
<li class="chapter" data-level="3.5.2" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#inspecting-the-random-effects"><i class="fa fa-check"></i><b>3.5.2</b> Inspecting the random effects</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#but-what-does-it-all-mean"><i class="fa fa-check"></i><b>3.6</b> But what does it all mean?</a></li>
<li class="chapter" data-level="3.7" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#simulating-the-two-group-model"><i class="fa fa-check"></i><b>3.7</b> Simulating the two-group model</a></li>
<li class="chapter" data-level="3.8" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#frequentist-corner-1"><i class="fa fa-check"></i><b>3.8</b> Frequentist corner</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#bayesian-multilevel-modesl-vs.-lmer-1"><i class="fa fa-check"></i><b>3.8.1</b> Bayesian multilevel modesl vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#exercises-2"><i class="fa fa-check"></i><b>3.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html"><i class="fa fa-check"></i><b>4</b> Comparing many groups</a>
<ul>
<li class="chapter" data-level="4.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#data-and-research-questions-3"><i class="fa fa-check"></i><b>4.1</b> Data and research questions</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#factors-as-batches-effects"><i class="fa fa-check"></i><b>4.1.1</b> Factors as ‘batches’ effects</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#comparing-four-or-any-number-of-groups"><i class="fa fa-check"></i><b>4.2</b> Comparing four (or any number of) groups</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#the-model"><i class="fa fa-check"></i><b>4.2.1</b> The model</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#investigating-many-groups-using-predictors-analysis-of-variance"><i class="fa fa-check"></i><b>4.3</b> Investigating many groups using predictors: Analysis of Variance</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#description-of-the-model-3"><i class="fa fa-check"></i><b>4.3.1</b> Description of the model</a></li>
<li class="chapter" data-level="4.3.2" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#fitting-the-model-and-interpreting-the-results"><i class="fa fa-check"></i><b>4.3.2</b> Fitting the model and interpreting the results</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#investigating-model-fit"><i class="fa fa-check"></i><b>4.4</b> Investigating model fit</a></li>
<li class="chapter" data-level="4.5" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#interactions-and-interaction-plots"><i class="fa fa-check"></i><b>4.5</b> Interactions and interaction plots</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#interactions-in-our-f0-data"><i class="fa fa-check"></i><b>4.5.1</b> Interactions in our f0 data</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#investigating-interactions-with-a-model"><i class="fa fa-check"></i><b>4.6</b> Investigating interactions with a model</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#fitting-the-model-and-interpreting-the-results-1"><i class="fa fa-check"></i><b>4.6.1</b> Fitting the model and interpreting the results</a></li>
<li class="chapter" data-level="4.6.2" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#assessing-model-fit"><i class="fa fa-check"></i><b>4.6.2</b> Assessing model fit</a></li>
<li class="chapter" data-level="4.6.3" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#making-plots"><i class="fa fa-check"></i><b>4.6.3</b> Making plots</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#frequentist-corner-2"><i class="fa fa-check"></i><b>4.7</b> Frequentist corner</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#bayesian-multilevel-modesl-vs.-lmer-2"><i class="fa fa-check"></i><b>4.7.1</b> Bayesian multilevel modesl vs. lmer</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#exercises-3"><i class="fa fa-check"></i><b>4.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html"><i class="fa fa-check"></i><b>5</b> Including continuous predictors in our model</a>
<ul>
<li class="chapter" data-level="5.1" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#data-and-research-questions-4"><i class="fa fa-check"></i><b>5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="5.2" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#continuous-predictors-modeling-variation-along-lines"><i class="fa fa-check"></i><b>5.2</b> Continuous predictors: modeling variation along lines</a></li>
<li class="chapter" data-level="5.3" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#models-with-a-single-slope-and-intercept"><i class="fa fa-check"></i><b>5.3</b> Models with a single slope and intercept</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#description-of-the-model-4"><i class="fa fa-check"></i><b>5.3.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.3.2" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#fitting-the-model-3"><i class="fa fa-check"></i><b>5.3.2</b> Fitting the model</a></li>
<li class="chapter" data-level="5.3.3" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#interpreting-the-model-2"><i class="fa fa-check"></i><b>5.3.3</b> Interpreting the model</a></li>
<li class="chapter" data-level="5.3.4" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#centering-predictors"><i class="fa fa-check"></i><b>5.3.4</b> Centering predictors</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#interactions-in-our-line-parameters"><i class="fa fa-check"></i><b>5.4</b> Interactions in our line parameters</a></li>
<li class="chapter" data-level="5.5" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#models-with-group-dependent-intercepts-but-shared-slopes"><i class="fa fa-check"></i><b>5.5</b> Models with group-dependent intercepts, but shared slopes</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#description-of-the-model-5"><i class="fa fa-check"></i><b>5.5.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.5.2" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#fitting-the-model-4"><i class="fa fa-check"></i><b>5.5.2</b> Fitting the model</a></li>
<li class="chapter" data-level="5.5.3" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#the-effect-of-including-a-slope"><i class="fa fa-check"></i><b>5.5.3</b> The effect of including a slope</a></li>
<li class="chapter" data-level="5.5.4" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#interpreting-group-effects-in-the-presence-of-a-continuous-predictor"><i class="fa fa-check"></i><b>5.5.4</b> Interpreting group effects in the presence of a continuous predictor</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#models-with-group-dependent-slopes-and-intercepts"><i class="fa fa-check"></i><b>5.6</b> Models with group-dependent slopes and intercepts</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#description-of-the-model-6"><i class="fa fa-check"></i><b>5.6.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.6.2" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#fitting-the-model-5"><i class="fa fa-check"></i><b>5.6.2</b> Fitting the model</a></li>
<li class="chapter" data-level="5.6.3" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#interpreting-the-model-3"><i class="fa fa-check"></i><b>5.6.3</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#exercises-4"><i class="fa fa-check"></i><b>5.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="random-slopes.html"><a href="random-slopes.html"><i class="fa fa-check"></i><b>6</b> Random slopes</a>
<ul>
<li class="chapter" data-level="6.1" data-path="random-slopes.html"><a href="random-slopes.html#data-and-research-questions-5"><i class="fa fa-check"></i><b>6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="6.2" data-path="random-slopes.html"><a href="random-slopes.html#repeated-measures-and-speaker-dependent-parameter-values"><i class="fa fa-check"></i><b>6.2</b> Repeated measures and speaker-dependent parameter values</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="random-slopes.html"><a href="random-slopes.html#description-of-the-model-7"><i class="fa fa-check"></i><b>6.2.1</b> Description of the model</a></li>
<li class="chapter" data-level="6.2.2" data-path="random-slopes.html"><a href="random-slopes.html#fitting-the-model-6"><i class="fa fa-check"></i><b>6.2.2</b> Fitting the model</a></li>
<li class="chapter" data-level="6.2.3" data-path="random-slopes.html"><a href="random-slopes.html#interpreting-the-model-4"><i class="fa fa-check"></i><b>6.2.3</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="random-slopes.html"><a href="random-slopes.html#random-effects-and-the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.3</b> Random effects and the multivariate normal distribution</a></li>
<li class="chapter" data-level="6.4" data-path="random-slopes.html"><a href="random-slopes.html#random-slopes-1"><i class="fa fa-check"></i><b>6.4</b> Random slopes</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="random-slopes.html"><a href="random-slopes.html#description-of-the-model-8"><i class="fa fa-check"></i><b>6.4.1</b> Description of the model</a></li>
<li class="chapter" data-level="6.4.2" data-path="random-slopes.html"><a href="random-slopes.html#fitting-the-model-7"><i class="fa fa-check"></i><b>6.4.2</b> Fitting the model</a></li>
<li class="chapter" data-level="6.4.3" data-path="random-slopes.html"><a href="random-slopes.html#interpreting-the-model-5"><i class="fa fa-check"></i><b>6.4.3</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="random-slopes.html"><a href="random-slopes.html#more-predictors-and-more-random-slopes"><i class="fa fa-check"></i><b>6.5</b> More predictors and more random slopes</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="random-slopes.html"><a href="random-slopes.html#adding-another-random-slope"><i class="fa fa-check"></i><b>6.5.1</b> Adding another random slope</a></li>
<li class="chapter" data-level="6.5.2" data-path="random-slopes.html"><a href="random-slopes.html#adding-random-factors"><i class="fa fa-check"></i><b>6.5.2</b> Adding random factors</a></li>
<li class="chapter" data-level="6.5.3" data-path="random-slopes.html"><a href="random-slopes.html#the-independence-of-continuous-predictors"><i class="fa fa-check"></i><b>6.5.3</b> The independence of continuous predictors</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="random-slopes.html"><a href="random-slopes.html#answering-our-research-questions-1"><i class="fa fa-check"></i><b>6.6</b> Answering our research questions</a></li>
<li class="chapter" data-level="6.7" data-path="random-slopes.html"><a href="random-slopes.html#lmer-corner"><i class="fa fa-check"></i><b>6.7</b> Lmer corner</a></li>
<li class="chapter" data-level="6.8" data-path="random-slopes.html"><a href="random-slopes.html#exercises-5"><i class="fa fa-check"></i><b>6.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="plot-code.html"><a href="plot-code.html"><i class="fa fa-check"></i><b>7</b> Plot Code</a></li>
<li class="divider"></li>
<li><a href="http://www.santiagobarreda.com" target="blank">Written by Santiago Barreda</a></li>
<li><a href="A-Quick-Introduction-to-Multilevel-Bayesian-Models-for-Linguistic-Researchers.pdf" target="blank">Download possibly outdated and badly-formatted PDF</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayesian multilevel models in R: A conceptual and practical introduction for linguists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inspecting-a-single-group-of-observations" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Inspecting a single group of observations</h1>
<p>We’ll begin with what is perhaps the simplest question a researcher can ask: What’s the average value of (some value) <span class="math inline">\(x\)</span>? We’re going to use this basic question to discuss some fundamental statistical concepts, and how to use these concepts to make inferences about the observations we collect.</p>
<p>Although statistical knowledge might seem like declarative knowledge, in many ways it is more similar to procedural knowledge. You would never read a chapter from a French textbook once and expect to have memorized all the vocabulary and irregular forms. Similarly, you would never practice a piano piece a single time and assume that you are just ‘bad at the piano’ because you can’t play it flawlessly.</p>
<p>Think of acquiring statistical knowledge like learning a language. It is normal, and in fact should be expected, that the reader will need to read some parts of the text multiple times, and <em>practice</em>, before being able to really <em>understand</em> all of the concepts presented here. That being said, the things we talk about in this chapter will be come up in every chapter, so if things don’t all make sense right now that’s fine. Things will make more sense bit by bit as we learn how to use more and more complicated models. After reading a few chapters you should come back and read this chapter again. You may notice that I say a lot of things in this chapter that you did not notice the first time you read it.</p>
<div id="data-and-research-questions" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Data and research questions</h2>
<p>Variables are placeholders for some value, whether we know it or not. For example I can say “my weight is X pounds”. Random variables are variables whose value varies from observation to observation based on some unknown process. The classic example is the result of a coin flip or the roll of a die. Neither of the values of these variables can be known <em>a priori</em> (i.e., before ‘experimentation’) and can only be known after an observation has been carried out.</p>
<p>Anything that is expected to vary slightly from observation to observation can be though of a random variable. For example, your exact weight varies from day to day around your ‘average’ weight. In principle, you could probably explain exactly why your weight varies from day if you were so inclined. However, in practice you are probably not exactly sure <em>why</em> your weight is a bit higher one day and a bit lower the next. So, your weight is a random variable not necessarily because it is <em>impossible</em> to know why it varies, but simply because <em>you don’t know</em> why it does so.</p>
<p>We’re going to begin by focusing on what are known as <em>continuous</em> variables. These variables take on an infinite, or at least reasonably large, set of possible values between the highest and lowest possible values of the variable. Some examples of continuous variables that arise often in linguistics are reaction times, frequencies of words in lexicons, and just about any acoustic measurement in phonetics.</p>
<p>For example, imagine listeners in an experiment hear a word they are asked to determine whether this is a ‘real’ word or not (i.e., a lexical decision task). You measure reaction times and are interested in how long people take to answer, on average. The variable of interest here is “reaction time in a lexical decision task”. It is a random variable because you don’t know what any given reaction time will be on any given trial. Keep in mind, this doesn’t mean we don’t know anything about it. The human body is physically incapable or reacting faster than about 150 ms, and decision should not be taking more than (let’s say) 3 seconds. So, although you don’t know what the <em>exact</em> value of any observation will be, it is possible to make statements about <em>where</em> you expect this variable to be along the number line.</p>
<p>In this chapter, we’re going to investigate the average voice fundamental frequency (f0) for a group of speakers. The f0 of a voice is the primary determinant of perceived pitch, and is a very important cue in speech communication. It it extremely important in speech prosody, in the relation of emotion in speech, it helps determine vowel quality and is crucial in the communication of social and indexical information (speaker gender, age, …etc.). In other words, when you hear a person speak, your impression of their gender, age and size are strongly influenced by pitch. As a result, this is a very important variable in linguistics and speech communication more generally.</p>
<p>We’re going use a well-known data set, the <a href="https://homepages.wmich.edu/~hillenbr/Papers/HillenbrandGettyClarkWheeler.pdf">Hillenbrand et al. (1995)</a> data representing vowels produced by 139 speakers of Michigan English. The data contains information about the productions of 12 vowel phonemes by 48 adult females, 45 adult males, 19 girls and 27 boys, both groups featuring children 10-12 years of age.</p>
<p>In this chapter we’re going to focus only on the f0 values produced by the 48 adult female speakers in the sample, across the 12 vowel phonemes they each produced (n=576=48*12). Below, I get the data from the course GitHub page and pass the relevant information to a vector called <code>f0</code> that we will be referring to throughout the chapter.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="inspecting-a-single-group-of-observations.html#cb1-1" aria-hidden="true" tabindex="-1"></a>url1 <span class="ot">=</span> <span class="st">&quot;https://raw.githubusercontent.com/santiagobarreda&quot;</span></span>
<span id="cb1-2"><a href="inspecting-a-single-group-of-observations.html#cb1-2" aria-hidden="true" tabindex="-1"></a>url2 <span class="ot">=</span> <span class="st">&quot;/stats-class/master/data/h95_vowel_data.csv&quot;</span></span>
<span id="cb1-3"><a href="inspecting-a-single-group-of-observations.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># read data from the book Github page</span></span>
<span id="cb1-4"><a href="inspecting-a-single-group-of-observations.html#cb1-4" aria-hidden="true" tabindex="-1"></a>h95 <span class="ot">=</span> <span class="fu">read.csv</span> (<span class="fu">url</span>(<span class="fu">paste0</span> (url1, url2)))</span>
<span id="cb1-5"><a href="inspecting-a-single-group-of-observations.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># select only &#39;f0&#39; values produced by adult females (speaker type = &#39;w&#39;)</span></span>
<span id="cb1-6"><a href="inspecting-a-single-group-of-observations.html#cb1-6" aria-hidden="true" tabindex="-1"></a>f0 <span class="ot">=</span> h95[[<span class="st">&#39;f0&#39;</span>]][h95<span class="sc">$</span>group <span class="sc">==</span> <span class="st">&#39;w&#39;</span>]</span></code></pre></div>
<p>The individual f0 values produced by any given woman from Michigan for any given utterance cannot be known a priori. In other words, you don’t know what pitch a random woman from Michigan will produce until you actually observe and record the production. For this reason, “the f0 produced by adult women from Michigan” is a random variable.</p>
<p>We’re going use the information in our sample to try to answer two questions about the f0 produced by female speakers from Michigan:</p>
<ol style="list-style-type: decimal">
<li><p>What is the average f0 of the whole <em>population</em> likely to be?</p></li>
<li><p>Can we set bounds on likely mean f0 values based on the data we collected?</p></li>
</ol>
<p>In order to answer questions about reasonable values for random variables, scientists often collect measurements of that variable. For example, although you may not know your exact weight in any given day, you may have enough observations to have a pretty god idea of what it might be tomorrow. In fact, your expectation may be so strong that a large deviation from it would be more likely to result in your buying a new scale than believing the measurement.</p>
<p>The measurements you make of a random variable are called <em>samples</em>. The sample is a finite set of observations that you actually have. The population is the (hypothetical) larger group of all possible observations that you are <em>actually</em> interested in. It is the entire set of possible values of the random variable. For example, the population of “f0 produced by adult women from Michigan” contains all possible values of f0 produced by the entire set of women from Michigan.</p>
<p>Usually, a linguist will collect a sample to make inferences about the population. In other words, we are interested in the general behavior of the variable itself, not just of the small number of instances that we observed. For example, Hillenbrand et al. collected their data to make inferences about Michigan speakers overall, and not because they were particularly interested in the specific speakers in their sample. They are using their small sample of speakers to make inferences about the whole population of speakers in Michigan.</p>
<p>Scientific research is often focused on questions such as (1) above. For example someone might ask “what’s the average f0 produced by adult female speakers?” and you can say, for example, “I have some data that suggests 220 Hz is a reasonable estimate”. However, reliable inference requires having good answers to the second sort of questions as well.</p>
<p>There is no chance that the average of the sample you collected will <strong>exactly</strong> match the population average. After all, the sample is just a (relatively) small collection of random observations! So, if the sample mean is 220 Hz, it seems reasonable that the <em>population</em> mean could actually equal 219 Hz. Could it equal 215 Hz? 200 Hz? Where does it stop? Clearly, we need some principled way to ‘guess’ reasonable ranges based on our sample of observations. In this chapter we will discuss how statistics provides us with a framework to answer both questions above using only our sample of values.</p>
<div id="inspecting-the-central-location-and-spread-of-values" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Inspecting the central location and spread of values</h3>
<p>Using R, we can easily find important information about our sample of f0 values. Below, we calculate the sample mean (<span class="math inline">\(\bar{x}\)</span>), the number of observations, the sample standard deviation (<span class="math inline">\(s_x\)</span>), and important quantiles. The quantiles below correspond to the values of ordered observations, as in the right plot in Figure <a href="inspecting-a-single-group-of-observations.html#fig:initialplot">1.1</a>. The 0% quantile is the smallest (leftmost) observation, while 100% is the highest (rightmost) observation. Any other quantile is found by ordering the observations and selecting the observation that is higher than <span class="math inline">\(x\%\)</span> of the sample values. For example, the 50% quantile (the median) is higher than 50% of values, and the 25% quantile is higher than 1/4 of the values in the sample.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="inspecting-a-single-group-of-observations.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate basic descriptive statistics</span></span>
<span id="cb2-2"><a href="inspecting-a-single-group-of-observations.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (f0)</span>
<span id="cb2-3"><a href="inspecting-a-single-group-of-observations.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 220.401</span></span>
<span id="cb2-4"><a href="inspecting-a-single-group-of-observations.html#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span> (f0)</span>
<span id="cb2-5"><a href="inspecting-a-single-group-of-observations.html#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 576</span></span>
<span id="cb2-6"><a href="inspecting-a-single-group-of-observations.html#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span> (f0)</span>
<span id="cb2-7"><a href="inspecting-a-single-group-of-observations.html#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 23.22069</span></span>
<span id="cb2-8"><a href="inspecting-a-single-group-of-observations.html#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span> (f0)</span>
<span id="cb2-9"><a href="inspecting-a-single-group-of-observations.html#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="do">##     0%    25%    50%    75%   100% </span></span>
<span id="cb2-10"><a href="inspecting-a-single-group-of-observations.html#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="do">## 149.00 207.00 220.00 236.25 307.00</span></span></code></pre></div>
<p>We can use this information to make some basic, and potentially useful statements about our data. The mean and median are 220 Hz, and f0 values range from 149 to 307 Hz. However, there are not many observations at the extremes, and 50% of values are between 207 and 236 Hz. The standard deviation is 23 Hz, indicating that the bulk of observations were within one standard deviation of the mean (220±23 Hz).</p>
<div class="figure"><span id="fig:initialplot"></span>
<img src="01_files/figure-html/initialplot-1.png" alt="(left) Plot of values in the order they appear in the original data. (right) Observations ordered by increasing value." width="768" />
<p class="caption">
Figure 1.1: (left) Plot of values in the order they appear in the original data. (right) Observations ordered by increasing value.
</p>
</div>
<p>We can look at the distribution of our sample of f0 values in several ways, as seen in Figure <a href="inspecting-a-single-group-of-observations.html#fig:F1-distributioncomparison">1.2</a>. In the top row each point indicates an individual production. Points are jittered along the y axis to make them easier to distinguish so that dense and sparse locations can be compared.</p>
<p>In the middle row we see a box plot of the same data. The edges of the box correspond to the 25% and 75% quantiles of the distribution, and the line in the middle of it corresponds to the median. So, the box spans the <em>interquartile range</em> of your observations and 50% of observations are contained in the box. From the edges of the box extend the boxplot ‘whiskers’. By default, these extend out 1.5 times the interquartile range. These whiskers are simply intended to give you an estimate of the amount of ‘typical’ variation in your sample. Beyond the whiskers we see individual <em>outliers</em>, points considered to be substantially different from the rest of the sample. We can see that the boxplot does a good job of summarizing the information in the top row, and provides information related to both average f0 values and to the expected variability in these values.</p>
<p>The bottom row presents what is knows as a <em>histogram</em> of the same data. The histogram divides the x axis into a set of discrete sections (‘bins’), and gives you the count (or frequency) of observations in each bin. Bins with lots of observations are relatively taller (more <em>dense</em>) than bins without many observations in them. As a result, histograms can be used to summarize where observations tend to be. For example, we can see that the bins under the interquartile range have the most observations, and that values further from the mean value become increasingly less frequent.</p>
<div class="figure" style="text-align: center"><span id="fig:F1-distributioncomparison"></span>
<img src="01_files/figure-html/F1-distributioncomparison-1.png" alt="Different ways to consider our distribution of f0 values." width="576" />
<p class="caption">
Figure 1.2: Different ways to consider our distribution of f0 values.
</p>
</div>
</div>
</div>
<div id="probability-distributions" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Probability Distributions</h2>
<p>Histograms are particularly useful to understand because of how they relate to probability distributions. The probability is the number of times an event is expected to occur, out of all the other observed events and outcomes that can occur. This can also be thought of as the <em>percent</em> of times an event is expected to occur. For example, the probability that any given day is Monday is 1/7 (14% of days), while the probability that any given day is a weekday is 2/7 (29% of days).</p>
<p>By definition, the total probability of all of the possible values in a population is always equal to 1. This is like using 100 to communicate percentages by convention. This is not <em>correct</em> or <em>true</em>, it is arbitrary just like our base 10 number system. We could make all probabilities add up to 1.37 instead, but why would we want to? Making all probabilities add up to one has many practical advantages and so this convention is actually beneficial for us. As a result of this convention, you know that a probability of 0.5 means something is expected to occur half the time (i.e., on 50% of trials). For example, suppose we want to know the probability of being an adult female in our sample who produces an f0 under 175 Hz. Finding the probability of observing this event is easy:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="inspecting-a-single-group-of-observations.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the evaluation in the parenthesis will return 1 if true, 0 if false</span></span>
<span id="cb3-2"><a href="inspecting-a-single-group-of-observations.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># number of observations the fall below threshold</span></span>
<span id="cb3-3"><a href="inspecting-a-single-group-of-observations.html#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span> (f0 <span class="sc">&lt;</span> <span class="dv">175</span>)  </span>
<span id="cb3-4"><a href="inspecting-a-single-group-of-observations.html#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 22</span></span>
<span id="cb3-5"><a href="inspecting-a-single-group-of-observations.html#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="inspecting-a-single-group-of-observations.html#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># divided by total number of events</span></span>
<span id="cb3-7"><a href="inspecting-a-single-group-of-observations.html#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span> (f0 <span class="sc">&lt;</span> <span class="dv">175</span>) <span class="sc">/</span> <span class="fu">length</span> (f0)  </span>
<span id="cb3-8"><a href="inspecting-a-single-group-of-observations.html#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.03819444</span></span>
<span id="cb3-9"><a href="inspecting-a-single-group-of-observations.html#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="inspecting-a-single-group-of-observations.html#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># a shortcut to calculate probability, mean = total/length</span></span>
<span id="cb3-11"><a href="inspecting-a-single-group-of-observations.html#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (f0 <span class="sc">&lt;</span> <span class="dv">175</span>)</span>
<span id="cb3-12"><a href="inspecting-a-single-group-of-observations.html#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.03819444</span></span></code></pre></div>
<p>The top value is the frequency of the occurrence. This is not so useful because this number can mean very different things given different sample sizes (e.g., 22/23, 22/10000). The middle and bottom values have been divided by the total number of observations. As a result, these now represent a proportion, or probability.</p>
<p>The histogram on the left below shows frequency on the y axis, indicating the total number of observations in each bin. The histogram on the right shows <em>density</em> along the y axis. When you see <em>density</em> on the y axis, that means that y axis values have been scaled so that the total area of all the rectangles making up the histogram are equal to 1. This has two benefits:</p>
<ol style="list-style-type: decimal">
<li>It lets you compare the distribution of values across different sample sizes.</li>
<li>It makes the histogram more comparable to a probability distribution.</li>
</ol>
<div class="figure"><span id="fig:F1-twohists1"></span>
<img src="01_files/figure-html/F1-twohists1-1.png" alt="(left) A histogram of our f0 data showing counts in each bin. (right) A histogram of our f0 data showing densities." width="768" />
<p class="caption">
Figure 1.3: (left) A histogram of our f0 data showing counts in each bin. (right) A histogram of our f0 data showing densities.
</p>
</div>
<p>Imagine a circle like in a Venn diagram that contains all possible productions of female f0. This circle has an area of 1 since it contains all possible values in the population. Imagine we spread out this circle along the x axis so that its shape reflected the relative frequencies of different values presents in the population. For example, if some outcomes were 5 times more probable than others, the shape should be 5 times taller there, and so on. If we managed to do this, the height (or ‘density’) of this shape would exactly correspond to a probability distribution like that seen in the right plot above.</p>
<p>The <em>density</em> of a histogram (of probability distribution) is just the thickness of the distribution at a certain location along the number line. A higher density in a certain location tells us values in that vicinity are relatively more probable, and differences in density reflect differences in the relative probability of different values.</p>
<p>Below I’ve repeated the data vector, doubling the counts by including each observation twice. Notice that the y axis in the right panel does not change. This is because increasing the number of observations changes your counts but not the relative frequencies of observations with different values. For instance, increasing the number of coin flips should not change the fact that 50% will be heads, but it will change the number of heads you observe.</p>
<div class="figure"><span id="fig:F1-twohists2"></span>
<img src="01_files/figure-html/F1-twohists2-1.png" alt="The counts have been doubled relative to above." width="768" />
<p class="caption">
Figure 1.4: The counts have been doubled relative to above.
</p>
</div>
<div id="the-normal-distribution" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> The normal distribution</h3>
<p>The distribution of many <a href="https://www.youtube.com/watch?v=4HpvBZnHOVI">random variables</a> (including f0) follows what’s called a <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a>, also called a Gaussian distribution. So, if you take a sample of a random variable and arrange observations into bins, they resulting histogram will tend to have the familiar, bell-shaped curve common to normally-distributed data.</p>
<p>The normal distribution has the following important characteristics.</p>
<ol style="list-style-type: decimal">
<li><p>The distribution is symmetrical - i.e., producing a higher or lower than average f0 is about equally likely.</p></li>
<li><p>The probability of observing a given value decreases as you get further from the mean (i.e., <em>average</em>) value.</p></li>
<li><p>It’s easy to work with, very well understood, and naturally arises in basically all domains.</p></li>
</ol>
<p>Normal distributions have two parameters. This means they vary from each other in only two ways. Think of parameters like ways that things can be ‘set’ differently from each other. For example, a radio has three parameters: tuner frequency, band (AM/FM), and volume. A toaster may have only one, a single knob determining the degree of toasting required. The more parameters something has, the more complicated it is (an airplane has thousands!).</p>
<p>Luckily, the normal distribution only has two parameters:</p>
<ol style="list-style-type: decimal">
<li><p>A mean parameter, <span class="math inline">\(\mu\)</span>, which determines the location of the distribution along the x axis. When the mean changes, the whole shape of the distribution slides along the number line. The mean is the 50% halfway point of the ‘mass’ of the distribution. If the distribution were an physical object, its mean would be its center of gravity. You would balance the distribution on your fingertip along this point.</p></li>
<li><p>A standard deviation, <span class="math inline">\(\sigma\)</span>, that determines its <em>spread</em> along the x axis. When the standard deviation changes the distribution is stretched wide or made very narrow, but stays in place. Since every distribution has an area under the curve equal to one (i.e., they all have the same ‘volume’), distributions with a small variance must necessarily be very dense.</p></li>
</ol>
<p>The parameters of a probability distribution are used to draw its shape, which can be used to make inferences about likely values. Think back to high school math and the function defining the shape of a parabola <span class="math inline">\(y = ax^2+bx+c\)</span>. This function draws a shape based on the settings of its parameters <span class="math inline">\(a, b\)</span> and <span class="math inline">\(c\)</span>. In the exact same way, the formula defining the density of the normal distribution draws the shape given the settings of its <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> parameters.</p>
<p>Below, I compare the histogram of f0 values to the density of a normal distribution with a mean equal to our sample mean (<span class="math inline">\(\mu = 220 Hz\)</span>) and a standard deviation equal to our sample standard deviation (<span class="math inline">\(\sigma = 23 Hz\)</span>), calculated above. The density was drawn using the <code>dnorm</code> function, which draws a curve representing the shape of a theoretical normal distribution with a given mean and standard deviation.</p>
<div class="figure"><span id="fig:theoretical"></span>
<img src="01_files/figure-html/theoretical-1.png" alt="A comparison of the data distribution with a theoretical normal distribution." width="768" />
<p class="caption">
Figure 1.5: A comparison of the data distribution with a theoretical normal distribution.
</p>
</div>
<p>Clearly, there is a very good alignment between our random sample of real-world data and the normal density, which is a theoretical mathematical function. This suggests that we could potentially use the <em>theoretical</em> shape of the normal distribution to talk about the characteristics of our observed random sample of data. This would be a bit like using the properties of a parabola to predict the path of a projectile in real life: although the actual path will differ form that of a perfect parabola, there are enough similarities to make the comparison worthwhile.</p>
</div>
<div id="referring-to-the-normal-distribution-to-make-inferences" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Referring to the normal distribution to make inferences</h3>
<p>In general, it’s impossible to know what the ‘true’ data distribution is, so that <em>perfect</em> inference is not possible. As a result, scientists often use theoretical probability distributions to make inferences about real-life populations and observations. Since our f0 measurements follow the ‘shape’ predicted by the theoretical normal distribution, we may be able to use the characteristics of an appropriate normal distribution to make inferences about female f0 (and other variables).</p>
<p>Using a normal distribution to make inferences about your data is like using a mathematical model for spheres to understand the behavior of billiard balls. In reality the balls are not perfect spheres. However, their shapes will be <em>spherical enough</em> to allow us to make useful predictions based on the simplified model. In general, it is useful to keep in mind that reality will never exactly conform to our model. This can result in unpredictable errors in our conclusions. In general, the things you don’t know you don’t know are the things that will cause the most problems. If you know that your model was wrong, you would have fixed it!</p>
<p>Since we expect the distribution of f0 values to have the shape of the normal distribution, we can use the shape of the normal distribution to make inferences about the distribution of f0 values, even the ones we did not observe. For example, we can use the theoretical normal density to estimate the probability of observing a female production with an f0 of under 175 Hz, from among <em>all</em> possible values of this random variable (i.e., all observable productions of f0 in this <em>population</em>).</p>
<p>We do this by referring to the proportion of values expected to be less 175 Hz in the normal distribution that has the same ‘shape’ as our sample. This can be found by finding the area under the curve of the probability density to the left of that point (the red area below). Since the <em>total</em> area is always equal to 1, the area of the red portion below corresponds to a percentage/probability.</p>
<p>Below, I use the function <code>pnorm</code> to find the proportion of values that are expected to be greater/less than 175 Hz. We can do this for the normal density because it is very well understood. Think of how easy it is to calculate the area of a circle of the perimeter of a square. The shape of the normal distribution is only a little more complicated to understand, and the <code>pnorm</code> function helps you make predictions about it.</p>
<p>I use the parameters estimated form our sample to run the <code>pnorm</code>function, as these are our best guesses of the population parameters. As we can see, this value is reasonably close to our empirical probability of 0.038 (3.8%).</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="inspecting-a-single-group-of-observations.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Observed probability observing a token with an f0 &lt; 175 Hz</span></span>
<span id="cb4-2"><a href="inspecting-a-single-group-of-observations.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (f0 <span class="sc">&lt;</span> <span class="dv">175</span>)</span>
<span id="cb4-3"><a href="inspecting-a-single-group-of-observations.html#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.03819444</span></span>
<span id="cb4-4"><a href="inspecting-a-single-group-of-observations.html#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="inspecting-a-single-group-of-observations.html#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Theoretical probability of observing a production below 175 Hz</span></span>
<span id="cb4-6"><a href="inspecting-a-single-group-of-observations.html#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span> (<span class="dv">175</span>, <span class="fu">mean</span> (f0), <span class="fu">sd</span>(f0))</span>
<span id="cb4-7"><a href="inspecting-a-single-group-of-observations.html#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.02527988</span></span>
<span id="cb4-8"><a href="inspecting-a-single-group-of-observations.html#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="inspecting-a-single-group-of-observations.html#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Theoretical probability of observing a production greater than 175 Hz</span></span>
<span id="cb4-10"><a href="inspecting-a-single-group-of-observations.html#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span> (<span class="dv">175</span>, <span class="fu">mean</span> (f0), <span class="fu">sd</span>(f0))</span>
<span id="cb4-11"><a href="inspecting-a-single-group-of-observations.html#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.9747201</span></span></code></pre></div>
<div class="figure"><span id="fig:F1-prediction"></span>
<img src="01_files/figure-html/F1-prediction-1.png" alt="The read area relects the distribution of outcomes that satisfy f0 &lt; 175 Hz." width="768" />
<p class="caption">
Figure 1.6: The read area relects the distribution of outcomes that satisfy f0 &lt; 175 Hz.
</p>
</div>
<p>Imagine you had 1 pound of clay and I asked you to make a shape <strong>exactly</strong> like the normal density (red curve) above. This shape should be perfectly flat, i.e., it should have a constant depth (like a coin). If you had this shape made of clay used a knife to remove the part left of 175 Hz (the red subsection) and weighed it, it should weigh 2.5% of a pound. The ‘area under the curve’ of this clay sculpture would just correspond to the amount of clay in a certain area, and in this case we know that only 2.5% of the clay should be in that section of the shape. So, the area under the curve, the probability, is just the amount of the <em>stuff</em> in the density that falls below/above a certain point, or between two points.</p>
<p>We can use the probabilities to compare our predicted and expected observations. Since we had 576 observations and there was a probability of 0.025 of observing tokens with an f0 below 175 Hz, we expect about 14 tokens (<span class="math inline">\(576 \times 0.025\)</span>) to have an f0 lower than 175 Hz. Instead, we have 22 such observations indicating that our actual data has more extreme values than our theoretical distribution.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="inspecting-a-single-group-of-observations.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># probability of observing a production with an f0 under 175 Hz</span></span>
<span id="cb5-2"><a href="inspecting-a-single-group-of-observations.html#cb5-2" aria-hidden="true" tabindex="-1"></a>probability <span class="ot">=</span> <span class="fu">pnorm</span> (<span class="dv">175</span>, <span class="fu">mean</span> (f0), <span class="fu">sd</span>(f0)) </span>
<span id="cb5-3"><a href="inspecting-a-single-group-of-observations.html#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="inspecting-a-single-group-of-observations.html#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># expected count</span></span>
<span id="cb5-5"><a href="inspecting-a-single-group-of-observations.html#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span> (f0) <span class="sc">*</span> probability</span>
<span id="cb5-6"><a href="inspecting-a-single-group-of-observations.html#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 14.56121</span></span>
<span id="cb5-7"><a href="inspecting-a-single-group-of-observations.html#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="inspecting-a-single-group-of-observations.html#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># actual count</span></span>
<span id="cb5-9"><a href="inspecting-a-single-group-of-observations.html#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span> (f0 <span class="sc">&lt;</span> <span class="dv">175</span>)</span>
<span id="cb5-10"><a href="inspecting-a-single-group-of-observations.html#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 22</span></span></code></pre></div>
</div>
</div>
<div id="probabilities-of-events-and-likelihoods-of-parameters" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Probabilities of events and likelihoods of parameters</h2>
<p>We’re going to switch from talking about <em>probabilities</em> to talking about <em>likelihoods</em>. A probability is the odds of observing some data/event/outcome, given some parameter(s). A likelihood places odds on different <em>parameter</em> values given some observed data. These concepts are basically the inverse of each other. For example, you could say “how probable is it that a random person from San Francisco will be over 6 feet tall?”. In contrast, you could ask “how likely is it that the average height of people from San Francisco is 6 feet?”.</p>
<p>The <em>likelihood function</em> is a curve showing the relative likelihoods of different parameter values, given a fixed set of data. The likelihood function tells you what values are <em>believable</em> given your data. If a value is very unlikely, that means that it is not supported by your data. In other words, unlikely parameter estimates represent conclusions that your data is rejecting as not viable. Every parameter for every probability distribution has a likelihood function, given some data. Here, we’re only going to discuss the likelihood of the normal mean parameter, <span class="math inline">\(\mu\)</span>, in detail.</p>
<p>Here are three useful properties of the likelihood functions of <span class="math inline">\(\mu\)</span>, the mean parameter of the normal distribution:</p>
<ol style="list-style-type: decimal">
<li><p>The likelihood function of <span class="math inline">\(\mu\)</span> will tend to be a normal distribution.</p></li>
<li><p>The mean (and peak) of the likelihood function of <span class="math inline">\(\mu\)</span> given some sample <span class="math inline">\(x\)</span> is equal to the arithmetic mean of the sample (<code>mean(x)</code>).</p></li>
<li><p>The standard deviation of the likelihood of <span class="math inline">\(\mu\)</span> is equal to the standard deviation of the data (<code>sd(x)</code>), divided by the square root of N (the sample size).</p></li>
</ol>
<p>The first point tells us that we can use the normal distribution to make inferences about likely, and unlikely values for means, given some data.</p>
<p>The second point says that if you are wondering what the best (most likely) estimate of the population mean is given your sample, the answer is the arithmetic mean of your sample (<span class="math inline">\(\bar{x}\)</span>).</p>
<p>The third point means that the likelihood function for <span class="math inline">\(\mu\)</span> will tend to be <em>much</em> narrower than the distribution of our original data. This is because a mean based on, for example, 50 samples will contain many positive and negative deviations from the average that will tend to cancel out. As a result, the more data you have the more <em>precise</em> your estimates are, and the less <em>uncertainty</em> is associated with any estimate. This is why scientists focus so much on sample size when they conduct statistical analyses.</p>
<p>Figure <a href="inspecting-a-single-group-of-observations.html#fig:F1-likelihood1">1.7</a> shows the likelihood function for <span class="math inline">\(\mu\)</span> based on the first 10 observations of our f0 vector (indicated by the blue points at the bottom of the plot). I chose this small sample just to make this example clearer. Notice that the most likely mean values of <span class="math inline">\(\mu\)</span> for these points like over the bulk of the sampled values. The vertical dotted lines show three possible mean values that will be highlighted in this discussion.</p>
<p>The likelihood of any parameter estimate (e.g., <span class="math inline">\(\mu\)</span> = 175 Hz in the right panel of Figure <a href="inspecting-a-single-group-of-observations.html#fig:F1-likelihood1">1.7</a>) is equal to the product of the density of each observation in the sample, if we assume that the estimate were true. This is actually deceptively simple. For example, to calculate the likelihood that <span class="math inline">\(\mu=175\)</span>, we:</p>
<ol style="list-style-type: decimal">
<li><p>Assume that the data is generated by a normal distribution with a <span class="math inline">\(\mu\)</span> equal to 175 Hz, and <span class="math inline">\(\sigma\)</span> equal to the sample standard deviation of 23 Hz.</p></li>
<li><p>Find the the height of the curve of the probability distribution (the density) over each point (indicated by the vertical lines in the right panel below).</p></li>
<li><p>The likelihood is the product of all of these densities (heights). In practice, often the logarithms of the individual probabilities are added together, yielding the <em>log-likelihood</em>. This is because multiplying together too many fractions can lead to numbers so small computers have a hard time representing them, and adding logarithms is equivalent to multiplying the original values.</p></li>
</ol>
<p>Imagine I follow the steps above for each position along the x axis, recording the likelihood values I calculate. I then plot the product of the densities for each corresponding x value. If I do this I have just plotted a likelihood function for <span class="math inline">\(\mu\)</span> given our data, and the result would be a curve identical to that of the left panel in <a href="inspecting-a-single-group-of-observations.html#fig:F1-likelihood1">1.7</a>.</p>
 
<div class="figure"><span id="fig:F1-likelihood1"></span>
<img src="01_files/figure-html/F1-likelihood1-1.png" alt="(Left) The likelihood of the population mean given the blue points in the figure. (right) The probability of the points given an assumed mean of 175 Hz." width="768" />
<p class="caption">
Figure 1.7: (Left) The likelihood of the population mean given the blue points in the figure. (right) The probability of the points given an assumed mean of 175 Hz.
</p>
</div>
<p> </p>
<p>In the right panel in Figure <a href="inspecting-a-single-group-of-observations.html#fig:F1-likelihood1">1.7</a> we see that a normal distribution with a <span class="math inline">\(\mu\)</span> of 175 Hz is very unlikely to generate this data. Many points are extremely improbable and have densities close to zero. As a result, the product of these values (the heights of the lines) will be a very small number. This is reflected in the extremely small values in the likelihood function at 175 Hz in the left panel above.</p>
<p>In the left panel in Figure <a href="inspecting-a-single-group-of-observations.html#fig:F1-likelihood2">1.8</a>, we see that a normal distribution with a <span class="math inline">\(\mu\)</span> of 200 Hz is more likely to generate this data, and the probability distribution is clearly a much better fit. However a distribution with a mean of 200 Hz is still not very likely to have generated this data.</p>
<p>Finally, in the right panel below we see the the maximum likelihood estimate for this sample of 225 Hz, the value representing the peak of the likelihood function (in the left panel above). When we say that 225 Hz is the most likely mean for this data, we are saying that this data is probably generated by a normal distribution centered at 225 Hz, relative to the alternatives.</p>
 
<div class="figure"><span id="fig:F1-likelihood2"></span>
<img src="01_files/figure-html/F1-likelihood2-1.png" alt="(Left) The probability of the points given an assumed mean of 200 Hz. (right) The probability of the points given an assumed mean of 225 Hz." width="768" />
<p class="caption">
Figure 1.8: (Left) The probability of the points given an assumed mean of 200 Hz. (right) The probability of the points given an assumed mean of 225 Hz.
</p>
</div>
<p> </p>
<div id="making-inferences-using-likelihoods" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Making inferences using likelihoods</h3>
<p>Previously, we discussed using the normal distribution to make inferences about the probable values of random variables. When variables are normally distributed we can use the theoretical normal distribution and functions such as <code>pnorm</code> to answer questions about values we expect, and don’t expect, to see. We can take this same approach to make inferences about <em>parameters</em> when their likelihood functions follow a normal distribution.</p>
<p>It helps to understand this if we start thinking about our parameters as random variables themselves. Think of the average height of the people in a large city. You can go out and sample 100 individual people, and each one of those samples is an observation from a random variables. You can find the mean of your sample, arriving at a single estimate of the population mean. Now imagine that 50 people went out in the same city and each sampled 100 random people. There is no chance that every single of those 50 people would find identical means across all of their samples. Instead, there will be a distribution of sample means, in the same way there is a distribution of the original data used to calculate the means.</p>
<p>We know that our f0 data is approximately normally distributed. As a result, we know the following information:</p>
<ol style="list-style-type: decimal">
<li><p>The likelihood function of our sample mean parameter follows a normal distribution.</p></li>
<li><p>The mean of this distribution is equal to the sample mean, the mean of our observations.</p></li>
<li><p>The standard deviation of this distribution is equal to the standard deviation if the samples, divided by the square root of the sample size (<code>sqrt(576)=24</code>).</p></li>
</ol>
<p>We can calculate these values and use them to draw the curve representing the likelihood function given our data and model structure. You may be thinking, what model? It may seem too simple to be a model, but by assuming that our data can be understood as coming from a normal distribution with some given <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, we have already created a simple model for our data. I’ll return to this below.</p>
<p>We can also use the <code>qnorm</code> function to calculate quantiles for our likelihood, presented below. Vertical lines have been added at the 2.5% and 97.5% quantiles of the distribution. These vertical lines enclose 95% of the likelihood density, and so represent the range of values representing the 95% most likely values of <span class="math inline">\(\mu\)</span>. I chose an interval enclosing 95% of the likelihood because this is used by convention. This is a commonly-used interval but otherwise has no special significance.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="inspecting-a-single-group-of-observations.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># sample mean</span></span>
<span id="cb6-2"><a href="inspecting-a-single-group-of-observations.html#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (f0)   </span>
<span id="cb6-3"><a href="inspecting-a-single-group-of-observations.html#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 220.401</span></span>
<span id="cb6-4"><a href="inspecting-a-single-group-of-observations.html#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="inspecting-a-single-group-of-observations.html#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># sample standard deviation</span></span>
<span id="cb6-6"><a href="inspecting-a-single-group-of-observations.html#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span> (f0)     </span>
<span id="cb6-7"><a href="inspecting-a-single-group-of-observations.html#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 23.22069</span></span>
<span id="cb6-8"><a href="inspecting-a-single-group-of-observations.html#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="inspecting-a-single-group-of-observations.html#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># sample size</span></span>
<span id="cb6-10"><a href="inspecting-a-single-group-of-observations.html#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span> (f0)  </span>
<span id="cb6-11"><a href="inspecting-a-single-group-of-observations.html#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 576</span></span>
<span id="cb6-12"><a href="inspecting-a-single-group-of-observations.html#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="inspecting-a-single-group-of-observations.html#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># the standard deviation of the likelihood function</span></span>
<span id="cb6-14"><a href="inspecting-a-single-group-of-observations.html#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span> (f0) <span class="sc">/</span> <span class="fu">sqrt</span> ( <span class="fu">length</span> (f0) ) </span>
<span id="cb6-15"><a href="inspecting-a-single-group-of-observations.html#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.9675289</span></span>
<span id="cb6-16"><a href="inspecting-a-single-group-of-observations.html#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="inspecting-a-single-group-of-observations.html#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># the 2.5% and 97.5% quantiles of the likelihood function</span></span>
<span id="cb6-18"><a href="inspecting-a-single-group-of-observations.html#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span> (<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="fu">mean</span> (f0), <span class="fu">sd</span> (f0) <span class="sc">/</span> <span class="fu">sqrt</span> (<span class="fu">length</span> (f0) ) )</span>
<span id="cb6-19"><a href="inspecting-a-single-group-of-observations.html#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 218.5047 222.2974</span></span></code></pre></div>
<div class="figure"><span id="fig:F1-likelihood"></span>
<img src="01_files/figure-html/F1-likelihood-1.png" alt="Likelihood of population mean given our data. Horizontal lines indicate intervals enclosing 95% of the distribution." width="768" />
<p class="caption">
Figure 1.9: Likelihood of population mean given our data. Horizontal lines indicate intervals enclosing 95% of the distribution.
</p>
</div>
<p> </p>
<p>The likelihood tells you about the most believable/credible parameter values, given your model and data. Given the information presented in the figure above, we may conclude that the most likely parameter values fall between (around) 218 and 222 Hz. This means that it is reasonable that the true population mean might be 221 Hz, as this value is very likely given our sample. Basically, maybe our sample mean is wrong and arose by accident, and 221 Hz is the true population <span class="math inline">\(\mu\)</span>. This outcome is compatible with our data.</p>
<p>However, a value of 216 Hz is extremely <em>unlikely</em> to fit our data. It is just too far from our sample mean relative to the amount of variation in our sample. This is like if you measured the heights of 100 women in a small town (pop. 1500) and found the average was 5’4“. You might accept that the actual population average is 5’5”, but may find it difficult to accept that it was actually 6’0". It would mean you happened to measure all of the shortest women in the town, an extremely unlikely event.</p>
<p>So, since we think that 216 Hz is not a plausible mean f0 given our sample, this also means that it is very unlikely that the real <span class="math inline">\(\mu\)</span> is 216 Hz. This is because a distribution centered at 216 would be extremely unlikely to generate a sample mean of 220 Hz. Using this approach, we can rule out implausible values of <span class="math inline">\(\mu\)</span> based on the characteristics of our data.</p>
<p>At this point we can offer conventional responses to the research questions posed at the start of the Chapter:</p>
<p>Q1) What is the average f0 of the whole <em>population</em> likely to be?</p>
<p>A1) The most likely value for the population mean is our sample mean, 220.4 Hz.</p>
<p>Q2) Can we set bounds on likely mean f0 values based on the data we collected?</p>
<p>A2) Yes, there is a 95% probability that the population mean is between 218.5 222.3 Hz, given our data and model structure.</p>
<p>Traditional approaches to statistics (sometimes generally referred to as ‘frequentist’) estimate parameters by trying to find the most likely values for parameters (i.e., ‘maximum likelihood estimation’). They do this by referring to the theoretical likelihood functions such as what we plotted above. Although this works very well for simple data, it is difficult if not impossible for some of the more complicated datasets that often arise for even the simplest research questions in linguistics.</p>
</div>
</div>
<div id="bayesian-models" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Bayesian models</h2>
<p>In this class we are going to learn about <em>multilevel Bayesian models</em>. These models have many advantages over ‘traditional’ approaches. They provide researchers with more information, are more robust, and at <strong>worst</strong>, they are as good as traditional models. I may sound biased, but the main reason for all of these advantages is that traditional models were developed over 100 years ago. On the other hand, mathematical and technological advances have only made Bayesian multilevel models possible in the last 10+ years. It’s only reasonable that the newer approaches should offer some advantages over methods developed before calculators existed.</p>
<p>Here, I am going to address what is meant by two aspects of the term ‘Bayesian multilevel models’: ‘Bayesian’ and ‘models’.</p>
<div id="what-are-regression-models" class="section level3" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> What are regression models?</h3>
<p>Before beginning this section I just want to say that its ok if a lot of this section doesn’t makes sense right now. It will make more sense once you start to actually build models and it becomes less hypothetical and more practical. I will use the terms and concepts described here in later chapters, but I will re-explain it each time. If you think that a model in a later section is not explained in as much detail as you would like, look at this section again!</p>
<p>I have been referring somewhat obliquely to ‘models’ without really explaining what I mean by this. It’s difficult to offer a precise definition because the term is so broad, but ‘regression’ modeling can be thought of as trying to understand variation the mean parameter (<span class="math inline">\(\mu\)</span>) of a normal distributions. Actually, you can use many other probability distributions, but for now we will focus on models based on the normal distribution.</p>
<p>Basically it goes like this:</p>
<ul>
<li><p>you have a variable you are interested in, <span class="math inline">\(y\)</span>, which is is a vector containing N observations. We can refer to any one of these observations like this <span class="math inline">\(y_{[i]}\)</span> for the <span class="math inline">\(i^{th}\)</span> observation. In our case this is a vector of 576 f0 values (<code>f0[1:576]</code>). Although its not necessary, I am going to put the index variables associated with trial number (<span class="math inline">\(i\)</span>) in brackets like this <span class="math inline">\(y_{[i]}\)</span>. This is just to make it easier to identify, and to highlight the similarity to vectors (e.g., <code>f0[i]</code>).</p></li>
<li><p>you assume that your data is well described by a normal probability distribution. This is a mathematical function (<span class="math inline">\(\mathcal{N}(\mu,\sigma)\)</span>) that describes what is and is not probable based on two parameters.</p></li>
<li><p>the mean of this distribution is either fixed, or varies in a systematic manner.</p></li>
<li><p>the variation in the mean of this distribution can be understood using some other variables.</p></li>
</ul>
<p>We can write this model more formally like this:</p>
<p><span class="math display" id="eq:1">\[
y_{[i]} \sim \mathcal{N}(\mu,\sigma)
\tag{1.1}
\]</span></p>
<p>This says that we expect that the tokens of the variable we are interested in is distributed according to (<span class="math inline">\(\sim\)</span>) a normal distribution with those parameters.</p>
<p>Notice hat <span class="math inline">\(y\)</span> gets a subscript while <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> do not. This is because for right now, those parameters are fixed for all observations, while the value of <span class="math inline">\(y\)</span> changes for each observation based on the <span class="math inline">\(i\)</span> subscript. For example, below I set <span class="math inline">\(i=2\)</span> and use this index variable to show the second element of the data vector, i.e. <span class="math inline">\(f0_{[i=2]}=214\)</span>.</p>
<pre class="rcollapse"><code>f0[1:6]
i = 2
f0[i]</code></pre>
<p>Equation <a href="inspecting-a-single-group-of-observations.html#eq:1">(1.1)</a> just formalizes the fact that we think the <em>shape</em> of our data will be like that of a normal distribution with a mean equal to <span class="math inline">\(\mu\)</span> and a standard deviation equal to <span class="math inline">\(\sigma\)</span>.</p>
<p>When you see this, <span class="math inline">\(\mathcal{N}(\mu,\sigma)\)</span>, just picture in your mind the shape of a normal distribution, like if you see this <span class="math inline">\(y=x^2\)</span> you may imagine a parabola. <span class="math inline">\(\mathcal{N}(\mu,\sigma)\)</span> Really just represents that shape of the normal distribution, and the associated expectation about more and less probable outcomes.</p>
<p>The above relationship can also be presented like this:</p>
<p><span class="math display" id="eq:2">\[
y_{[i]} = \mu + \mathcal{N}(0,\sigma)
\tag{1.2}
\]</span></p>
<p>Notice that we got rid of the <span class="math inline">\(\sim\)</span> symbol, moved <span class="math inline">\(\mu\)</span> out of the distribution function (<span class="math inline">\(\mathcal{N}()\)</span>), and that the mean of the distribution function is now 0. This breaks up our variable into two components:</p>
<ol style="list-style-type: decimal">
<li><p>A systematic component, <span class="math inline">\(\mu\)</span>, that contributes the same value to all instances of a variable.</p></li>
<li><p>A random component, <span class="math inline">\(\mathcal{N}(0,\sigma)\)</span>, that causes unpredictable variation around <span class="math inline">\(\mu\)</span>.</p></li>
</ol>
<p>In terms of our data, I might express the distribution in either of the following ways:</p>
<p><span class="math display" id="eq:3">\[
f0_{[i]} = \mathcal{N}(220.4,23.2)
\tag{1.3}
\]</span></p>
<p><span class="math display" id="eq:4">\[
f0_{[i]} = 220.4 + \mathcal{N}(0,23.2)
\tag{1.4}
\]</span></p>
<p>The distribution on the left below is the original data, centered at 220.4 Hz and with a standard deviation of 23.2 Hz. On the right, the mean has been subtracted from each value. The sample now represents random variation around the sample mean, variation that our model can’t explain. From the perspective of our model, this is <em>noise</em>, or <em>error</em>. This doesn’t mean that it’s unexplainable, it only means that we’ve structured our model in a way that doesn’t let us explain it. Right now our model basically says “I know the mean is 220.4 Hz but the rest of the variation in values of f0 is random”.</p>
<div class="figure"><span id="fig:F1-errorhist"></span>
<img src="01_files/figure-html/F1-errorhist-1.png" alt="(left) Histogram of data. (right) Histogram of centered data, basically the error distribution." width="768" />
<p class="caption">
Figure 1.10: (left) Histogram of data. (right) Histogram of centered data, basically the error distribution.
</p>
</div>
<p>In regression models, we can decompose systematic variation in <span class="math inline">\(\mu\)</span> into component parts, based on some predictor variables, <span class="math inline">\(\mathrm{x}\)</span>. These predictor variables co-vary (vary with) our <span class="math inline">\(y\)</span> variable, and we think help explain the variation in <span class="math inline">\(y\)</span>.</p>
<p>Below, I am saying that I think <span class="math inline">\(\mu\)</span> is actually equal to some combination of <span class="math inline">\(\mathrm{x}_{1}\)</span> <span class="math inline">\(\mathrm{x}_{2}\)</span> and <span class="math inline">\(\mathrm{x}_{3}\)</span>. For example, I could think that f0 is affected by the speaker age (<span class="math inline">\(\mathrm{x}_{1}\)</span>) and gender of the speaker (<span class="math inline">\(\mathrm{x}_{2}\)</span>), and vowel category (<span class="math inline">\(\mathrm{x}_{3}\)</span>) of the production.</p>
<p><span class="math display" id="eq:5">\[
\mu = \mathrm{x}_{1} + \mathrm{x}_{2} + \mathrm{x}_{3}
\tag{1.5}
\]</span></p>
<p>The values of the predictor variables will vary from trial to trial, and are not fixed. Often the whole point of running an experiment is to predict differences in observations based on differing predictor values! So obviously, <span class="math inline">\(\mu\)</span> will need to vary from trial to trial. That means that the equation above should actually include <span class="math inline">\(i\)</span> subscripts indicating that the equation refers to the value of the predictors and expected mean, <em>for that trial</em> rather than overall.</p>
<p><span class="math display" id="eq:5">\[
\mu_{[i]} = \mathrm{x}_{1[i]} + \mathrm{x}_{2[i]} + \mathrm{x}_{3[i]} \tag{1.5}
\]</span></p>
<p>Actually, the mean is very unlikely to just be an equal combination of the predictors, so that a <em>weighting</em> of the predictors will be necessary. We can use the symbol <span class="math inline">\(\alpha\)</span> for these weights. For example, maybe <span class="math inline">\(\mathrm{x}_{1}\)</span> is twice as important as the other two predictors and so <span class="math inline">\(\alpha_1\)</span> is 2, while <span class="math inline">\(\alpha_2\)</span> and <span class="math inline">\(\alpha_3\)</span> are 1.</p>
<p><span class="math display" id="eq:6">\[
\mu_{[i]} = \alpha_1*\mathrm{x}_{1[i]} + \alpha_2*\mathrm{x}_{2[i]} + \alpha_3*\mathrm{x}_{3[i]}  
\tag{1.6}
\]</span></p>
<p>Note that the weight terms (<span class="math inline">\(\alpha\)</span>) do not get an <span class="math inline">\(i\)</span> subscript. This is because they do not change from trial to trial. The <em>values</em> of the predictors change from trial to trial, but the way that these are combined does not, they are a stable property of the model.</p>
<p>Decomposition of <span class="math inline">\(\mu\)</span> into sub-components makes our model something more like:</p>
<p><span class="math display" id="eq:7">\[
y_{[i]} = \mu_{[i]} + \mathcal{N}(0,\sigma)  
\tag{1.7}
\]</span></p>
<p><span class="math display" id="eq:8">\[
y_{[i]} =  (\alpha_1*\mathrm{x}_{1[i]} + \alpha_2*\mathrm{x}_{2[i]} + \alpha_3*\mathrm{x}_{3[i]} ) + \mathcal{N}(0,\sigma)  
\tag{1.8}
\]</span></p>
<p>Often, <span class="math inline">\(\varepsilon\)</span> is used to represent the random component, as in:</p>
<p><span class="math display" id="eq:9">\[
y_{[i]} = \alpha_1*\mathrm{x}_{1[i]} + \alpha_2*\mathrm{x}_{2[i]} + \alpha_3*\mathrm{x}_{3[i]}+ \varepsilon_{[i]}
\tag{1.9}
\]</span></p>
<p>Notice that the error term <em>does</em> get a, <span class="math inline">\(i\)</span> subscript, as in <span class="math inline">\(\varepsilon_{[i]}\)</span>. That is because the exact value of the error changes from trial to trial, even of the general characteristics of the error (i.e., <span class="math inline">\(\mathcal{N}(0,\sigma)\)</span>) do not.</p>
<p>When expressed in this manner, this is now a ‘regression equation’ or a ‘regression model’. ‘Fitting’ a regression model basically consists of trying to guess the most likely values of <span class="math inline">\(\alpha_1\)</span>, <span class="math inline">\(\alpha_2\)</span>, and <span class="math inline">\(\alpha_3\)</span> given our data.</p>
<p>Notice that the above formulation means that regression models do not require that our <em>data</em> be normally distributed, but only that the <em>random variation</em> in our data (<span class="math inline">\(\varepsilon\)</span>) be normally distributed. For example, in the left panel below I plot the distribution of f0 from among the entire Hillenbrand et al. data, including boys, girls, men and women. The data is not normally distributed, however, we can still use a regression based on normally-distributed data to model this as long as we expect that:</p>
<ol style="list-style-type: decimal">
<li><p>There is systematic variation in the <span class="math inline">\(\mu_{[i]}\)</span> of f0 across different groups, speakers, conditions, etc.</p></li>
<li><p>The <em>random variation</em> around these predicted values of <span class="math inline">\(\mu_{[i]}\)</span> more or less follows a normal distribution.</p></li>
</ol>
<p>In the right panel I plot the individual densities for different speaker classes. We see that although the data is not normally distributed, the within-group variation is. This suggests a regression model is appropriate for this data.</p>
<div class="figure"><span id="fig:F1-allf0s"></span>
<img src="01_files/figure-html/F1-allf0s-1.png" alt="(left) Distribution of f0 across all speakers. (right) Densities of distributions of f0 for different speaker classes: boys (red), girls (green), men (blue) and women (cyan)." width="768" />
<p class="caption">
Figure 1.11: (left) Distribution of f0 across all speakers. (right) Densities of distributions of f0 for different speaker classes: boys (red), girls (green), men (blue) and women (cyan).
</p>
</div>
</div>
<div id="whats-bayesian-about-these-models" class="section level3" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> What’s ‘Bayesian’ about these models?</h3>
<p>The major difference between Bayesian and traditional models is that Bayesian models rely on <em>posterior distributions</em> rather than likelihood functions. I am going to define some terms:</p>
<ul>
<li><p>prior probability distribution: the distribution of possible/believable parameter values <strong>prior</strong> to the <em>current</em> experiment. This <em>a priori</em> expectation can come from world knowledge, previous experiments, or some combination of the two. Before ytou measure the height of adults in San Francisco, you know the average is not 4 feet and it is not 7 feet.</p></li>
<li><p>the likelihood: this is the distribution of possible/credible parameter values given the <strong>current</strong> data and probability model, and nothing else. After you go out and measure the heights of some adults, you have more and less believable conclusions based on your data.</p></li>
<li><p>posterior probability distribution: the distribution of possible/believable parameter values you have <strong>after</strong> your current experiment. You get this by combining the prior distribution and the likelihood. Maybe you went in there thinking people in San Francisco were relatively short. After sampling many tall people you update your beliefs somewhat, adjusting these based on the new information.</p></li>
</ul>
<p>So, more traditional models focus exclusively on how likely different conclusions are given only your data, while Bayesian models combine notions of likelihood with prior beliefs. The use of prior probabilities is often said to make Bayesian models ‘subjective’ but its not really a big deal. First, every model involves arbitrary decisions which can substantially affect our results. Second, a researcher will always use common sense to interpret a model. For example, before collecting my sample I can say that I expect my female average f0 to be 200 Hz or so, but think its reasonable to expect anything from 100 to 300 Hz. Based on everything we know about human speech, even these bounds are too wide and anything outside this would suggest something is very wrong. So, even if I did not specifically assign prior probabilities to results, I would still use my expectations to ‘screen’ my results, and be very wary of anything that did not meet my expectations.</p>
<p>A Bayesian model simply requires that you build your expectations into your model. It formalizes it, makes it definable and replicable. Also, being ‘objective’ does not quite make sense in many cases. Is it really being ‘objective’ to ignore common sense and act as if a mean f0 of 250 is exactly as likely a priori as one of 20,000 Hz? In any case, in practice the prior probabilities hardly have any effect on our outcomes, and employing them allows our Bayesian models to be extremely flexible and robust.</p>
</div>
</div>
<div id="posterior-distributions" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Posterior distributions</h2>
<p>The combination of probability distributions is straightforward conceptually: you just multiply the values of the distributions at each x-axis location, and the result is the new curve. In the figure below several sets of probability distributions are combined, showing the effects of variations in priors and likelihoods. In each plot, the posterior density has been scaled so that it is the same height as the likelihood. This is only to make the figures interpretable but does not affect any of the points I make below.</p>
<p>In the top-left panel, I plot the likelihood function for <span class="math inline">\(\mu\)</span> given a sample of size 5 with a mean of 220 Hz. This is combined with a relatively weak but very different prior: the standard deviation is the same as our f0 data, however the mean is much higher (250 Hz). The curve indicating the posterior is nothing more than the product of the prior and the likelihood at each x axis location. We can see that even with only 5 data points the likelihood already dominates the posterior, though the prior distribution is exerting a pull.</p>
<p>In the top-right panel, the posterior is almost identical to the likelihood. The likelihood represents a sample of size 100, which is actually a tiny sample in experimental linguistics work where you may have 200+ samples from each of 50+ subjects and 10,000 observations overall. As you might imagine, when the sample size is that large the prior exerts almost no influence on results.</p>
<p>In the bottom-left panel we see the same likelihood as in the top-left panel based on only 5 observations. In this case the prior the prior has been made very narrow and therefore dominates the estimate. Consider a situation where we actually have really good reasons to think that the mean is 250 Hz. If we really <em>know</em> this, why would we accept and estimate of 220 Hz based on only 5 samples? In this case, the posterior distribution is basically saying: your estimate is great, but come back when you have more evidence and I might believe you.</p>
<p>In the bottom-right panel we see a situation where the likelihood and the prior are equal. Since they are about equally believable the posterior represents a perfect compromise between new and prior knowledge, forming a distribution exactly between the prior and the likelihood.</p>
<div class="figure"><span id="fig:F1-posterior"></span>
<img src="01_files/figure-html/F1-posterior-1.png" alt="Demonstration of the effect of different types of priors and likelihoods on posterior distributions." width="768" />
<p class="caption">
Figure 1.12: Demonstration of the effect of different types of priors and likelihoods on posterior distributions.
</p>
</div>
<div id="sampling-from-the-posterior" class="section level3" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> Sampling from the posterior</h3>
<p>We want to understand the posterior distribution of parameters. How do we get this information? It is difficult to get this <em>analytically</em>, that is, using exact methods and solving a bunch of equations. Many traditional methods can actually be solved in this way, and that is a big part of their popularity.</p>
<p>Understanding the characteristics of posterior probabilities is not possible analytically for many Bayesian models. As a result, these questions are answered ‘numerically’, basically by using a bunch of ‘guesses’. To understand the properties of posterior distributions, we use ‘sampling’ software that knows how to investigate these distributions.</p>
<p>You do not need to understand any of this section to use and understand Bayesian multilevel models, the software we will be using will do all of this for us. I am only presenting this to demystify them a little.</p>
<p>The way these samplers work is you specify a set of data and some relationships you think are represented in your data (i.e., a model). The sampler then ‘walks around’ the parameter space, which is the range of possible values a parameter (or set of parameters) can take. For example, for a single parameter the parameter space is a line (like the x axis in the plots above) along which the parameter varies.</p>
<p>The sampler then does some variant of the following algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>Pick a random value for the parameter (i.e., <span class="math inline">\(\mu_{tmp}\)</span> = 221 Hz).</p></li>
<li><p>Calculate the posterior probability for the current estimate of <span class="math inline">\(\mu_{tmp}\)</span>.</p></li>
<li><p>If the posterior estimate meets some criteria (e.g., it is better than the last one, it is not too low, etc.), then the value of <span class="math inline">\(\mu_{tmp}\)</span> is recorded, and becomes <span class="math inline">\(\mu_{estimate}\)</span>. If not it is just discarded.</p></li>
<li><p>Go back to step 1.</p></li>
</ol>
<p>As incredible, and magical, as it may seem, under a very reasonable set of conditions if you do the above enough times, the distribution of <span class="math inline">\(\mu_{estimate}\)</span> that results from the above process will converge on the posterior distribution of <span class="math inline">\(\mu\)</span> given your data and model structure (including prior probabilities).</p>
<p>Below I have made a small example of this process. I use the Metropolis-Hastings algorithm, which is an algorithm to sample from probability distributions. The small example below assumes the standard deviation of the population is known, and just tries to investigate the posterior distribution of <span class="math inline">\(\mu\)</span>. It uses a very broad prior distribution (<span class="math inline">\(\mu = 0\)</span>, <span class="math inline">\(\sigma = 5000\)</span>) so that it will have a very weak effect on the outcomes.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="inspecting-a-single-group-of-observations.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># the function below takes a random sample, an initial mean estimate and a fixed</span></span>
<span id="cb8-2"><a href="inspecting-a-single-group-of-observations.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># standard deviation. It then takes a certain amount of samples from the </span></span>
<span id="cb8-3"><a href="inspecting-a-single-group-of-observations.html#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># posterior distribution of the parameter, assuming a broad prior centered at 0</span></span>
<span id="cb8-4"><a href="inspecting-a-single-group-of-observations.html#cb8-4" aria-hidden="true" tabindex="-1"></a>sampler_example <span class="ot">=</span> <span class="cf">function</span> (sample, <span class="at">mu_estimate =</span> <span class="dv">0</span>, <span class="at">stdev =</span> <span class="dv">1</span>, <span class="at">nsamples =</span> <span class="dv">1000</span>){</span>
<span id="cb8-5"><a href="inspecting-a-single-group-of-observations.html#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># initial posterior calculation. This is the sum of the log likelihood and</span></span>
<span id="cb8-6"><a href="inspecting-a-single-group-of-observations.html#cb8-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># the logarithm of the prior probability.</span></span>
<span id="cb8-7"><a href="inspecting-a-single-group-of-observations.html#cb8-7" aria-hidden="true" tabindex="-1"></a>  prior <span class="ot">=</span> <span class="fu">log</span> (<span class="fu">dnorm</span> (mu_estimate[<span class="dv">1</span>],<span class="dv">0</span>, <span class="dv">500</span>))</span>
<span id="cb8-8"><a href="inspecting-a-single-group-of-observations.html#cb8-8" aria-hidden="true" tabindex="-1"></a>  loglik <span class="ot">=</span> <span class="fu">sum</span> (<span class="fu">dnorm</span> (sample, mu_estimate[<span class="dv">1</span>],stdev,<span class="at">log=</span><span class="cn">TRUE</span>))</span>
<span id="cb8-9"><a href="inspecting-a-single-group-of-observations.html#cb8-9" aria-hidden="true" tabindex="-1"></a>  old_posterior <span class="ot">=</span> loglik <span class="sc">+</span> prior</span>
<span id="cb8-10"><a href="inspecting-a-single-group-of-observations.html#cb8-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb8-11"><a href="inspecting-a-single-group-of-observations.html#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>nsamples){</span>
<span id="cb8-12"><a href="inspecting-a-single-group-of-observations.html#cb8-12" aria-hidden="true" tabindex="-1"></a>    accept <span class="ot">=</span> <span class="cn">FALSE</span></span>
<span id="cb8-13"><a href="inspecting-a-single-group-of-observations.html#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this loop will keep proposing new steps until one gets accepted. </span></span>
<span id="cb8-14"><a href="inspecting-a-single-group-of-observations.html#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> (<span class="sc">!</span>accept){</span>
<span id="cb8-15"><a href="inspecting-a-single-group-of-observations.html#cb8-15" aria-hidden="true" tabindex="-1"></a>      <span class="co"># (step 1 above)</span></span>
<span id="cb8-16"><a href="inspecting-a-single-group-of-observations.html#cb8-16" aria-hidden="true" tabindex="-1"></a>      <span class="co"># draw new proposal by randomly changing the previous mu_estimate</span></span>
<span id="cb8-17"><a href="inspecting-a-single-group-of-observations.html#cb8-17" aria-hidden="true" tabindex="-1"></a>      mu_tmp <span class="ot">=</span> mu_estimate[i<span class="dv">-1</span>] <span class="sc">+</span> <span class="fu">rnorm</span> (<span class="dv">1</span>, <span class="dv">0</span>, .<span class="dv">3</span>)</span>
<span id="cb8-18"><a href="inspecting-a-single-group-of-observations.html#cb8-18" aria-hidden="true" tabindex="-1"></a>      <span class="co"># (step 2 above)</span></span>
<span id="cb8-19"><a href="inspecting-a-single-group-of-observations.html#cb8-19" aria-hidden="true" tabindex="-1"></a>      <span class="co"># find prior probability for new mu_tmp proposal</span></span>
<span id="cb8-20"><a href="inspecting-a-single-group-of-observations.html#cb8-20" aria-hidden="true" tabindex="-1"></a>      prior <span class="ot">=</span> <span class="fu">log</span> (<span class="fu">dnorm</span> (mu_tmp,<span class="dv">0</span>, <span class="dv">500</span>))</span>
<span id="cb8-21"><a href="inspecting-a-single-group-of-observations.html#cb8-21" aria-hidden="true" tabindex="-1"></a>      <span class="co"># find log likelihood for new mu_tmp proposal</span></span>
<span id="cb8-22"><a href="inspecting-a-single-group-of-observations.html#cb8-22" aria-hidden="true" tabindex="-1"></a>      loglik <span class="ot">=</span> <span class="fu">sum</span> (<span class="fu">dnorm</span> (sample, mu_tmp,stdev,<span class="at">log=</span><span class="cn">TRUE</span>))</span>
<span id="cb8-23"><a href="inspecting-a-single-group-of-observations.html#cb8-23" aria-hidden="true" tabindex="-1"></a>      <span class="co"># calculate the new posterior probability</span></span>
<span id="cb8-24"><a href="inspecting-a-single-group-of-observations.html#cb8-24" aria-hidden="true" tabindex="-1"></a>      new_posterior <span class="ot">=</span> prior <span class="sc">+</span> loglik</span>
<span id="cb8-25"><a href="inspecting-a-single-group-of-observations.html#cb8-25" aria-hidden="true" tabindex="-1"></a>      <span class="co"># (step 3 above)</span></span>
<span id="cb8-26"><a href="inspecting-a-single-group-of-observations.html#cb8-26" aria-hidden="true" tabindex="-1"></a>      <span class="co"># if better accept always. If worse, accept sometimes</span></span>
<span id="cb8-27"><a href="inspecting-a-single-group-of-observations.html#cb8-27" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> ( ( new_posterior <span class="sc">-</span> old_posterior ) <span class="sc">&gt;=</span> <span class="fu">log</span> ( <span class="fu">runif</span> (<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>) ) ){</span>
<span id="cb8-28"><a href="inspecting-a-single-group-of-observations.html#cb8-28" aria-hidden="true" tabindex="-1"></a>        mu_estimate[i] <span class="ot">=</span> mu_tmp</span>
<span id="cb8-29"><a href="inspecting-a-single-group-of-observations.html#cb8-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># if you accept, the new estimate becomes the current estimate</span></span>
<span id="cb8-30"><a href="inspecting-a-single-group-of-observations.html#cb8-30" aria-hidden="true" tabindex="-1"></a>        old_posterior <span class="ot">=</span> new_posterior</span>
<span id="cb8-31"><a href="inspecting-a-single-group-of-observations.html#cb8-31" aria-hidden="true" tabindex="-1"></a>        accept <span class="ot">=</span> <span class="cn">TRUE</span></span>
<span id="cb8-32"><a href="inspecting-a-single-group-of-observations.html#cb8-32" aria-hidden="true" tabindex="-1"></a>      }</span>
<span id="cb8-33"><a href="inspecting-a-single-group-of-observations.html#cb8-33" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb8-34"><a href="inspecting-a-single-group-of-observations.html#cb8-34" aria-hidden="true" tabindex="-1"></a>  } </span>
<span id="cb8-35"><a href="inspecting-a-single-group-of-observations.html#cb8-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span> (mu_estimate)</span>
<span id="cb8-36"><a href="inspecting-a-single-group-of-observations.html#cb8-36" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>In the plots below, I show this algorithm at work. In the top row, a random sample with a mean of -50 is used. You can see that the sampler starts at 0 but quickly finds the sample mean (left column). In the middle, I show the distribution of the samples on the left, minus the section it uses to ‘find’ the parameter value (called the ‘burn-in phase’). On the right, I compare our samples (blue) to the theoretical posterior distribution for the mean given the data and prior (red). I toss out the samples during the ‘burn in’ phase, as there are used up in trying to ‘find’ the correct location in the parameter space.</p>
<p>In the bottom row, I use this algorithm on our f0 data! This is a ‘Bayesian’ analysis since it combines information about parameter likelihood and prior probabilities. We can also see that even this simple approach yields a good correspondence to the theoretical posterior distribution of the parameter, and results in broadly the same conclusions we have arrived at by other means.</p>
<div class="figure"><span id="fig:F1-mcmc"></span>
<img src="01_files/figure-html/F1-mcmc-1.png" alt="Demonstration of parameter estimation using a random walk, yielding a good approximation of analytically-derived values." width="768" />
<p class="caption">
Figure 1.13: Demonstration of parameter estimation using a random walk, yielding a good approximation of analytically-derived values.
</p>
</div>
<p>The results clearly coincide, but aren’t perfect. But this sampler isn’t very sophisticated! The samplers we will be using in this class <em>do</em> provide an excellent match to the posterior distribution. As a result, we can inspect the distribution of collected <span class="math inline">\(\mu_{estimate}\)</span> to understand the posterior of our parameter. We can use these distributions in the same way that we used the theoretical likelihood functions above, by using them to make statements about likely parameter values and ranges of values.</p>
</div>
</div>
<div id="exercises" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Exercises</h2>
<p>There is no modeling in this chapter, and the focus is on presenting some fundamental concepts. In Chapter 2 we will begin fitting Bayesian multilevel models to data. This book does not really go into very much detail about the usage of R, and it instead assumes a basic familiarity with the software.</p>
<p>In place of statistical exercises, the time devoted to this chapter can be used to make sure that the reader (or student or class) has a reasonable understanding of R and a basic familiarity with the important data structures in R (e.g., vectors, dataframes), and some of the more basic functionalities of the software.</p>

</div>
</div>
<!-- Default Statcounter code for statsbook
https://santiagobarreda.github.io/stats-class/ -->
<script type="text/javascript">
var sc_project=12454226; 
var sc_invisible=1; 
var sc_security="a1959418"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12454226/0/a1959418/1/"
alt="Web Analytics"></a></div></noscript>
<!-- End of Statcounter Code -->
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
