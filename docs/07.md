
# Logistic regression 

In this chapter we're going to talk about the prediction of categorical variables. These are variables that can take on a (usually small) number of discrete values. We're only going to talk about dichotomous (i.e. binary) outcomes for now, though the same ideas can be extended to model ordinal (ordered categories such as in 1st, 2nd, 3rd), and multinomial data (unordered categories such as English, French and Spanish). 

## Data and research questions

We're going to use the data from our perceptual experiment discussed in Chapter 6. Listeners were presented with monosyllabic words, presented at random. For each trial, listeners reported the height of the speaker (in feet and inches) and guessed whether the speaker was a boy, girl, man or woman. 



```r
library (brms)
options (contrasts = c("contr.sum","cont.sum"))
## source data
url1 = "https://raw.githubusercontent.com/santiagobarreda"
url2 = "/stats-class/master/data/h95_experiment_data.csv"
h95 = read.csv (url(paste0 (url1, url2)))
## set up colors for plotting
devtools::source_url (paste0 (url1, "/stats-class/master/data/colors.R"))
## source functions
devtools::source_url (paste0 (url1, "/stats-class/master/data/functions.R"))
```



In chapter 6 we saw that there is a relationship between perceived height and f0. The final model we considered in Chapter 5 suggested that this relationship may vary in a gender-dependent manner. In addition to size, listeners reported the age group (adult or child) and gender (female or male) of the speaker. 

Below, we see average reported height for each speaker as a function of f0, separately for males and females. We also see the probability of observing a classification of 'adult' (i.e., 'man' or 'woman') for the speaker. Clearly, the perception of size and the perception of adultness are related. This is not surprising as, in general, adultness and size are related. Broadly speaking, larger speaker are more likely to be adults. 


<div class="figure">
<img src="07_files/figure-html/F7-1-1.png" alt="(left) Average perceived height for each speaker as a function of average f0. Boys in red, girls in yellow, men in green and women in teal. (middle) The same relationship as the left but only males are plotted. (right) Only females are plotted." width="768" />
<p class="caption">(\#fig:F7-1)(left) Average perceived height for each speaker as a function of average f0. Boys in red, girls in yellow, men in green and women in teal. (middle) The same relationship as the left but only males are plotted. (right) Only females are plotted.</p>
</div>

There are clear similarities between the two sets of relationships, but also clear differences. When we modeled a normally distributed variable, we were predicting the mean but also the outcomes. What I mean by that is that the mean is a perfectly reasonable observation to be predicted (the most likely one in fact!). 

<div class="figure">
<img src="07_files/figure-html/F7-2-1.png" alt="(left) Average adultness classifications for each speaker as a function of average f0. Boys in red, girls in yellow, men in green and women in teal. (middle) The same relationship as the left but only males are plotted. (right) Only females are plotted." width="768" />
<p class="caption">(\#fig:F7-2)(left) Average adultness classifications for each speaker as a function of average f0. Boys in red, girls in yellow, men in green and women in teal. (middle) The same relationship as the left but only males are plotted. (right) Only females are plotted.</p>
</div>

Adultness is a binary variable with outcomes of 0 (child, "not adult") and 1 (adult) only. What are we predicting when we predict a binary variable using a line? The line in the figure above goes from 0 to 1 but only exactly 0 and 1 (and nothing in between) are possible outcomes. There is no 0.4 child outcome being predicted!

We can instead say that we are modeling the probability of observing a certain outcome, instead of the outcome itself. So, we could model the continuous probability of observing an 'adult' response using a line. However, this doesn't  work well because the prediction line will predict values outside of 0 and 1, which doesn't work for probabilities. 

I mentioned in the first chapter that our models are like spherical models for billiards balls: not exactly right but close enough for many purposes. If you *do* just model $p$ with a line like in the Figure above, your model will likely *not* be close enough for many purposes, potentially leading to major problems. 

For these reasons, we don't actually use linear models with Gaussian (normally-distributed) errors when modeling dichotomous variables. Instead, we use logistic regression, which is specifically intended to model binary outcomes. 

In this chapter, we will use a logistic regression model to investigate the perception of adultness from f0 acoustics, and the way that this may vary in a gender-dependent manner.


### Dichotomous variables and data

Dichotomous variables are ones that can take on only one of two possible discrete outcomes. We can easily think of many examples of this kind of data, perhaps the most obvious being something that is wrong or right. 

When you can only have two outcomes, you can arbitrarily set one to equal a 'success'. Keep in mind this is not a judgment of any kind, and the other category could be chosen with no important change to your model (all coefficient signs flip, that's it). 

After we pick a category to be a success we then make all instance of that category equal to 1 and the other category equal to 0. This means we can take all observations of the variable and find the average to calculate the probability of a success. This is actually very obvious: If I am practicing basketball and add up all my made baskets (1 point each) and divide by the total number of baskets (thereby averaging), I will clearly get the percent (or probability) of baskets made overall. 

There are two main distributions used to model dichotomous variables: the binomial distribution and the Bernoulli distribution. 

  * The binomial distribution models a batch of multiple dichotomous outcomes. This distribution has two parameters: the probability of a success ($p$) and the number of trials ($n$). So for example, if I took 5 free throws and missed 2 (3/5=0.6), then I say the data is likely to come from a binomial distribution with a $p$ parameter of 0.6 and an $n$ parameter of 5. If I use this distribution, I am treating all 5 trials as a single observation.  
  
  * The Bernoulli distribution models individual dichotomous outcomes. This distribution has only one parameter: the probability of a success ($p$). In this case, if I took 5 free throws and missed 2 (3/5=0.6), I would still describe the data using a $p$ parameter of 0.6. However, with this distribution, I would model each 5 trial as a separate observation so there is no $n$ parameter (it is always 1).   

Below we generate random binomial variables (R doesn't specifically make Bernoulli variables). The `rbinom` function takes parameters in this order `number of observations, batch size, probability of success`. Below, I generate first a single Bernoulli variable (a [Bernoulli trial(https://en.wikipedia.org/wiki/Bernoulli_trial)]), and then ten variables with the same probability of success. 


```r
# a single trial, probability of 0.5
rbinom (1,1,.5)
## [1] 0

# ten single trials, probability of 0.5
rbinom (10,1,.5)
##  [1] 1 1 0 1 0 0 0 0 0 0
```

Below we compare the data generated by the Bernoulli and the Binomial distributions. In the to row we get a single number, the total number of successes in the trials. We don't get any information about what happened on any individual trial. In the bottom row we do get information about what happened on each individual trial.  


```r
# a single batch of 10 trials, probability of 0.5
rbinom (1,10,.5)
## [1] 2

# ten individual trials, probability of 0.5
rbinom (10,1,.5)
##  [1] 0 1 0 1 1 1 0 1 0 1
```

The $p$ parameter must be estimated using the observations generated by the binomial and Bernoulli distributions. Unlike the normal distribution, these distributions do not directly generate data with values near their mean. Instead, they generate sequences of 1s and 0s, the average of which (as the number of observations approach infinity) is expected to [converge](https://en.wikipedia.org/wiki/Consistent_estimator) on the probability of the distribution that generated the data. For example, below I generate sequences of a dichotomous variable with a $p$ parameter of 0.5. In each case, the estimate of the probability of the distribution gets closer as the length of the sample gets longer. 



```r
set.seed (1)
mean (rbinom (10,1,.5)) # the mean of 10 observations
## [1] 0.6
mean (rbinom (100,1,.5))  # the mean of 100 observations
## [1] 0.47
mean (rbinom (1000,1,.5))  # the mean of 1000 observations
## [1] 0.481
mean (rbinom (100000,1,.5))  # the mean of 100000 observations
## [1] 0.50009
```

So, when we have a binary outcome variable and are talking about individual trials, we model it as being generated by a Bernoulli distribution. This distribution has a single parameter $p$ so that our data model is $adult \sim \mathrm{Bernoulli} (p)$. Unlike normally distributed data, there are only two outcomes (0 and 1) and variation in $p$ is bound to be between 0 and 1. If we are modeling multiple Bernoulli trials as a single observation, we can use the binomial distribution and our data model would be $adult \sim \mathrm{Bernoulli} (p, n)$ where $p$ is again bound to be between 0 and 1, and $n$ must be a potivie integer.

## Generalizing our linear models

The [general linear model](https://en.wikipedia.org/wiki/General_linear_model) is the extension of the same general design used in 'regular' regression (featuring normally distributed errors) to models for other kinds of variables. 

Our regression models have consisted of a bunch of component parts stuck together. We can think of two general parts, introduced early on: the random components and the systematic component. We can consider a model that seeks to predict perceived height from f0. Our model for this relationship is:

`pheight ~ g0_s + (g0_s|subj) + (1|speaker)`

We know that this model is basically this:

\begin{equation}
\begin{split}
pheight_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{error}) \\
\mu_{[i]} = a_{[i]} + b_{[i]} * \mathrm{x}_{[i]}  \\ 
...
\end{split}
(\#eq:71)
\end{equation}

These two parts are called the *random component* and the *systematic component* respectively. The random component specifies how our data randomly varies given some parameter ($\mu$ for normal distributions). The systematic component predicts variation in the parameter of interest using shapes like lines and planes. 

Generalized linear models also consist of what is know as *link functions*. We haven't dealt with this at all yet because the normal distribution does not need one. For normally distributed data we directly model variation in mu so there is no need for a link function. We could add a link function in between the random and systematic components as below:

\begin{equation}
\begin{split}
pheight_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{error}) \\
\mu_{[i]} = \theta_{[i]} \\
\theta_{[i]}  = a_{[i]} + b_{[i]} * \mathrm{x}_{[i]}  \\ 
...
\end{split}
(\#eq:72)
\end{equation}

Regression models with normally distributed errors use what is called the "identity" link function, which basically means that they model a parameter that directly translates into the $\mu$ parameter. This is equivalent to modeling the $\mu$ parameter directly in our prediction equation. 

Why can we directly model variation in $\mu$ with our prediction equation? The key is that normally distributed data is *supposed* to be continuous and extend from positive to negative infinity. Of course this is only in principle, but it still means that *in principle*, you *can* model $\mu$ using lines since this might actually vary continuously along them. 

WIn contrast, the probability of an event occurring must be between 0 and 1, so it obviously does not extend to infinity. As a result, even if we *did* switch to the Bernoulli distribution we could not just do something like this:

\begin{equation}
\begin{split}
adult_{[i]} \sim \mathrm{Bernoulli}(p_{[i]}) \\
p_{[i]} = \theta_{[i]} \\
\theta_{[i]}  = a_{[i]} + b_{[i]} * \mathrm{x}_{[i]}  \\ 
\end{split}
(\#eq:73)
\end{equation}

Because this would cause weird things like predictions with negative probabilities. Instead, we need to employ an actual link function in the second step above (e.g., $f(\theta_{[i]})$), in order to turn $p$ into something we can model using lines. 

### Link functions

A link function allows you to *link* variation in a parameter to variation in straight lines. Modeling becomes a three step process like this:

  1) Predict variation of some parameter along straight lines (or related shapes), for example $\theta_{[i]}  = a_{[i]} + b_{[i]} * \mathrm{x}_{[i]}$.

  2) Transform the parameter using a link function ($p_{[i]} = f(\theta_{[i]})$). 
  
  3) Use the *transformed* parameter in data generating distribution (e.g., $adult_{[i]} \sim \mathrm{Bernoulli}(p_{[i]})$)
  
For dichotomous variables, it's common to predict outcomes in units called *logits* (step 3). The logits are then turned into probabilities ($p$) using the [logistic link function](https://en.wikipedia.org/wiki/Logistic_function#Mathematical_properties) (step 2). The parameter $p$ can the be used together with an appropriate probability distribution to model the data. 


### Logits 

Logits are log-odds, the logarithm of the odds of a success. The odds of a success is defined as:

`odds = total number of successes / total number of failiures`

Odds of 3/1 indicate that success is three times as likely as a failure. We can turn this into a probability with the following calculation:

`probability = total number of successes / (total number of successes + total number of failiures`)

So, odds of 3/1 implies a probability of 0.25 (3 / (3+1)). Odds are still bounded by zero so they don't work for linear modeling, but if we take the *logarithm* of the odds we get a logit: a value that extends continuously from positive to negative infinity. This is because logarithms represent the space from 0 to 1 with values from -∞ to 0, and values from 1 to +∞ with values from 0 to +∞. 

We can calculate the logit ($z$) with either of the two equivalent calculations:

`logit = log (probability of successes / probability of failiures)`

`logit = log (probability of successes) - log (probability of failiures)`

I wrote a function that calculates logits from probabilities (`ptoz`) seen below. Note that I have a special case for situations where $p$ is equal to 0 or 1, where I arbitrarily change those to 0.99 and 0.01. Since the log(0)=-∞, the logit of probabilities of 0 and 1 and positive and negative infinity respectively. One way to think about this is is that to really know that something is 1 and not, for example 0.9999...., you would have to observe it infinitely long. This would require you to observe an infinite number of successes with no failures, leading to infinite odds (successes/failures) and log odds. Infinite logit values are not useful for us (and can't be plotted), and so the `ptoz` function sets extreme (but manageable) values for probabilities of 0 and 1. 


```r
## changes probabilities (p) to logits
ptoz = function (p){
  p[p==1] = .99  # if p=1, change to 0.99
  p[p==0] = .01  # if p=0, change to 0.01 (i.e. 1-0.99)
  log (p) - log(1-p)
}
```

Our logistic regression models are going to predict **logits**. Our lines will describe continuous changes in logits. Our intercepts will describe shifts on the values of logits across several categories. Given a predicted value expressed in logits, we can then use the *logistic function* (sometimes called the *antilogit function*) to convert this value into a probability. For this reason, the logistic function is said to be the link function for logistic regression: the logistic function links the prediction made by our model (in logits) to the parameter used in the data-generating function assumed by our model (the $p$ in a Bernoulli distribution). 

### The logistic link function

The logistic link function looks like below, for values of $z$ ranging continuously from positive to negative infinity, where $e$ is the [mathematical constant used for natural logarithms](https://en.wikipedia.org/wiki/Natural_logarithm).  

$$
f(z) = \frac{1}{1 + e^{-z}}
(\#eq:74)
$$



I've written this simple R function that implements the function:


```r
## changes logits to probabilities.
ztop = function (z) 1 / (1 + exp(-z)) 
```

The function may look complicated but its a bit simpler when you remember than any number raised to the power of zero is 1. So $e^0=1$, meaning that when $z=0$, $\frac{1}{1 + e^{0}}=\frac{1}{1 + 1}=0.5$. When $z$ is a positive value, $e^{-z}$ will be a fraction between 0 and 1, for example if $z=3$ then $e^{-z}=e^{-(3)}=0.05$. This means that when $z<0$, we expect probabilities between 0.5 and 1, as in:

$$
\frac{1}{1 + e^{-3}}=\frac{1}{1 + 0.05}=0.95
(\#eq:75)
$$

On the other hand, when $z$ is negative, $e^{-z}$ will be a positive number greater than 1, as in $e^{-z}=e^{-(-3)}=e^{3}=20.1$. Thus, for negative values of $z$ we expect probabilities between 0 and 0.5 as in:

$$
\frac{1}{1 + e^{3}}=\frac{1}{1 + 20.1}=0.05
(\#eq:76)
$$

Below I draw a line with a slope of one and an intercept of 0 (i.e., y = x). We can imagine that this line defines expected changes in *logits* as a function of some predictor x. For example, where we might directly model change in perceived height (in inches) as a function of continuous variation in f0, we model the change in the *logit* of the probability of observing an adult response, as a function of f0. 

In the middle panel, we've applied the logistic function to the line, resulting in a [sigmoid curve](https://en.wikipedia.org/wiki/Sigmoid_function). If we calculate logits for this sigmoid curve in the middle, the result is our original line on the left. We can see that the logit and sigmoid functions/transformations are just in the inverse of each other. They can be applied to data to go back and forth between a probability or logit interpretation. 


<div class="figure">
<img src="07_files/figure-html/F7-3-1.png" alt="(left) A plot of a line with a slope of 1 and intercept of 0. We can treat the y axys as logits. (middle) The result of applying the logistic function to every point of the line in the left column. (right) Calculating the logit of each value specified on the curve in the middle turns our sigmoid curve back to a line." width="768" />
<p class="caption">(\#fig:F7-3)(left) A plot of a line with a slope of 1 and intercept of 0. We can treat the y axys as logits. (middle) The result of applying the logistic function to every point of the line in the left column. (right) Calculating the logit of each value specified on the curve in the middle turns our sigmoid curve back to a line.</p>
</div>

In chapter 5 I mentioned that when you do a linear regression you model the data as normal distributions sliding along a line, generating normally distributed data along the line as they move. In logistic regression you model the data as a Bernoulli distribution sliding along the sigmoid curve above, generating 1s and 0s as it modes. This is accomplished by modeling variation in logits ($z$) using our regression models, and only converting to a probability ($p$) after prediction. 

In other words, our model consists of the three elements seen below: the data generation (random component), the link function, and our prediction equation (systematic component). 

$$
adult \sim \mathrm{Bernoulli}(p) \\
p = \mathrm{logistic} (z) \\
z  = a + b * \mathrm{x}  \\ 
(\#eq:762)
$$

Keep in mind that we could put the prediction equation directly inside the logistic function:

$$
p = \frac{1}{1 + e^{-(a + b*x)}}
(\#eq:77)
$$

And even put the output of that inside the Bernoulli distribution directly.

$$
adult \sim \mathrm{Bernoulli}(\frac{1}{1 + e^{-(a + b*x)}}) \\
(\#eq:78)
$$

When you look at it this way, it shows that our link function really does *link* our prediction equation (a + b*x) and our data distribution (Bernoulli). The fomulation above is reminiscent of this formulation of or model for normally distributed data:

$$
pheight \sim \mathrm{Normal}(a + b*x, \sigma) \\
(\#eq:79)
$$


### Building intuitions about logits

The logit (and logistic) functions are *non linear* functions. In the simplest sense this means that they take lines and make them into non lines, and vice versa. This can be seen below where the left and right panels present the equivalent relation in logits and probabilities. 

Another consequences of the non-linearity of the transformation is that a given unit increase in probabilities does not equal a given unit increase in logits. The reverse is also true. This can clearly be seen in the figure below which has horizontal lines from 0.1 to 0.9, spaced every 0.1 units of probability. We see that this reads to equally-spaced lines on the right, but not on the left. 


<div class="figure">
<img src="07_files/figure-html/F7-4-1.png" alt="(left) A line relating some predictor to logits. (right) A sigmoid curve expressing the probability associated with each value along the line in the left." width="768" />
<p class="caption">(\#fig:F7-4)(left) A line relating some predictor to logits. (right) A sigmoid curve expressing the probability associated with each value along the line in the left.</p>
</div>

We can easily check what the logit values are for a set of probabilities using the `ptoz` function. The top row below contains a sequence of probabilities and the bottom row shows equivalent logits.  


```r
rbind ( (seq (0,1,.1)), 
        round ( ptoz (seq (0,1,.1)) , 3) )
##        [,1]   [,2]   [,3]   [,4]   [,5] [,6]  [,7]  [,8]  [,9] [,10] [,11]
## [1,]  0.000  0.100  0.200  0.300  0.400  0.5 0.600 0.700 0.800 0.900 1.000
## [2,] -4.595 -2.197 -1.386 -0.847 -0.405  0.0 0.405 0.847 1.386 2.197 4.595
```

Notice that the difference between 0.5 and 0.6 is 0.4 logits, but the difference between 0.7 and 0.9 is about 0.8 logits. Meanwhile the difference between 0.9 and 1 is infinity! 

There is actually a perfectly logical reason for this, and it affects how we think about our logits. Imagine you sink 500/1000 free throws, giving you a 0.5 probability of success. Now imagine you take a further 100 and sink them all. Now your probability of success is 600/1100, meaning your amazing streak has increased your probability to 0.54. However, suppose that you had been a 900/1000 shooter, with a probability of 0.9. If you had the same streak of 100 baskets, you would only have increased your probability to 90.1. 

So, we see that 'the same' increase results in a large increase in one probability (0.5 -> 0.54, almost 10%) and a minuscule change in another (0.9 -> 0.901, about 1%). Basically, as you approach 0 and 1 it gets harder to make large changes in your probabilities. The result of this is that a given logit difference will equal a large probability difference near 0 logits, and a smaller probability difference as the underlying logit values is larger in magnitude. 

Since logit values can seem a bit wacky at first, here are some useful things to keep in mind. 

  * 50% is 0. Positive means more likely to be a success, negative means more likely to be a failure. 
  
  * -3 and 3 are 4.7% and 95.2%. Basically -3 and 3 logits are useful bounds for "almost always" and almost never". 
  
  * Logits far beyond 3 might not have much practical significance. A logit of 6 is 99.7%, a 2.5% increase over 3 logits. The logits have doubled but the probability has barely changed for most purposes. Also, it is very difficult to distinguish 95% and 99% in practice since by definition, you will be observing very few mistakes to distinguish the two!
  
  * Effects can be considered important or not based on how far they got you along -3 to 3 (or -4 to 4). Basically, anything in the +1 range matters, while effects smaller than 0.2 or so are likely having only small effects on outcomes.
  
Here is one final thing to keep in mind: you **must** consider your model effects as logits **before** transforming them into probabilities. This important constraint follows directly from the fact that a given logit difference can lead to varying differences for different probability values. 

For example, imagine that we have an intercept of 1 and an effect of 2 for some group, expressed in logits. This results in an expected probability of:


```r
ztop (1 + 2)
## [1] 0.9525741
```

For that group. In contrast, if we had first converted to logits and tried to add after:


```r
ztop (1) + ztop(2)
## [1] 1.611856
```

We get something that is not even a valid probability. 

## Logistic regression with one predictor

We're going to fit a model to our data that predicts the perception of adultness using logistic regression.

### Description of the model

The model would be quite familiar to use if it were dealing with normally distributed data. The formula for the model we are using looks like this:

`adult ~ g0_s*pfemale + (g0_s*pfemale|subj) + (1|speaker)`

We are going to use scaled log-f0 (g0_s) instead of just the centered version of the predictor. When we scale predictors we first center them and then divide by the standard deviation. This results in slope coefficients relating to a standard deviation of variation in the predictor (rather than a 1 unit change). 

Scaling is very useful for logistic regression because it make most parameters have small values (<10 or so) by making the range of any predictor mostly fall within -4 and 4. If you don't scale your continuous predictors, you can end up with slope coefficients like 200 when the predictor has very large values. Since we know that 3 logits is already 95%, coefficient values as large as 200 are difficult to interpret quickly. 

The formula above says tells our model to predict adultness using scaled log f0 (`g0_s`), information about whether the speaker was identified as female or male, and the gender-dependent use of f0. It allows for random by-subject effects for all predictors and random intercepts for speaker. We need to add one important parameter to our `brm` function call:  

`family="bernoulli"`

That tells `brm` that our model is analyzing Bernoulli distributed data, and so `brm` knows to use the logistic link function. Our full model specification is now:


\begin{equation}
\begin{split}
\textrm{Likelihood:} \\
adult_{[i]} \sim \mathrm{Bernoulli}(p_{[i]}) \\
p_{[i]} = \mathrm{logistic} (z_{[i]}) \\
z_{[i]} = a_{[i]} + b_{[i]} * \mathrm{x}_{[i]}  \\ 
a_{[i]} = Intercept + pfemale_{[i]} + \alpha_{[\mathrm{subj}_{[i]}]}  +  \alpha_{pfemale[\mathrm{subj}_{[i]}]}+ \alpha_{[\mathrm{speaker}_{[i]}]}  \\
b_{[i]} =  g0\_c + g0\_s \colon pfemale_{[i]} + \beta_{[\mathrm{subj}_{[i]}]} + \beta_{g0\_s \colon pfemale[\mathrm{subj}_{[i]}]} \\ \\

\textrm{Priors:} \\
\alpha_{speaker} \sim \mathrm{Normal}(0,\sigma_{speaker}) \\ \\  

\begin{bmatrix} \alpha_{subj} \\ \alpha_{pfemale,subj} \\ \beta_{subj} \\ \beta_{g0\_s \colon pfemale,subj} \\ \end{bmatrix}	
\sim \mathrm{MVNormal} ( \begin{bmatrix} 0\\ 0 \\ 0 \\ 0 \\ \end{bmatrix}, \Sigma) \\ \\

Intercept \sim t(3, 0, 5) \\
g0\_c \sim t(3, 0, 3) \\
pfemale \sim t(3, 0, 3) \\
g0\_c \colon female \sim t(3, 0, 3) \\ \\

\sigma_{\alpha_{speaker}} \sim t(3, 0, 4) \\ 
\sigma_{\alpha_{subj}} \sim t(3, 0, 4) \\ 
\sigma_{\alpha_{pfemale,subj}} \sim t(3, 0, 4) \\ 
\sigma_{\beta_{g0\_s \colon pfemale, subj}} \sim t(3, 0, 4) \\ 
\sigma_{\beta_{subj}} \sim t(3, 0, 4) \\ 
R \sim \mathrm{LKJCorr} (2)
\end{split}
(\#eq:710)
\end{equation}



> We are treating our adultness judgments (1 or 0 for adult and child) as coming from a Bernoulli distribution with a probability that varies trial to trial. The *logit of the probability* (z) varies along lines. The lines are specified by intercepts and slopes that vary trom trial to trial, and there is a single continuous predictor (g0_s). The intercept of these lines vary based on an overall intercept (the main effect), an overall effect for the perception of a female speaker (pfemale), subject-specific deviations from the mean, subject-specific deviations from the female effect, and speaker-specific deviations from the mean. The slope of these lines vary based on an overall slope (the main effect), subject-specific deviations from the average slope, and subject-specific deviation from the slope based on whether the speaker was identified as female or not. 

> The speaker intercepts were drawn from a normal distribution with a mean of zero and a standard deviation estimated from the data. The subject random effects were drawn from a normal distribution with means of 0 of zero and a covariance matrix estimated from the data. All other effects (e.g., the Intercept, g0_s, etc.) were treated as 'fixed' and drawn from prior distributions appropriate for their expected range of values (e.g., subj ~ t(3,0,3)). 


That model specification is pretty long, and our models are not even *that* complicated yet. Going forward I'm going to start skipping some or all of the model specification, and provide a verbal description of our models based on the `brm` function calls we use to fit them. We've already covered all the components of these models so you can always look back at previous chapters to see how to 'translate' model formulas to their model specifications. 

For example, here is the function call we need to run the model described above:


```r
logistic_g0 =
  brm (adult ~ g0_s*pfemale + (g0_s*pfemale|subj) + (1|speaker), data=h95, 
       chains=4, cores=4, family="bernoulli", 
       warmup=1000, iter = 11000, thin = 10, control = list(adapt_delta = 0.95), 
       prior = c(set_prior("student_t(3, 0, 5)", class = "Intercept"),
                 set_prior("student_t(3, 0, 3)", class = "b"),
                 set_prior("student_t(3, 0, 4)", class = "sd"),
                 set_prior("lkj_corr_cholesky (2)", class = "cor")))
```

Every piece of information in the model description above is predictable from this function call. The model structure is predictable from the model formula. The fact that we are using a logistic link is evident from the `family` parameter. Even all of the priors can be understood from the call if we remember that: 

  * `class = "Intercept"` sets the prior only for the intercept. 

  * `class = "b"` sets the prior only for all 'fixed effect' (i.e. what `brm` calls "population level") predictors, other than the intercept.
  
  * `class = "sd"` sets the prior for standard deviation parameters for our 'random' effects.
  
  * `class = "corr"` sets the prior for correlation matrices needed to build the covariance matrix for our multivariate normal distributions (discussed in chapter 6).
  
### Fitting the model

I fit the model below, except I am just going to use a standard deviation of 3 for everything. These distributions have most of their mass between -9 and +9, which are relatively large values for logits. In general we expect factors and scaled continuous predictors to mostly have values under <10, or for values under 10 to be most meaningful to us.


```r
h95$g0_s = (log(h95$f0)-mean(log(h95$f0))) / sd (log(h95$f0))
h95$adult = as.numeric(h95$group %in% c('m','w'))
h95$padult = as.numeric(h95$pgroup %in% c('m','w'))
h95$female = 2*(as.numeric(h95$group %in% c('g','w')) - 0.5)
h95$pfemale = 2*(as.numeric(h95$pgroup %in% c('g','w')) - 0.5)
h95$group = factor(h95$group)
h95$pgroup = factor(h95$pgroup)
```


```r
## Fit the model yourself, or
## download pre-fit model from: 
## github.com/santiagobarreda/stats-class/tree/master/models
## and load after placing in working directory
## logistic_g0 = readRDS ('7_logistic_g0.RDS')

set.seed (1)
logistic_g0 =
  brm (adult ~ g0_s*pfemale + (g0_s*pfemale|subj) + (1|speaker), data=h95, 
       chains=4, cores=4, family="bernoulli", 
       warmup=1000, iter = 11000, thin = 10, control = list(adapt_delta = 0.95), 
       prior = c(set_prior("student_t(3, 0, 3)", class = "Intercept"),
                 set_prior("student_t(3, 0, 3)", class = "b"),
                 set_prior("student_t(3, 0, 3)", class = "sd"),
                 set_prior("lkj_corr_cholesky (2)", class = "cor")))

## save model
saveRDS (logistic_g0, '7_logistic_g0.RDS')
```


### Interpreting the model

We can look at the model printout below. The estimates and credible intervals are being expressed in logits, however, most of this model output is the same as it was before with two noteworthy differences. First, the top of the model now indicates `Family: bernoulli` and `Links: mu = logit`. Second, notice the absence of the `Family-Specific` parameter section of the model where `sigma` (i.e., $\sigma_{error}$) was usually found.


```r
logistic_g0
```

```
##  Family: bernoulli 
##   Links: mu = logit 
## Formula: padult ~ g0_s * pfemale + (g0_s * pfemale | subj) + (1 | speaker) 
##    Data: h95 (Number of observations: 2780) 
## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~speaker (Number of levels: 139) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     2.76      0.33     2.18     3.49 1.00     3176     3529
## 
## ~subj (Number of levels: 10) 
##                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)                   1.32      0.41     0.73     2.35 1.00     3943
## sd(g0_s)                        1.37      0.42     0.74     2.34 1.00     3888
## sd(pfemale)                     0.27      0.22     0.01     0.81 1.00     3707
## sd(g0_s:pfemale)                1.28      0.46     0.59     2.37 1.00     4154
## cor(Intercept,g0_s)            -0.45      0.26    -0.85     0.14 1.00     3747
## cor(Intercept,pfemale)          0.14      0.37    -0.60     0.77 1.00     3803
## cor(g0_s,pfemale)              -0.12      0.36    -0.74     0.60 1.00     4109
## cor(Intercept,g0_s:pfemale)     0.10      0.30    -0.48     0.65 1.00     3836
## cor(g0_s,g0_s:pfemale)         -0.48      0.27    -0.87     0.12 1.00     4009
## cor(pfemale,g0_s:pfemale)      -0.02      0.36    -0.69     0.66 1.00     4201
##                             Tail_ESS
## sd(Intercept)                   3850
## sd(g0_s)                        3814
## sd(pfemale)                     3888
## sd(g0_s:pfemale)                3964
## cor(Intercept,g0_s)             4141
## cor(Intercept,pfemale)          3974
## cor(g0_s,pfemale)               3688
## cor(Intercept,g0_s:pfemale)     3968
## cor(g0_s,g0_s:pfemale)          3464
## cor(pfemale,g0_s:pfemale)       3927
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept       -0.67      0.54    -1.72     0.42 1.00     3304     3620
## g0_s            -3.31      0.59    -4.54    -2.18 1.00     3839     3873
## pfemale          2.30      0.24     1.85     2.79 1.00     3486     3523
## g0_s:pfemale     1.18      0.54     0.15     2.31 1.00     4078     3971
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

Our model is basically three lines: the overall (main effects) line, the line for when subjects indicated hearing a male speaker, and the line for for subjects indicated hearing a female speaker. We recover the parameters for these lines by adding the appropriate terms using the hypothesis function:


```r
logistic_g0_hypothesis = 
  hypothesis (logistic_g0,
              hypothesis = 
                c("Intercept = 0", ## overall intercept
                  "Intercept + pfemale = 0", ## female intercept
                  "Intercept - pfemale = 0", ## male intercept
                  "g0_s = 0", ## overall slope
                  "g0_s + g0_s:pfemale = 0", ## female slope
                  "g0_s - g0_s:pfemale = 0") ## male slope
  )
```

```r
logistic_g0_hypothesis[[1]][,2:5]
```

```
##     Estimate Est.Error   CI.Lower   CI.Upper
## 1 -0.6655764 0.5423298 -1.7190876  0.4205114
## 2  1.6375637 0.6114904  0.4891297  2.9094686
## 3 -2.9687164 0.5745495 -4.1385524 -1.8568979
## 4 -3.3056271 0.5946181 -4.5416908 -2.1776848
## 5 -2.1243535 0.7199209 -3.5391104 -0.6766319
## 6 -4.4869007 0.8802750 -6.3235852 -2.8792430
```

We can use these parameters to plot lines predicting the logit of the probability of a male response given f0. Below, we see these lines overall, and separately for voices judged to be male or female. A comparison of each prediction line to the classifications of the voices in our model show that this model offers reasonable predictions of the trends in the data.

<div class="figure">
<img src="07_files/figure-html/F7-5-1.png" alt="(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted." width="768" />
<p class="caption">(\#fig:F7-5)(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted.</p>
</div>

Below we see the result of applying the logistic transform on the lines above. The result is sigmoid curves that represent expected variation in the $p$ parameter of a Bernoulli distribution as a function of f0. This variation is along lines in the sigmoid space but not in the probability space. Obviously, these curves are a much better fit for the data than the lines we originally used in the first figure in the chapter.    

<div class="figure">
<img src="07_files/figure-html/F7-6-1.png" alt="(left) Curve indicating the relationship between f0 and the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted." width="768" />
<p class="caption">(\#fig:F7-6)(left) Curve indicating the relationship between f0 and the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted.</p>
</div>

## Answering our research question

In the beginning of the chapter, we considered a pretty broad research question: how is f0 used by listeners to determine adultness, possibly in a gender-dependent manner? In this final section we're going to try to answer this question using the information presented in our model below. 

Keep in mind, I don't have a 'right answer' or correct interpretation of the data that I am working towards. This is just an example of the way you might approach understanding real data in order to see what you can learn about your research question. 


```r
logistic_g0
```

```
##  Family: bernoulli 
##   Links: mu = logit 
## Formula: padult ~ g0_s * pfemale + (g0_s * pfemale | subj) + (1 | speaker) 
##    Data: h95 (Number of observations: 2780) 
## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~speaker (Number of levels: 139) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     2.76      0.33     2.18     3.49 1.00     3176     3529
## 
## ~subj (Number of levels: 10) 
##                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)                   1.32      0.41     0.73     2.35 1.00     3943
## sd(g0_s)                        1.37      0.42     0.74     2.34 1.00     3888
## sd(pfemale)                     0.27      0.22     0.01     0.81 1.00     3707
## sd(g0_s:pfemale)                1.28      0.46     0.59     2.37 1.00     4154
## cor(Intercept,g0_s)            -0.45      0.26    -0.85     0.14 1.00     3747
## cor(Intercept,pfemale)          0.14      0.37    -0.60     0.77 1.00     3803
## cor(g0_s,pfemale)              -0.12      0.36    -0.74     0.60 1.00     4109
## cor(Intercept,g0_s:pfemale)     0.10      0.30    -0.48     0.65 1.00     3836
## cor(g0_s,g0_s:pfemale)         -0.48      0.27    -0.87     0.12 1.00     4009
## cor(pfemale,g0_s:pfemale)      -0.02      0.36    -0.69     0.66 1.00     4201
##                             Tail_ESS
## sd(Intercept)                   3850
## sd(g0_s)                        3814
## sd(pfemale)                     3888
## sd(g0_s:pfemale)                3964
## cor(Intercept,g0_s)             4141
## cor(Intercept,pfemale)          3974
## cor(g0_s,pfemale)               3688
## cor(Intercept,g0_s:pfemale)     3968
## cor(g0_s,g0_s:pfemale)          3464
## cor(pfemale,g0_s:pfemale)       3927
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept       -0.67      0.54    -1.72     0.42 1.00     3304     3620
## g0_s            -3.31      0.59    -4.54    -2.18 1.00     3839     3873
## pfemale          2.30      0.24     1.85     2.79 1.00     3486     3523
## g0_s:pfemale     1.18      0.54     0.15     2.31 1.00     4078     3971
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

First we need to look at the fixed effects, and we can do so more easily by plotting them as in the left panel. The most important thing to remember when interpreting the coefficients of a logistic model is that positive coefficients push us towards 'success', in this case an 'adult' response, and negative values push us towards a 'child' response. The right panel in the figure below shows us a summary of our data, with the 'main effects' classification line drawn on top of it. 

The negative effect for f0 tells us that as f0 increases, we are less likely to observe a classification of 'adult' and more likely to observe one of 'child'. Basically, our line relating f0 to logits has a negative slope. 

The model intercept is the value of the line when x = 0. We can see this below as the value of y where our diagonal line crosses the vertical dotted line. Since we scaled our f0 predictor, our intercept corresponds to the expected logit value when we saw a mean log f0. So, the intercept of our model reflects the probability of observing a response of 'adult' at the average log f0. We can see that when scaled log f0 = 0, there is a small negative value, meaning we have a slight expectation that there will be a classification of adult. 

When we use models based on logits, we might actually be interested in the *x intercept* of our lines (or planes). The x intercept of our line is the value along x where *y is equal to zero*. Why might we care about this? Well, when y = 0, that means that the probability of classification is 0.5. So, if you go even slightly beyond the value of the x intercept, you are more likely to see a success, and if you go ever so slightly below the value of the x intercept you are more likely to see a failure. In other words, the x intercept of these models tells us about the location of the *category boundary* between our two categories, with respect to the continuous predictor.   


<div class="figure">
<img src="07_files/figure-html/F7-7-1.png" alt="(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted." width="768" />
<p class="caption">(\#fig:F7-7)(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted.</p>
</div>

The location of the x intercept is presented using a bold vertical line in the figure above This is the location where our prediction line intersects with the x axis. We can find the x intercept by setting y to zero in our prediction equation and solving for (i.e. isolating) x, as seen below. For complicated prediction equations (and even for simple ones), I often rely on [websites like this one](https://www.mathpapa.com/algebra-calculator.html).

$$
\begin{equation}
\begin{split}
y = a + b*x \\
0 = a + b*x \\
-a = b*x \\ 
-a/b = x
\end{split}
(\#eq:711)
\end{equation}
$$

We can use this information to calculate our category boundary between adult and child classifications. Remember that to do arithmetic operations on our parameters, we have to use the original samples and not the summaries. For example, based on the numbers in our print out above, we expect that the category boundary is at `-(-0.67 / -3.31)=-0.20`. Instead, when I do the same calculation using our original samples, we see that the boundary is actually nearer to `-0.22`.


```r
fixef_samples = fixef (logistic_g0, summary = FALSE)
boundary = -fixef_samples[,"Intercept"] / fixef_samples[,"g0_s"]
posterior_summary (boundary)
```

```
##        Estimate Est.Error       Q2.5     Q97.5
## [1,] -0.2194171 0.1943839 -0.6339821 0.1127638
```

The units above are not super useful. After fitting our model, we can convert our parameters back to 'unscaled' log-Hz and then even to 'regular' Hertz values. Below, we calculate the mean and standard deviations of the original variable. We then use these to *undo* our scaling operation by first multiplying by the standard deviation and then adding the mean. We can then exponentiate these unscaled log-Hz values to get Hz ranges from the model parameters. 

This is basically like, imagine your model was specified in centimeters. If you wanted to understand it in meters, it seems obvious that it would be ok to divide everything by 100. If the model were somehow radically different just because you divided the output by 100, that would obviously be a big problem for us. Scaling and unscaling predictors are these kinds of changes, they don't really hurt our ability to make reliable inferences as long as we do th operation to all of the samples, and *then* summarize. 

Below I unscale and exponentiate the data, before using the `posterior_summary` function to get a summary of the samples. Note that we can use this information to get a point estimate for the boundary, in addition to getting information about uncertainty in the estimate of the boundary (via the credible interval). 


```r
h95_sd = sd(h95$g0)   ## calculate data mean
h95_mean = mean(h95$g0) ## calculate data sd
boundary_unscaled = boundary * h95_sd + h95_mean ## unscale data
boundary_hz = exp (boundary_unscaled) ## exponentiate to get hertz
posterior_summary (boundary_hz) ## check out boundary statistics
```

```
##      Estimate Est.Error     Q2.5   Q97.5
## [1,] 178.3559  10.13531 157.3807 196.577
```


```r
posterior_summary (boundary_hz) ## check out boundary statistics
##      Estimate Est.Error     Q2.5   Q97.5
## [1,] 178.3559  10.13531 157.3807 196.577
```

Below I present a histogram of the posterior distribution of the x-intercept of the line in our model. As we can see, our model suggests that this is likely between 160 and 200, with the mean being at around 180 Hz. We can see this distribution presented along with our data and 'main effects' line in the figure on the right below. 

Notice that the plot below uses Hz labels rather than log-Hz. All of our plots so far had ticks at -2, -2, 0 and 1. I used the process above to calculate the Hz value corresponding to each of those scaled log-Hz values as shown below:


```r
exp ( c(-2,-1,0,1) * h95_sd + h95_mean )
## [1] 104.7793 141.1276 190.0852 256.0263
```

And then used these values to make the labels for the figure. 

<div class="figure">
<img src="07_files/figure-html/F7-8-1.png" alt="(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted." width="768" />
<p class="caption">(\#fig:F7-8)(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted.</p>
</div>

It's can be useful to think about category boundaries when interpreting logits because the effects in our model can be interpreted as shifts in these boundaries. For example, consider the figure below which compares the lines generated by our models with our data. The dotted vertical line represented the location in the figure where x=0, remember that this won't correspond to 0 when we label the axis using unscaled Hz. 

Our model intercept estimate was -0.66, and the effect for `pfemale` was 2.3. That means that when subjects indicated hearing a female speaker, the lines representing their judgments were shifted up by 2.3 logits. Keep in mind this shift occurs when x=0, so if you haven't centered your continuous predictors this can all be more complicated, especially if your lines also have different slopes. 

However, when a line with a fixed negative slope is shifted up it's x-intercept must *increase* in value. This causes the category boundary to move to the *left* on our plots. We can see this below, the higher intercept for the teal line moves the adult/child boundary higher along f0 for speakers perceived as female. We can see the opposite effect for the male line (in red), where the effect of -2.3 (`-pfemale`) results in a lower line and a rightward move for the category boundary.

Our model slope was -3.3, with the `g0_s:pfemale` interaction equaling 1.18. This means that when speakers were identified as female, slopes were smaller in magnitude (-3.3 + 1.18) than when speakers were identified as male (-3.3 - 1.18). Increasing the magnitude of our slopes (positive or negative) without changing intercepts does not affect the category boundary. Instead, it results in more 'categorical', less fuzzy classifications. 

When lines differ in both slopes and intercepts, the effect on classification boundaries needs to be considered on a case by case basis. However in genera it is quite straightforward, one only needs to imagine the effects on our lines and the locations where they will cross 0. Below, we can see that the intercept and slope differences between speakers identified as male and female both serve to increase the f0 threshold which a speaker must cross to be identified as an adult. 

<div class="figure">
<img src="07_files/figure-html/F7-9-1.png" alt="(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted." width="768" />
<p class="caption">(\#fig:F7-9)(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted.</p>
</div>

Below I present a final figure that might be the most useful to present at a conference or submitted to a journal. It contains the data summarized as probabilities and the x-axis labeled with Hz, both of which help interpret the figure. 

<div class="figure">
<img src="07_files/figure-html/F7-10-1.png" alt="(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted." width="768" />
<p class="caption">(\#fig:F7-10)(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted.</p>
</div>



```r
boundary_female = -(fixef_samples[,"Intercept"]+fixef_samples[,"pfemale"]) / 
  (fixef_samples[,"g0_s"]+fixef_samples[,"g0_s:pfemale"])
boundary_male = -(fixef_samples[,"Intercept"]+fixef_samples[,"pfemale"]) / 
  (fixef_samples[,"g0_s"]+fixef_samples[,"g0_s:pfemale"])

h95_sd = sd(h95$g0)   ## calculate data mean
h95_mean = mean(h95$g0) ## calculate data sd
boundary_unscaled = boundary * h95_sd + h95_mean ## unscale data
boundary_hz = exp (boundary_unscaled) ## exponentiate to get hertz
posterior_summary (boundary_hz) ## check out boundary statistics
```

```
##      Estimate Est.Error     Q2.5   Q97.5
## [1,] 178.3559  10.13531 157.3807 196.577
```


## Plot Code




```r
################################################################################
### Figure 7.1
################################################################################

par (mfrow = c(1,3), mar = c(4,2,1,1), oma = c(1,4,0,0))

plot (agg_males$g0, agg_males$pheight, cex =2, col = cols[c(3,5)][agg_males$group], 
      xlim=c(4.5,5.75),  pch=1,lwd=2,ylim = c(45,75),xlab = "",
      ylab="Height (inches)")
grid()
points (agg_females$g0, agg_females$pheight, cex =2, pch=1,lwd=2,
      col = cols[c(4,6)][agg_males$group])

legend (4.6,60, legend = c("Boys","Girls","Men","Women"),lwd=2,lty=0,
        col = cols[3:6], bty='n',pch=1,pt.cex=1.5)

abline(lm (pheight ~ g0, data = agg_males)$coefficients, lwd=1,lty=3)
abline(lm (pheight ~ g0, data = agg_females)$coefficients, lwd=1,lty=3)
abline(lm (pheight ~ g0, data = h95)$coefficients, lwd=3,lty=3, col=4)

plot (agg_males$g0, agg_males$pheight, cex =2, col = cols[c(3,5)][agg_males$group], 
      xlim=c(4.5,5.75),  pch=1,lwd=2,ylim = c(45,75),xlab = "",
      ylab="Height (inches)")
grid()
abline(lm (pheight ~ g0, data = agg_females)$coefficients, lwd=1,lty=3)
abline(lm (pheight ~ g0, data = agg_males)$coefficients, lwd=3,lty=3,col=4)
abline(lm (pheight ~ g0, data = h95)$coefficients, lwd=1,lty=3, col=1)

plot (agg_females$g0, agg_females$pheight, cex =2, pch=1,lwd=2,ylim = c(45,75),
      col = cols[c(4,6)][agg_males$group], xlim=c(4.5,5.75),xlab = "",
      ylab="Height (inches)")
grid()
abline(lm (pheight ~ g0, data = agg_males)$coefficients, lwd=1,lty=3)
abline(lm (pheight ~ g0, data = agg_females)$coefficients, lwd=3,lty=3,col=4)
abline(lm (pheight ~ g0, data = h95)$coefficients, lwd=1,lty=3, col=1)

mtext (side=1, text = "f0 (log Hz)", outer = TRUE, cex = 0.8, line=-1)
mtext (side=2, text = "Height (inches)", outer = TRUE, cex = 0.9, line=2)

################################################################################
### Figure 7.2
################################################################################

par (mfrow = c(1,3), mar = c(4,2,1,1), oma = c(1,4,0,0))

plot (agg_males$g0, agg_males$padult, cex =2, col = cols[c(3,5)][agg_males$group], 
      xlim=c(4.5,5.75),  pch=1,lwd=2,ylim = c(0,1),xlab = "",
      ylab="")
points (agg_females$g0, agg_females$padult, cex =2, pch=1,lwd=2,
      col = cols[c(4,6)][agg_males$group])
grid()
abline(lm (padult ~ g0, data = agg_males)$coefficients, lwd=1,lty=3)
abline(lm (padult ~ g0, data = agg_females)$coefficients, lwd=1,lty=3)
abline(lm (padult ~ g0, data = h95)$coefficients, lwd=3,lty=3, col=4)

legend (4.6,0.5, legend = c("Boys","Girls","Men","Women"),lwd=2,lty=0,
        col = cols[3:6], bty='n',pch=1,pt.cex=1.5)

plot (agg_males$g0, agg_males$padult, cex =2, col = cols[c(3,5)][agg_males$group], 
      xlim=c(4.5,5.75),  pch=1,lwd=2,ylim = c(0,1),xlab = "",
      ylab="")
grid()
abline(lm (padult ~ g0, data = agg_females)$coefficients, lwd=1,lty=3)
abline(lm (padult ~ g0, data = agg_males)$coefficients, lwd=3,lty=3,col=4)
abline(lm (padult ~ g0, data = h95)$coefficients, lwd=1,lty=3, col=1)

plot (agg_females$g0, agg_females$padult, cex =2, pch=1,lwd=2,ylim = c(0,1),
      col = cols[c(4,6)][agg_males$group], xlim=c(4.5,5.75),xlab = "",
      ylab="")
grid()
abline(lm (padult ~ g0, data = agg_males)$coefficients, lwd=1,lty=3)
abline(lm (padult ~ g0, data = agg_females)$coefficients, lwd=3,lty=3,col=4)
abline(lm (padult ~ g0, data = h95)$coefficients, lwd=1,lty=3, col=1)

mtext (side=1, text = "f0 (log Hz)", outer = TRUE, cex = 0.9, line=-1)
mtext (side=2, text = "P(response=\"adult\")", outer = TRUE, cex = 0.9, line=2)

################################################################################
### Figure 7.3
################################################################################

x = seq (-8,8,.01)
y = x

par (mfrow = c(1,3), mar=c(4,4,3,1))
plot (x,y, type = 'l',lwd=2, col=deepgreen, xlim=c(-7,7), main = "y = x",
      xlab = "Predictor", ylab = "Logits")
abline (h=0,v=seq(-8,8,2),lty=3)

plot (x,ztop (y), type = 'l',lwd=2, col=darkorange, xlim=c(-7,7), 
      main = "y = logistic ( x )", xlab = "Predictor", ylab="Probability")
abline (h=c(0,1,.5),v=seq(-8,8,2),lty=3)

plot (x,ptoz(ztop (y)), type = 'l',lwd=2, col=lavender, xlim=c(-7,7), 
      main = "y = logit ( logistic ( x ) )", xlab = "Predictor", ylab="Logits")
abline (h=0,v=seq(-8,8,2),lty=3)

################################################################################
### Figure 7.4
################################################################################

x = seq (-3,3,.01)
y = x

par (mfrow = c(1,2), mar=c(4,4,1,1))
plot (x,y, type = 'l',lwd=3, col=deepgreen, xlim=c(-3,3),xlab="Predictor (x)",
      ylab = "Logits (log (p) - log(1-p))")
abline (h=ptoz(seq(0.1,0.9,.1)),v=c(-9:9),lty=3)
abline (h = 0, lwd=2)

plot (x,ztop (y), type = 'l',lwd=3, col=darkorange, xlim=c(-3,3),
      xlab="Predictor (x)",ylab="Probability")
abline (h=seq(0,1,.1),v=(-9:9),lty=3)
abline (h = 0.5, lwd=2)


################################################################################
### Figure 7.5
################################################################################

cffs = logistic_g0_hypothesis[[1]][,2]

par (mfrow = c(1,3), mar = c(4,2,1,1), oma = c(1,4,0,0))

plot (agg_males$g0_s, ptoz(agg_males$padult), cex =2, ylim = c(-4,4),xlab="", 
      col = cols[c(3,5)][agg_males$group],pch=1,lwd=2, xlim =range (h95$g0_s))
points (agg_females$g0_s, ptoz(agg_females$padult), cex=2, 
      col=cols[c(4,6)][agg_males$group],  pch=1,lwd=2)
curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 2, lwd=2)
curve ( (cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)
curve ( (cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)

legend (-2,-1, legend = c("Boys","Girls","Men","Women"),lwd=2,lty=0,
        col = cols[3:6], bty='n',pch=1,pt.cex=1.5)

plot (agg_males$g0_s, ptoz(agg_males$padult), cex =2, ylim = c(-4,4),xlab="", 
      col = cols[c(3,5)][agg_males$group],  pch=1,lwd=2, xlim =range (h95$g0_s))
curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)
curve ( (cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 2, lwd=2)
curve ( (cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)

plot (agg_females$g0_s, ptoz(agg_females$padult), cex=2,ylim = c(-4,4),xlab="", 
      col=cols[c(4,6)][agg_males$group],  pch=1,lwd=2,xlim =range (h95$g0_s))
curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)
curve ( (cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)
curve ( (cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 2, lwd=2)

mtext (side=1, text = "f0 (log Hz)", outer = TRUE, cex = 0.8, line=-1)
mtext (side=2, text = "Logit of P(response=\"male\")", outer = TRUE, 
       cex = 0.9, line=2)

################################################################################
### Figure 7.6
################################################################################

cffs = logistic_g0_hypothesis[[1]][,2]

par (mfrow = c(1,3), mar = c(4,2,1,1), oma = c(1,4,0,0))

plot (agg_males$g0_s, (agg_males$padult), cex =2, ylim = c(0,1),xlab="",  
      col = cols[c(3,5)][agg_males$group],  pch=1,lwd=2, xlim =range(h95$g0_s))
points (agg_females$g0_s, (agg_females$padult), cex=2, 
      col=cols[c(4,6)][agg_males$group],  pch=1,lwd=2)
curve ( ztop(cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 2, lwd=2)
curve ( ztop(cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)
curve ( ztop(cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)

legend (-2,ztop(-1), legend = c("Boys","Girls","Men","Women"),lwd=2,lty=0,
        col = cols[3:6], bty='n',pch=1,pt.cex=1.5)

plot (agg_males$g0_s, (agg_males$padult), cex =2, ylim = c(0,1),xlab="",  
      col = cols[c(3,5)][agg_males$group],  pch=1,lwd=2, xlim =range(h95$g0_s))
curve ( ztop(cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)
curve ( ztop(cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 2, lwd=2)
curve ( ztop(cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)

plot (agg_females$g0_s, (agg_females$padult), cex=2,ylim = c(0,1),xlab="",  
      col=cols[c(4,6)][agg_males$group],  pch=1,lwd=2,xlim =range (h95$g0_s))
curve ( ztop(cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)
curve ( ztop(cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=1, lty=3)
curve ( ztop(cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 2, lwd=2)

mtext (side=1, text = "f0 (log Hz)", outer = TRUE, cex = 0.8, line=-1)
mtext (side=2, text = "P(response=\"male\")", outer = TRUE, cex = 0.9,line=2)

################################################################################
### Figure 7.7
################################################################################

cffs = logistic_g0_hypothesis[[1]][,2]
fixed_effects = fixef(logistic_g0)

layout (mat = t(as.matrix(c(1,1,2,2,2))))
par (mar = c(4,4.5,1,1))

brmplot (fixed_effects, grid = TRUE)
abline (h=0,lty=3)
mtext (side=2, outer = TRUE, text = "Effect (logits)", line= -1.5, cex=.7)

plot (agg_males$g0_s, ptoz(agg_males$padult), ylim = c(-4,4),
      xlab="f0 (scaled log Hz)", 
      type='n',ylab="Logit of P(response=\"male\")",xlim =range(h95$g0_s))
abline (h=0,v=0,lwd=1, lty=3)
points (agg_males$g0_s, ptoz(agg_males$padult), cex =2,  
      col = cols[c(3,5)][agg_males$group],  pch=1,lwd=2)
points (agg_females$g0_s, ptoz(agg_females$padult), cex=2, 
      col=cols[c(4,6)][agg_males$group],  pch=1,lwd=2)

curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=2, lty=1)
x = -(cffs[1]) / (cffs[4]); abline (v = x, col = 1, lwd=2)

legend (-2,-1, legend = c("Boys","Girls","Men","Women"),lwd=2,lty=0,
        col = cols[3:6], bty='n',pch=1,pt.cex=1.5)

################################################################################
### Figure 7.8
################################################################################

par (mar = c(4,4.5,1,1), mfrow = c(1,2))

den = density(boundary_hz)
hist (boundary_hz, col=lightpink, breaks = 60, main = "", freq = FALSE)
lines (den$x, den$y,lwd=2,col=1)

plot (agg_males$g0_s, ptoz(agg_males$padult), ylim = c(-4,4),xlab="f0 (Hz)", 
      type='n', xlim =range (h95$g0_s), xaxt = 'n',
      ylab="Logit of P(response=\"male\")")
values = round (exp ( seq(-2,1,1)*sd(h95$g0)+mean(h95$g0) ))
axis (side = 1, at = -2:1, labels = values)
abline (h=0,lwd=1)
points (agg_males$g0_s, ptoz(agg_males$padult), cex =2,  
      col = cols[c(3,5)][agg_males$group],  pch=1,lwd=2)
points (agg_females$g0_s, ptoz(agg_females$padult), cex=2, 
      col=cols[c(4,6)][agg_males$group],  pch=1,lwd=2)

legend (-2.5,-0.5, legend = c("Boys","Girls","Men","Women"),lwd=2,lty=0,
        col = cols[3:6], bty='n',pch=1,pt.cex=1.5)

curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=2, lty=3)
x = -(cffs[1]) / (cffs[4]); abline (v = x, col = 1, lwd=1)

den = density(boundary)
lines (den$x, den$y/2,lwd=2,col=1)
polygon (den$x, den$y/2,lwd=2,col=lightpink)

################################################################################
### Figure 7.9
################################################################################

cffs = logistic_g0_hypothesis[[1]][,2]

par (mfrow = c(1,1), mar = c(4,4,1,1))

plot (agg_males$g0_s,ptoz(agg_males$padult),xlab="f0 (Hz)",type='n', 
      xlim=range (h95$g0_s),ylab="Logit of P(response=\"male\")",ylim=c(-4,4))
abline (v= 0,h=0, lty=3)
points (agg_males$g0_s, ptoz(agg_males$padult), cex =2,  
      col = cols[c(3,5)][agg_males$group],  pch=1,lwd=2)
points (agg_females$g0_s, ptoz(agg_females$padult), cex=2, 
      col=cols[c(4,6)][agg_males$group],  pch=1,lwd=2)

legend (-2.5,-0.5, legend = c("Boys","Girls","Men","Women"),lwd=2,lty=0,
        col = cols[3:6], bty='n',pch=1,pt.cex=1.5)

curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=2, lty=3)
x = -(cffs[1]) / (cffs[4]); abline (v = x, col = 1, lwd=2)

curve ( (cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = coral, lwd=2, lty=3)
x = -(cffs[3]) / (cffs[6]); abline (v = x, col = coral, lwd=2)

curve ( (cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = teal, lwd=2, lty=3)
x = -(cffs[2]) / (cffs[5]); abline (v = x, col = teal, lwd=2)

################################################################################
### Figure 7.10
################################################################################

cffs = logistic_g0_hypothesis[[1]][,2]

par (mfrow = c(1,1), mar = c(4,4,1,1))

plot (agg_males$g0_s, (agg_males$padult), ylim = c(0,1),xlab="f0 (Hz)",type='n',
     xlim =range (h95$g0_s), xaxt = 'n', ylab = "P(response=\"male\")")
values = round (exp ( seq(-2,1,1)*sd(h95$g0)+mean(h95$g0) ))
axis (side = 1, at = -2:1, labels = values)

abline (v= 0,h=0.5, lty=3)
points (agg_males$g0_s, (agg_males$padult), cex =2,  
      col = cols[c(3,5)][agg_males$group],  pch=1,lwd=2)
points (agg_females$g0_s, (agg_females$padult), cex=2, 
      col=cols[c(4,6)][agg_males$group],  pch=1,lwd=2)

curve ( ztop(cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = 1, lwd=2, lty=3)
x = -(cffs[1]) / (cffs[4]); abline (v = x, col = 1, lwd=2)

curve ( ztop(cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = coral, lwd=2, lty=3)
x = -(cffs[3]) / (cffs[6]); abline (v = x, col = coral, lwd=2)

curve ( ztop(cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, 
        col = teal, lwd=2, lty=3)
x = -(cffs[2]) / (cffs[5]); abline (v = x, col = teal, lwd=2)
```


