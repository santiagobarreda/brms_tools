---
output:
  pdf_document: default
  html_document: default
---

# Including continuous predictors in our model

Last chapter we talked about comparing many groups, and including interactions in our models. So far we have only discussed models that include nominal predictors. In this chapter we're going to talk about including continuous, numerical predictors in our models. 

Just as in chapter 1, we're going to begin by talking about 'single' level (*not* multi-level) models. These models have no random effects, and so are not really appropriate for our data. We're going to focus on the interpretation of model coefficients and what these mean for the geometry of the lines we make. Next chapter we'll extend these concepts to understand 'random slopes and intercepts' for variables such as speaker and listener. 

## Data and research questions 

We're still going to work with the Hillenbrand et al. data, again focusing on variation in f0. We are going to be focusing on a subset of these (productions of "heed" and "hod" only), produced by all 136 speakers in the data. 

Our data has a new continuous variable (`pheight`) representing the average perceived height for each talker, for each token. In this chapter, we are going to use perceived height as a predictor to see if it can be used to predict the f0 of a token. 


```r
library (brms)
options (contrasts = c("contr.sum","cont.sum"))

url1 = "https://raw.githubusercontent.com/santiagobarreda"
url2 = "/stats-class/master/data/h95_experiment_summary.csv"
h95 = read.csv (url(paste0 (url1, url2)))
## set up colors for plotting
devtools::source_url (paste0 (url1, "/stats-class/master/data/colors.R"))
## source functions
devtools::source_url (paste0 (url1, "/stats-class/master/data/functions.R"))

## make group a factor
h95$group = factor (h95$group)  
```

## Continuous predictors: modeling variation along lines

Below I plot the average f0 for each token against its average perceived height in inches. I use a scatter plot with the dependent variable (`f0`, the thing we are interested in) varying along the y axis (this is done by convention). 

The scatter plot clearly shows what is called a *linear relationship* between the two variables. As far as we're concerned, this just means when you make a scatter plot, you will see points you think are suggestive of a line, or a couple of lines.

Our models have so far featured only nominal predictors, things like group membership. Although it might be strange to think of it this way, our regression models *have* been making lines, however, they are lines with slopes of 0 along all x axis variables. What this means is, that for any x-axis variable, variation along the x axis variable **has no effect** on variation along the y axis variable. 

Below I show an example of what I mean by this. Figure \@ref(fig:F5-fig1) shows several an 'intercept only model' that features a fixed f0 for all tokens. In the right panel I show four lines, one for each group. These lines have different intercept but have the same slope (0). This means that these lines try to predict f0 based on group, but do not allow this to vary as a function of perceived height.

When we look at lines such as those below, they tell us that our model thinks mean f0 is *independent* of perceived height. This is because perceived height can vary from positive to negative infinity and we don't expect f0 to change (the line is flat!). The same statement could be made for any x variable we chose because our model does not include slopes for *any* variable. 

<div class="figure">
<img src="05_files/figure-html/F5-fig1-1.png" alt="(left) f0 plotted against perceived height for each token. Horizontal line is the mean of the group means. (right) Same as right panel except each group gets its own horizontal line. Groups are boys (yellow), girls (green), women (blue), and men (red)." width="768" />
<p class="caption">(\#fig:F5-fig1)(left) f0 plotted against perceived height for each token. Horizontal line is the mean of the group means. (right) Same as right panel except each group gets its own horizontal line. Groups are boys (yellow), girls (green), women (blue), and men (red).</p>
</div>

Recall that the equation for a line is the following:

$$
y = m*x + b \\ 
(\#eq:51)
$$

Where $y$ is the 'output' variable you are 'predicting' using a line with a slope of $m$ and and intercept of $b$. The slope $m$ represents how much of a change you expect in your $y$ variable for a *1 unit change* in your x variable. Obviously, this means that the slope depends on the units of measurement of your $x$ variable. In general, dividing your $x$ predictor by $z$ will increase your slopes by a factor of $z$. For example, imagine measuring the slope of a long hill with a constant rise. The amount of rise over 1 meter will necessarily be 1/1000 as much as the amount of rise over one kilometer. 

We can use the following line equation, which just uses symbols that are more similar to the ones we have been using (and presents them in a different order):

$$
\mu = \alpha + \beta*x \\
(\#eq:52)
$$

We can predict f0 in the left panel of Figure \@ref(fig:F5-fig1) using a horizontal line by setting the intercept to the overall grand mean, and setting the slope coefficient to 0. In this case we would have an 'intercept only' regression model just like the one we saw in chapters 1 and 2. 

$$
\alpha = Intercept, \beta = 0 \\ \\
f0 = Intercept = Intercept + 0*pheight \\
(\#eq:53)
$$

In the right panel of Figure \@ref(fig:F5-fig1) we see four horizontal lines, one for each group of speakers. These lines all differ in terms of intercept but have the same slope (0). This means that these lines try to predict f0 based on group, but don't allow this to vary as a function of perceived height. So, our four-group model with only nominal predictors in chapter 4 could really be thought of as predicting f0 along a set of lines that are horizontal along the perceived height dimension.  


$$
\alpha = Intercept + group_{[\mathrm{group}]}, \beta = 0 \\ \\
f0 = Intercept + group_{[\mathrm{group}]} + (0*pheight) \\
(\#eq:56)
$$

Ok, so what if we *do* want to think about variation in f0 as a function of variation in perceived height. In Figure \@ref(fig:F5-52) we can see what this might look like. On the left we have a normal distribution sliding along a horizontal line, generating numbers as it slides. The mean of this data does not vary based on the values of perceived height, and so is *independent* of them. The standard deviation of this distribution ($\sigma_{error}$) does not not change as a function of perceived height so its 'width' is stable. 

On the right in Figure \@ref(fig:F5-52) we can imagine that the mean of the normal distribution generating f0 values *does* change as a function of the value of perceived height. We can say that the model on the right predicts f0 *conditional on* values of perceived height.

The model on the right in Figure \@ref(fig:F5-52) places an important constraint on this conditional variation: the mean of f0 varies strictly along a straight line. So, to predict f0 given a certain perceived height, we slide our normal distribution along long the diagonal line in Figure \@ref(fig:F5-52) to the proper x axis location. The y axis value of the line at this x axis location represents our predicted value ($\mu$). The actual value of observations would then vary around this expected value in a normal distribution with a mean of 0 and a standard deviation equal to $\sigma_{error}$.

<div class="figure">
<img src="05_files/figure-html/F5-52-1.png" alt="(left) A normal distribution fixed along a horizontal line. (right) A Normal distribution allowed to slide along a diagonal line." width="768" />
<p class="caption">(\#fig:F5-52)(left) A normal distribution fixed along a horizontal line. (right) A Normal distribution allowed to slide along a diagonal line.</p>
</div>

## Models with a single slope and intercept

We can use the `brm` function to find the intercept and slope of the 'best' line through the points in our two-dimensional space (represented in the scatter plot). Our model formula will look like this:

`f0 ~ pheight`

Which tells `brms` to predict `f0` based on the values of `pheight`. If the variable on the right hand side of the `~` is numeric, `brm` will treat it as a continuous predictor and predict f0 using a line. 

### Description of the model

Our model for a regression model with a single continuous predictor (a 'bivariate' regression) is shown below. The first line says that we have a normally-distributed variable with an unknown mean that varies from trial to trial ($\mu_{[i]}$). The variation in the mean parameter varies along a line with an intercept equal to $Intercept$ and a slope of $pheight$ along the x axis. 

Note that the predicted value ($\mu_{[i]}$) and the predictor variable ($x_{[i]}$) receive subscripts, as these change from trial to trial. The coefficients **do not** receive subscripts because these do not vary in this model. This model contains a single intercept and a single slope (i.e. it draws a single line) for *every* trial. Also note that we're simply treating the slope and intercept as 'fixed' effects and specifying prior distributions.

\begin{equation}
\begin{split}
\\
\textrm{Likelihood:} \\
y_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{error}) \\
\mu_{[i]} = Intercept + pheight * x_{[i]}  \\ \\
\textrm{Priors:} \\
Intercept \sim t(3, 175, 100) \\
pheight \sim t(3, 0, 100) \\
\\
\end{split}
(\#eq:57)
\end{equation}

Here's another two ways to think about this model: we're making a line and add noise to it. Each of our observed values is just a line (representing systematic variation) and a random draw from an error distribution ($\mathcal{N}(0,\sigma_{error})$). 

$$
y_{[i]} = \alpha + \beta * \mathrm{x_{[i]}} + \mathcal{N}(0,\sigma_{error})
(\#eq:58)
$$

Alternatively, we could place the formula for the line *inside* the formula for the normal distribution. There is no particular reason to do this, but it is helpful to see it and realize that it's the same thing as the representation above. In the equation below, we're saying: the data is generated according to a normal distribution whose mean varies along a line ($\alpha + \beta * \mathrm{x_{[i]}}$), and we expect the variation around this line to have a standard deviation equal to $\sigma_{error}$. 

$$
\mathcal{N}(\alpha + \beta * \mathrm{x_{[i]}}, \sigma_{error})  \\
(\#eq:59)
$$

Regression models pick the intercept and slope for a line that results in the smallest value of $\sigma_{error}$. This is why 'regular' regression is called ["ordinary least-squares"](https://en.wikipedia.org/wiki/Ordinary_least_squares) regression, because it finds the solution that results in the 'least squares' (i.e., the smallest $(\sigma_{error})^2$) in the prediction errors. 

In a multilevel model, the estimation of the 'best' slopes and intercepts for our lines can be substantially more complicated than in least-squares regression. For example, in the case of experiments these estimates would take within and between-participant variation into account. However, in general the lines estimated by our regression models will tend to minimize the value of $\sigma_{error}$, given our data and model structure. 

### Fitting the model

We can use the `brm` function to find the intercept and slope of the 'best' line through the points in our two-dimensional space (represented in the scatter plot).  


```r
options (contrasts = c("contr.sum","cont.sum"))
## Fit the model yourself, or
## download pre-fit model from: 
## github.com/santiagobarreda/stats-class/tree/master/models
## and load after placing in working directory
## single_line_model = readRDS ('5_single_line_model.RDS')

set.seed (1)
single_line_model =
  brm (f0 ~ pheight, data = h95, chains=1, cores=1,  warmup=1000, iter = 6000,
       prior = c(set_prior("student_t(3, 175, 100)", class = "Intercept"),
                 set_prior("student_t(3, 0, 100)", class = "b")))
## save model
## saveRDS (single_line_model, '5_single_line_model.RDS')
```

### Interpreting the model

The model print statement is mostly the same as for our previous models. Our model contains only 'fixed' effects: an `Intercept`, indicating the intercept of our line, and `pheight` indicating the slope of f0 along the perceived height axis. We get credible intervals for our slope coefficient, and our model features an estimate of the error (`sigma`, $\sigma_{error}$) around our line.  


```r
## inspect model
single_line_model
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: f0 ~ pheight 
##    Data: h95 (Number of observations: 278) 
## Samples: 1 chains, each with iter = 6000; warmup = 1000; thin = 1;
##          total post-warmup samples = 5000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   588.68     16.42   556.41   620.73 1.00     5261     3935
## pheight      -6.36      0.27    -6.89    -5.84 1.00     5283     3962
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    30.21      1.31    27.78    32.93 1.00     4562     3693
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

We can see that the line predicting perceived height as a function of f0 has an intercept of 589 and a slope for the `pheight` (perceived height) predictor of -6.4. The fact that the slope of the `pheight` predictor is -6.4 means that for every 1 inch increase in perceived height, we expect a *decrease* of 6.4 Hz in f0. I draw this line on our set of points in Figure \@ref(fig:F5-53). 

The slope is a *weight* that allows the line to accurately fit the points. In the absence of a slope, regression models would only work if there was a 1 to 1 relationship between the x and y variables. This would mean that for every 1 inch change in perceived height, we would see a 1 Hz change in f0. What are the odds of that? What are the odds that the things we measure will be in a 1 to 1 relationship like that in general? The odds are basically zero. 

Instead, the slope ($\beta$ coefficient) on regression models allows a single unit change in the predictor to be associated with different units of change in the $y$ variable. In this case, a 1 unit change in perceived height (measured in inches) is associated with a 6 unit change in f0 (measured in Hz).

<div class="figure">
<img src="05_files/figure-html/F5-53-1.png" alt="(left) Points and best-fit line. (right) A zoomed-out view of the left panel shows the line intercept at y = 588.7 Hz." width="768" />
<p class="caption">(\#fig:F5-53)(left) Points and best-fit line. (right) A zoomed-out view of the left panel shows the line intercept at y = 588.7 Hz.</p>
</div>

The intercept is the value of your dependent variable ($y$, the variable you are interested in) when your predictor is equal to 0. In our model, this means that the expected value of f0 is 588 Hz when the predicted height is 0 inches. These are not particularly meaningful values for either variable as the f0 is too high and people are never 0 inches tall. However, in the right panel of Figure \@ref(fig:F5-53) we can see why we get this value: it is simply the y-axis "intercept" (zero crossing) of the line we drew through the points in the left panel. 

### Centering predictors

We can get more useful intercept values by simply centering our predictor variable(s). Centering a variable means subtracting the mean value from all observations. When this is done, each observation will now represent a deviation from 0, and the sum (and mean) of all the observations will equal zero. Since the intercept of the line is the value of the $y$ variable when the $x$ variable is equal to zero, centering our predictor makes the intercept equal to the value of $y$ when $x$ is equal to its zero, its mean. 

Centering predictor variables affects that intercept of the model but does does not affect the model slope or error estimates. Thus, centering is basically like choosing the 'coding' (e.g., sum coding vs. dummy coding) for lines, it affects how the information is represented in the model, but not the information itself. As a result, it can be tremendously useful in yielding more interpretable intercept estimates. Below, I center perceived height and again predict f0 based on this value. 


```r
## find mean perceived height
mean_perceived_height = mean(h95$pheight)
mean_perceived_height
```

```
## [1] 61.38813
```

```r
## center perceived height
h95$pheight_c = h95$pheight - mean_perceived_height
```



```r
## Fit the model yourself, or
## download pre-fit model from: 
## github.com/santiagobarreda/stats-class/tree/master/models
## and load after placing in working directory
## single_line_model = readRDS ('5_single_line_centered_model.RDS')

set.seed (1)
single_line_centered_model =
  brm (f0 ~ pheight_c, data=h95, chains=1,cores=1,warmup=1000,iter=6000,
       prior = c(set_prior("student_t(3, 175, 100)", class = "Intercept"),
                 set_prior("student_t(3, 0, 100)", class = "b")))
## save model
## saveRDS (single_line_centered_model, '5_single_line_centered_model.RDS')
```



```r
## inspect model
single_line_centered_model
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: f0 ~ pheight_centered 
##    Data: h95 (Number of observations: 278) 
## Samples: 1 chains, each with iter = 6000; warmup = 1000; thin = 1;
##          total post-warmup samples = 5000
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept          197.99      1.77   194.53   201.53 1.00     6644     4267
## pheight_centered    -6.37      0.27    -6.89    -5.84 1.00     4595     3507
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    30.19      1.28    27.86    32.86 1.00     5753     4025
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

We can see that the slope coefficient provided by this model is the same as the last model: the slope of the line has not changed. However, our intercept value is now 198. This means that when out predictor is at its mean (61.4 inches), we expect f0 to have a value of 198 Hz.   

<div class="figure">
<img src="05_files/figure-html/F5-slope-n-centered-1.png" alt="(left) Best fit line for points predicted by perceived height. Horizontal and vertical lines indicated y and x variable means respectively. (right) Same as left but with a centered predictor. " width="768" />
<p class="caption">(\#fig:F5-slope-n-centered)(left) Best fit line for points predicted by perceived height. Horizontal and vertical lines indicated y and x variable means respectively. (right) Same as left but with a centered predictor. </p>
</div>

I'm going to center predictors often, simply out of convenience. However, the decision whether to center or not should really be based on the information you hope to get out of your model, just like the decision of which coding system to use for nominal variables.

## Interactions in our line parameters

Here's something that I haven't really mentioned so far: group effects such as 'boy' and 'girl' are effectively 'interaction' terms. Remember that interactions are *conditional* effects. So, when we say "hey what's the intercept on the horizontal line you use to model f0?", the answer is actually "it depends on the group" (as seen in Figure \@ref:(fig:F5-fig1)). In other words, the intercept in our model is conditional on group. so, when we fit a model like:

`f0 ~ group`

The model could be thought of as something like:

`f0 ~ Intercept + Intercept:group`

This is because our model will estimate a 'main effects' (average) intercept term, and also estimate group-dependent deviations from the main effect (i.e., the `Intercept:group` interactions). By convention, we don't actually specify our models like this, however, it's useful to keep this perspective in mind when thinking about the lines defined by our models. 

A nominal predictor like `group` has the effect of allowing for group-specific intercepts in our lines. Consider what happens when a continuous predictor like `pheight` is added to our model formula:

`f0 ~ group + pheight`    (or in our expanded format `f0 ~ Intercept + Intercept:group + pheight`)

This model only includes a slope 'main effect', or an *average* slope effects across all groups. We can see this because the formula does not contain any interactions with our continuous predictor `pheight`. As a result, the model does not include any way for the `pheight` slope to vary (e.g., between groups). 

In the last chapter we discussed the fact that without interactions, shared slopes lead to parallel lines in interaction plots. The same applies here! If we draw a bunch of lines with different intercepts but a fixed slope, clearly these lines will all be parallel. 

If you want to know about the slope *conditional* on group, then you need to include a group by slope interaction in your model. We add this to out model formula:

`f0 ~ group * pheight`    or  `f0 ~ group + pheight + pheight:group`    

Which could be expanded like this:

`f0 ~ Intercept + Intercept:group + pheight + pheight:group`

In plain English, this says: "predict f0 as a function of an intercept, group-dependent variation in Intercepts (modeled by the `Intercept:group` interactions), a mean overall slope (`pheight`), and group-dependent variation in slopes (modeled by the `pheight:group` interactions).

A model like the one above allows us to represent group-dependent variation in lines *and* intercepts. Effectively, this allows us to have entirely different lines representing the relationships between perceived height and f0 in our data. We're going to see examples of these kinds of models in this chapter, first considering group-specific variation in intercepts.

## Models with group-dependent intercepts, but shared slopes

In the previous sections we focused on models that imposed a single line for all groups. Here, we're going to consider models that allow for differing intercepts between groups. We do this by including our vector specifying group membership `group` into our model formula:

`f0 ~ group + pheight`

The model above says: "model f0 as a function of perceived height, allowing for group-specific variation the effect for group". Note that our model does *not* include the interaction between `pheight` and `group`. this is because this model includes only a single slope across all groups. As a result, these models represent our data with a set of parallel lines, one for each group.

### Description of the model

The model including group-specific intercepts is presented below. The only difference compared to the previous model is that this now includes a $group$ predictor similar to the one we used initially in chapter 4. This predictor will allow our groups to be represented by different parallel lines with varying intercepts.  

\begin{equation}
\begin{split}
\\
\textrm{Likelihood:} \\
y_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{error}) \\
\mu_{[i]} = Intercept + group_{[\mathrm{group}_{[i]}]} + pheight * x_{[i]}  \\ \\
\textrm{Priors:} \\
Intercept \sim t(3, 175, 100) \\
pheight \sim t(3, 0, 100) \\ 
group \sim t(3, 0, 100) \\ 
\\
\end{split}
(\#eq:510)
\end{equation}

Here's another way to think about this model. In the model below we have $\alpha$ and $\beta$ parameters that vary from trial to trial. The trial-specific intercept is equal to the overall intercept, and the group predictor for that trial. The slope terms does not actually vary from trial to trial in practice, since it simply equals our $pheight$ slope parameter. 

Recall that I suggested that we could perform 'ANOVA-like' decompositions on our intercept predictors. In fact, we can do the same thing with our slopes. In the model below our $pheight$ parameter is basically an 'intercept' for the slope of our predictor. When we incorporate group-specific slopes in our model in the next section, these will be represented as group-specific deviations in slope relative the the slope 'main effect' (i.e., $pheight$) seen below. 

\begin{equation}
\begin{split}
\\
\textrm{Likelihood:} \\
y_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{error}) \\
\mu_{[i]} = \alpha_{[i]} + \beta_{[i]} * x_{[i]}  \\ 
\alpha_{[i]} = Intercept + group_{[\mathrm{group}_{[i]}]} \\
\beta_{[i]} = pheight \\ \\
\textrm{Priors:} \\
Intercept \sim t(3, 175, 100) \\
pheight \sim t(3, 0, 100) \\ 
group \sim t(3, 0, 100) \\ 
\\
\end{split}
(\#eq:511)
\end{equation}

### Fitting the model

We fit a model that contains the four group predictors and also includes our continuous predictor:


```r
## Fit the model yourself, or
## download pre-fit model from: 
## github.com/santiagobarreda/stats-class/tree/master/models
## and load after placing in working directory
## single_line_model = readRDS ('5_group_single_slope_model.RDS')

set.seed (1)
group_single_slope_model =
  brm (f0 ~ group + pheight_c, 
       data=h95, chains=1,cores=1,warmup=1000,iter=6000,
       prior = c(set_prior("student_t(3, 175, 100)", class = "Intercept"),
                 set_prior("student_t(3, 0, 100)", class = "b")))
## save model
## saveRDS (group_single_slope_model, '5_group_single_slope_model.RDS')
```


For the same of comparison, I am also going to fit a model with only group predictors and no continuous predictor (`pheight`). As noted above, this is effectively a model with a bunch of horizontal lines (slope = 0), one for each group. 


```r
## Fit the model yourself, or
## download pre-fit model from: 
## github.com/santiagobarreda/stats-class/tree/master/models
## and load after placing in working directory
## single_line_model = readRDS ('5_group_intercepts_model.RDS')

set.seed (1)
group_intercepts_model =
  brm (f0 ~ group, data=h95, chains=1,cores=1,warmup=1000,iter=6000,
       prior = c(set_prior("student_t(3, 175, 100)", class = "Intercept"),
                 set_prior("student_t(3, 0, 100)", class = "b")))
## save model
## saveRDS (group_intercepts_model, '5_group_intercepts_model.RDS')
```


###  The effect of including a slope

It's useful to think about the geometry of our models because then we can make pictures, which are usually much easier to interpret than coefficient values. The coefficient values in your model have a one-to-one relationship with a set of lines that make up a plot. Seeing (or imagining) what the picture might look like can go a long way towards understanding the meaning of your model parameters. 

Below I recover the overall intercept, and the intercept for each group from our `group_intercepts_model`. Since this model contains no slope terms these values represent the intercepts of horizontal lines, one for each group (and overall). 


```r
group_intercepts_hypothesis = 
  hypothesis (group_intercepts_model,
              hypothesis = 
                c("Intercept = 0",  ## overall intercept
                "Intercept + group1 = 0",  ## group 1 intercept
                "Intercept + group2 = 0",  ## group 2 intercept
                "Intercept + group3 = 0",  ## group 3 intercept
                "Intercept -(group1+group2+group3) = 0")) ## group 4 intercept
group_intercepts_hypothesis[[1]][,2:5]
```

```
##   Estimate Est.Error CI.Lower CI.Upper
## 1 207.7772  1.619998 204.5869 210.9643
## 2 237.8620  3.388196 231.2805 244.3704
## 3 240.5737  4.043656 232.7012 248.4879
## 4 132.0269  2.648429 126.8261 137.0310
## 5 220.6461  2.491914 215.6652 225.4884
```

We can then recover the intercepts for each group from our `group_single_slope_model`. Again, we do this by adding each group effect to the overall Intercept. Unlike our previous model, this model *does* have a slope. This slope is shared by all of our group lines meaning these differ in their intercepts but not their slopes.


```r
group_single_slope_hypothesis = 
  hypothesis (group_single_slope_model,
              hypothesis = 
                c("Intercept = 0", ## overall intercept
                  "Intercept + group1 = 0",  ## group 1 intercept
                  "Intercept + group2 = 0",  ## group 2 intercept
                  "Intercept + group3 = 0",  ## group 3 intercept
                  "Intercept + -(group1+group2+group3)=0", ## group 4 intercept
                  "pheight_c = 0") ## overall slope
)   
group_single_slope_hypothesis[[1]][,2:5]
```

```
##     Estimate Est.Error   CI.Lower   CI.Upper
## 1 199.179765 1.7911275 195.580187 202.805323
## 2 206.488554 4.9183377 196.650824 216.231669
## 3 203.383898 5.8259319 191.764627 214.944576
## 4 163.628055 4.5384288 154.794447 172.780375
## 5 223.218554 2.2955315 218.678988 227.655562
## 6  -4.267038 0.5208129  -5.306696  -3.266594
```

In Figure \@ref(fig:F5-55) I draw the the lines specified by our two models. On the left, the four group lines share a slope (that just happens to be zero). On the right, we see four lines with a shared slope that is *not* zero. It seems that these diagonal lines provide better fits for our data, and our `group_single_slope_model` lets us represent this.

<div class="figure">
<img src="05_files/figure-html/F5-55-1.png" alt="(left) Lines for each distribution in our no-slope model. (right) Lines for each distribution in our shared-slope model. Lines correspond to boys (yellow), girls (green), men (red), and women (blue)." width="768" />
<p class="caption">(\#fig:F5-55)(left) Lines for each distribution in our no-slope model. (right) Lines for each distribution in our shared-slope model. Lines correspond to boys (yellow), girls (green), men (red), and women (blue).</p>
</div>

### Interpreting group effects in the presence of a continuous predictor

The inclusion of a continuous predictor affects the interpretation of our group predictors. When we only had a group predictor the group effects had a simple interpretation: they represented the difference between our group mean and the intercept. In terms of our lines however, the group effects actually corresponded to the difference between the line intercept and the overall model intercept. It just so happened that in the previous model our group intercepts corresponded to our group means (because the slopes were 0). 

When you include a continuous predictor, the group effects *still* just represent the line intercepts for each group. However, since each line may have a non-zero slope, the group effects will not solely reflect the difference between the group means and the intercept.

Here's a more direct way to think about it: when lines share a slope, group effects affect the spacing between parallel lines. If you look at the right panel of Figure \@ref(fig:F5-55) and tilt your head 45 degrees to the right, its clear that the group effects are simply responsible for spacing out the parallel lines. This is the same thing the group effects are doing in the panel on the right.

We're going to compare the estimated group effects provided by the two models presented above (`group_intercepts_model`, and `group_single_slope_model`). Below, I use the `hypothesis` function to calculate the group effects according to each model. 


```r
group_intercepts_effects = 
  hypothesis (group_intercepts_model,
              hypothesis = c("group1 = 0", ## group 1 effect
                             "group2 = 0", ## group 2 effect
                             "group3 = 0", ## group 3 effect
                             "-(group1+group2+group3) = 0")) ## group 4 effect   
group_intercepts_effects[[1]][,2:5]
```

```
##    Estimate Est.Error   CI.Lower  CI.Upper
## 1  30.08481  2.896208  24.310787  35.74068
## 2  32.79653  3.260648  26.583129  39.21910
## 3 -75.75030  2.444318 -80.537948 -70.93498
## 4  12.86896  2.358541   8.181246  17.45949
```

```r
group_single_slope_effects = 
  hypothesis (group_single_slope_model,
              hypothesis = c("group1 = 0", ## group 1 effect
                             "group2 = 0", ## group 2 effect 
                             "group3 = 0", ## group 3 effect
                             "-(group1+group2+group3) = 0") ## group 4 effect
)   
group_single_slope_effects[[1]][,2:5]
```

```
##     Estimate Est.Error    CI.Lower  CI.Upper
## 1   7.308789  3.820567  -0.3534032  14.64993
## 2   4.204133  4.578811  -4.7655678  13.13831
## 3 -35.551710  5.396041 -46.0762439 -24.96786
## 4  24.038789  2.546818  19.0232649  29.19051
```

We can use the `brmplot` function to visually inspect the differences between the group effects across the models. We see that the groups effects are much smaller when the continuous predictor is included. This is visually apparent in the tighter clustering of the lines in the right panel of Figure \@ref(fig:F5-int-comparison). In the absence of group effects we would just see four overlapping lines, one for each group. 

<div class="figure">
<img src="05_files/figure-html/F5-int-comparison-1.png" alt=" (left) Comparison of estimated group effects for the model without (blue) and with (red) the perceived height predictor. (middle) Line intercepts reflect the blue coefficients in the left panel (without perceived height). (right) the red coefficients in the left panel (with perceived height). Lines correspond to boys (yellow), girls (green), men (red), and women (blue)." width="768" />
<p class="caption">(\#fig:F5-int-comparison) (left) Comparison of estimated group effects for the model without (blue) and with (red) the perceived height predictor. (middle) Line intercepts reflect the blue coefficients in the left panel (without perceived height). (right) the red coefficients in the left panel (with perceived height). Lines correspond to boys (yellow), girls (green), men (red), and women (blue).</p>
</div>

So how can we *interpret* the group effects when there is a continuous predictor? The group effects specify the difference in expected values for the dependent variable ($\mu$, in our case `f0`) between groups, given *any fixed value of the x axis variable*, in this case perceived height. 

To imagine this, pick any arbitrary location along the x-axis in the right panel of Figure \@ref(fig:F5-int-comparison). Then, find the predicted value for f0 for that location by sliding up the y axis until arriving at the black dotted line. The distance along the y axis between the black dotted line (the average line) and the group-specific lines (in different colors) is reflected in the red coefficients in the left panel of Figure \@ref(fig:F5-int-comparison). 

Ok, but what do the effects *mean*? The groups effects tell us that *given a certain perceived height*, f0 is lower than expected when the speaker is an adult male, and higher than expected when the speaker is an adult male. In other words, if you think you are listening to a person who is $x$ inches tall, the most likely f0 associated with that is about 40 Hz lower if the person is a man and about 20 Hz higher if the person is a woman. Importantly, this effect exists *independently* of the relationship between f0 and perceived height. 

This is evident in Figure \@ref(fig:F5-int-comparison). The blue line (women) runs parallel to but above the red line (men). Necessarily, for any given x axis location the y axis value will be higher for the blue line than the red line. This is what the group effects are telling us!

Compare this to the effects for girls and boys, which are near zero. When something has an effect near zero this means the 'effect' has no effect on outcomes! In this case, we say that for girls and boys, f0 is just about what you would expect given their perceived size. In other words, the perceived-size to f0 relationship for boys and girls is similar to the average relationship. Again, this is evident in Figure \@ref(fig:F5-int-comparison) where the green (girls) and yellow (boys) lines overlap with the black dotted line indicating the overall average line. 

## Models with group-dependent slopes intercepts

The model above is limited because it is constrained to have the same slope across all the groups. If we want to include different slopes for each group, we must consider the *conditional* effect of perceived height given group. To do this, we need to include the interaction between group and perceived height in our model as in the formula below:

`f0 ~ group * pheight`, or `f0 ~ group + pheight + pheight:group`

The model above says: "model f0 as a function of perceived height, allowing for group-specific variation in the effect for group, and in the effect for perceived height between groups".

### Description of the model

The 'ANOVA-style' decomposition re-introduced with the previous model becomes more useful now. Below we present the 'expanded' version of our prediction equation, in a format similar to that presented above. Note that each term that relates to the slope ($pheight, pheight \colon group$) is independently multiplied with our predictor ($x_{[i]}$). 

$$
\mu_{[i]} = Intercept + group_{[\mathrm{group}_{[i]}]} + pheight * x_{[i]} + pheight \colon group_{[\mathrm{group}_{[i]}]} * x_{[i]}
(\#eq:512)
$$

Below, we can group intercept and slope terms in parenthesis. This formulation makes it clear that we expect intercepts and slopes to actually vary trial to trial (whenever the `group` predictor does). 

$$
\mu_{[i]} = (Intercept + group_{[\mathrm{group}_{[i]}]}) + (pheight + pheight \colon group_{[\mathrm{group}_{[i]}]}) * x_{[i]}
(\#eq:513)
$$

We can take this one step further and break up our prediction equation into two steps. The three equations below say:

  * Our expected f0 value varies according to trial-dependent intercept and slope parameters. 
  
  * The intercept expected on a given trial is equal to the $Intercept$ (the intercept main effect) and the $group$ predictor (effectively, the $Intercept:group$ interaction). 
  
  * The slope expected on a given trial is equal to the $pheight$ predictor (effectively, the slope 'main effect') and the $pheight:group$ interaction.  

$$
\mu_{[i]} = \alpha_{[i]} + \beta_{[i]} * x_{[i]}  \\\\
\alpha_{[i]} = Intercept + group_{[\mathrm{group}_{[i]}]} \\\\
\beta_{[i]} = pheight + height \colon group_{[\mathrm{group}_{[i]}]}) 
(\#eq:514)
$$

As our models get more and more complicated, it can help to organize them in this manner. By considering all of our predictors as either 'main effects' or 'interaction' terms for different predictor variables in our data, we can organize the consideration of how different predictors are expected to relate to outcomes. For example, the representation above makes it clear that the $height \colon group$ predictor can affect the slopes of our lines, but has no mechanism by which to affect our line intercepts. 

We can represent our models in either of the following manners. This representation puts all our predictors directly in the prediction equation:

\begin{equation}
\begin{split}
\textrm{Likelihood:} \\
y_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{error}) \\
\mu_{[i]} = Intercept + group_{[\mathrm{group}_{[i]}]} + pheight * x_{[i]} + pheight \colon group * x_{[i]}  \\ \\
\textrm{Priors:} \\
Intercept \sim t(3, 175, 100) \\
pheight \sim t(3, 0, 100) \\ 
group \sim t(3, 0, 100) \\ 
pheight \colon group \sim t(3, 0, 100) \\ 
\end{split}
(\#eq:515)
\end{equation}

While this representation reflects the decomposition of line intercepts and slopes into several component parts, but is otherwise an identical model:

\begin{equation}
\begin{split}
\textrm{Likelihood:} \\
y_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{error}) \\
\mu_{[i]} = \alpha_{[i]} + \beta_{[i]} * x_{[i]}  \\ 
\alpha_{[i]} = Intercept + group_{[\mathrm{group}_{[i]}]} \\
\beta_{[i]} = pheight + height \colon group_{[\mathrm{group}_{[i]}]} \\ \\
\textrm{Priors:} \\
Intercept \sim t(3, 175, 100) \\
pheight \sim t(3, 0, 100) \\ 
group \sim t(3, 0, 100) \\ 
pheight \colon group \sim t(3, 0, 100) \\ 
\end{split}
(\#eq:516)
\end{equation}

### Fitting the model

We fit the model with group-dependent intercepts and slopes below:


```r
## Fit the model yourself, or
## download pre-fit model from: 
## github.com/santiagobarreda/stats-class/tree/master/models
## and load after placing in working directory
## single_line_model = readRDS ('5_group_multi_slope_model.RDS')

set.seed (1)
group_multi_slope_model =
  ##   f0 ~ group * pheight_c
  brm (f0 ~ group + pheight_c + pheight_c:group, 
       data=h95, chains=1,cores=1,warmup=1000,iter=6000,
       prior = c(set_prior("student_t(3, 175, 100)", class = "Intercept"),
                 set_prior("student_t(3, 0, 100)", class = "b")))
## save model
## saveRDS (group_multi_slope_model, '5_group_multi_slope_model.RDS')
```



### Interpreting the model

We can inspect this model below and see that it contains a relatively large number of parameters. This is a necessary outcome of the complexity of our research question. The model we fit looks for an effect for perceived height on f0, and allows the line relating these variables to vary between groups. Necessarily, this will require the model to present you with 4 intercepts and 4 slopes (or equivalent information), one for each group line.  

In fact, if we look below we see that our model has in fact estimated 8 fixed-effect coefficients: 4 slopes and 4 intercepts.


```r
## inspect model
group_multi_slope_model
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: f0 ~ group + pheight_c + pheight_c:group 
##    Data: h95 (Number of observations: 278) 
## Samples: 1 chains, each with iter = 6000; warmup = 1000; thin = 1;
##          total post-warmup samples = 5000
## 
## Population-Level Effects: 
##                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept          204.37      4.39   195.70   213.02 1.00     4426     3198
## group1             -20.32      7.92   -35.54    -4.94 1.00     2823     3138
## group2              17.31      8.70     0.15    33.97 1.00     2645     2656
## group3             -14.99      7.44   -29.37    -0.20 1.00     3081     3243
## pheight_c           -5.00      0.54    -6.07    -3.96 1.00     3257     3434
## group1:pheight_c    -2.33      1.06    -4.35    -0.25 1.00     2883     3140
## group2:pheight_c     2.84      0.98     0.95     4.75 1.00     3112     3394
## group3:pheight_c    -2.73      1.00    -4.71    -0.74 1.00     3746     2918
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    21.40      0.92    19.68    23.27 1.00     5004     3185
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

We can recover the overall (main effects) intercept and slope directly from the model estimates. We can get the group-specific intercept and slopes by adding the 'main effects' and specific interactions.


```r
group_multi_slope_hypothesis = 
  hypothesis (group_multi_slope_model,
              hypothesis = 
                c("Intercept = 0", ## overall intercept
                  "Intercept + group1 = 0", ## group 1 mean
                  "Intercept + group2 = 0", ## group 2 mean
                  "Intercept + group3 = 0", ## group 3 mean
                  "Intercept + -(group1+group2+group3) = 0", ## group 4 mean
                  "pheight_c = 0", ## overall slope
                  "pheight_c + group1:pheight_c = 0", ## group 1 slope
                  "pheight_c + group2:pheight_c = 0", ## group 2 slope
                  "pheight_c + group3:pheight_c = 0", ## group 3 slope
                  ## group 4 slope
                  "pheight_c +   
                  -(group1:pheight_c+group2:pheight_c+group3:pheight_c) = 0"))
```

```r
group_multi_slope_hypothesis[[1]][,2:5]
```

```
##      Estimate  Est.Error   CI.Lower     CI.Upper
## 1  204.371640  4.3888322 195.700834 213.01668927
## 2  184.055122  9.6549946 165.203107 203.07234380
## 3  221.682726 10.7287066 201.014970 242.32745786
## 4  189.381338  8.8824783 171.980463 207.12968030
## 5  222.367372  2.2426056 218.070029 226.80771577
## 6   -5.000915  0.5385537  -6.067112  -3.96349773
## 7   -7.326671  1.2577261  -9.761930  -4.84173802
## 8   -2.159917  1.1561613  -4.406119   0.04965291
## 9   -7.729564  1.1622074 -10.011286  -5.43739953
## 10  -2.787509  0.6934192  -4.168745  -1.40028689
```

The first four values above are intercepts, and the next four are slopes. These coefficients are presented in Figure \@ref(fig:F5-57). There appear to be gender-specific patterns in the intercept and slope coefficients between our groups. This pattern is evident when we use the line coefficients to draw each group-dependent line in the panel on the right. 

<div class="figure">
<img src="05_files/figure-html/F5-57-1.png" alt="(left) Group-specific intercepts. (middle) Group-specific slopes. (right) Lines for each group: boys (yellow), girls (green), men (red), and women (blue)." width="768" />
<p class="caption">(\#fig:F5-57)(left) Group-specific intercepts. (middle) Group-specific slopes. (right) Lines for each group: boys (yellow), girls (green), men (red), and women (blue).</p>
</div>

Below, we can see the incremental complexity of the models we have considered and how this complexity requires that our models have more and more coefficients. However, in this case the complexity is justified and reveals group-specific relationships between f0 and perceived height in our data.

<div class="figure">
<img src="05_files/figure-html/F5-group-slopes-1.png" alt="(left) Group-specific intercepts, slope is zero. (middle) Group-specific intercepts, slope is non-zero but fixed across groups. (right) Group-specific intercepts and group-specific slopes. In each panel, lines correspond to boys (yellow), girls (green), men (red), and women (blue)." width="768" />
<p class="caption">(\#fig:F5-group-slopes)(left) Group-specific intercepts, slope is zero. (middle) Group-specific intercepts, slope is non-zero but fixed across groups. (right) Group-specific intercepts and group-specific slopes. In each panel, lines correspond to boys (yellow), girls (green), men (red), and women (blue).</p>
</div>

## Plot Code




```r
################################################################################
### Figure 5.1
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1))
plot (f0 ~ pheight, data = h95, ylim = c(80,330),xlim = c(47,74), 
      pch=16,col=skyblue, xlab="Perceived Height (inches)", ylab = "f0 (Hz)") 
abline (h = mean (tapply (h95$f0, h95$group, mean)), col=skyblue, lwd=4)

plot (f0 ~ pheight, data=h95, ylim=c(80,330), xlim=c(47,74),ylab = "f0 (Hz)", 
      pch=16,col=cols[c(4,5,3,6)][h95$group], xlab="Perceived Height (inches)") 
abline (h = tapply (h95$f0, h95$group, mean), col=cols[c(4,5,3,6)], lwd=4)

################################################################################
### Figure 5.2
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1))
plot (f0 ~ pheight, data = h95, pch=16,col=yellow, cex=.7, ylim = c(70,340)) 
abline (h=209, col=deepgreen, lwd=3)

for (i in seq(50,70,5)){
  mu = 209
  y = seq(mu-104,mu+104,.1)
  x = dnorm (y ,mu, 52)
  x = x / max (x) * 3
  lines (i+x,y, lwd=2, col=lavender)
}

plot (f0 ~ pheight, data = h95, pch=16,col=yellow, cex=.7, ylim = c(70,340)) 
abline (lm(f0 ~ pheight,data=h95)$coefficients,col=deepgreen,lwd=3); 
abline (h=209, lty=3)

for (i in seq(50,70,5)){
  mu = 588.9 + i * -6.37
  y = seq(mu-60,mu+60,.1)
  x = dnorm (y ,mu, 30)
  x = x / max (x) * 3
  lines (i+x,y, lwd=2, col=lavender)
}

################################################################################
### Figure 5.3
################################################################################

single_line_model_coefficients = fixef (single_line_model)[,1]

par (mfrow = c(1,2), mar = c(4,4,1,1))
plot (f0 ~ pheight, data = h95, pch=16,col=4,ylim = c(70,340)); 
abline (single_line_model_coefficients,col=2,lwd=3); 
abline (h=mean(h95$f0), lty=3)

plot (f0 ~ pheight, data = h95, pch=16,col=4,xlim=c(0,75), ylim = c(0,600)); 
abline (single_line_model_coefficients,col=2,lwd=3); 
abline (h=mean(h95$f0), v = 0, lty=3)

################################################################################
### Figure 5.4
################################################################################

single_line_centered_model_coefficients = fixef (single_line_centered_model)[,1]

par (mfrow = c(1,2), mar = c(4,4,1,1))

plot (f0 ~ pheight, data = h95, pch=16,col=4); 
abline (single_line_model_coefficients,col=2, lwd=3); 
abline (h=mean(h95$f0),v=mean(h95$pheight), lty=3, lwd=3)

plot (f0 ~ pheight_c, data = h95, pch=16,col=4); 
abline (single_line_centered_model_coefficients,col=2, lwd=3); 
abline (h=mean(h95$f0),v=0, lty=3,lwd=3)


################################################################################
### Figure 5.5
################################################################################

par (mfrow = c(1,2), mar = c(4,4,1,1))

group_intercepts_coefficients = group_intercepts_hypothesis[[1]][,2]

plot (f0 ~ pheight_c, data=h95, ylim=c(80,330), xlim=c(-15,15),
      ylab = "f0 (Hz)", pch=16,col=cols[c(4,5,3,6)][h95$group], 
      xlab="Perceived Height (inches)") 
abline (h = group_intercepts_coefficients[2:5], col=cols[c(4,5,3,6)], lwd=4)
abline (h = group_intercepts_coefficients[1], lwd=3, lty=3)


group_single_slope_coefficients = group_single_slope_hypothesis[[1]][,2]

plot (f0 ~ pheight_c, data=h95, ylim=c(80,330), xlim=c(-15,15),
      ylab = "f0 (Hz)", pch=16,col=cols[c(4,5,3,6)][h95$group], 
      xlab="Perceived Height (inches)") 
for (i in 1:4) 
  abline (group_single_slope_coefficients[i+1], 
          group_single_slope_coefficients[6]
          , col=cols[c(4,5,3,6)][i], lwd=4)
abline (group_single_slope_coefficients[1], 
        group_single_slope_coefficients[6], lwd=3, lty=3)


################################################################################
### Figure 5.6
################################################################################

par (mfrow = c(1,3), mar = c(4.2,4.2,2,1))
brmplot (group_intercepts_effects[[1]][,2:5], col = 4, ylim = c(-90,45),
         labels = c("boys","girls","men","women"),ylab="f0 Effect (Hz)")
brmplot (group_single_slope_effects[[1]][-5,2:5], add = TRUE, col = 2,
         labels="")
abline (h=0,lty=3); 

group_intercepts_coefficients = group_intercepts_hypothesis[[1]][,2]

plot (f0 ~ pheight_c, data=h95, ylim=c(80,330), xlim=c(-15,15),
      ylab = "f0 (Hz)", pch=16,col=cols[c(4,5,3,6)][h95$group], 
      xlab="Perceived Height (inches)") 
abline (h = group_intercepts_coefficients[2:5], col=cols[c(4,5,3,6)], lwd=4)
abline (h = group_intercepts_coefficients[1], lwd=3, lty=3)


group_single_slope_coefficients = group_single_slope_hypothesis[[1]][,2]

plot (f0 ~ pheight_c, data=h95, ylim=c(80,330), xlim=c(-15,15),
      ylab = "f0 (Hz)", pch=16,col=cols[c(4,5,3,6)][h95$group], 
      xlab="Perceived Height (inches)") 
for (i in 1:4) 
  abline (group_single_slope_coefficients[i+1], 
          group_single_slope_coefficients[6]
          , col=cols[c(4,5,3,6)][i], lwd=4)
abline (group_single_slope_coefficients[1], 
        group_single_slope_coefficients[6], lwd=3, lty=3)



################################################################################
### Figure 5.7
################################################################################

labs = c("boys","girls","men","women")

par (mfrow = c(1,3), mar = c(4,4,1,1))

brmplot(group_multi_slope_hypothesis[[1]][2:5,2:5],labels = labs, cex=2,
        col = cols[c(4,5,3,6)], xlim = c(.75,4.25), ylim = c(155,250))
abline (h = 0)

brmplot(group_multi_slope_hypothesis[[1]][7:10,2:5],labels = labs,cex=2,
        col = cols[c(4,5,3,6)], xlim = c(.75,4.25), ylim = c(-11,0))
abline (h = 0, lty = 3)


group_multi_slope_coefficients = group_multi_slope_hypothesis[[1]][,2]

plot (f0 ~ pheight_c, data=h95, ylim=c(80,330), xlim=c(-15,15),
      ylab = "f0 (Hz)", pch=16,col=cols[c(4,5,3,6)][h95$group], 
      xlab="Perceived Height (inches)") 
for (i in 1:4) 
  abline (group_multi_slope_coefficients[i+1], 
          group_multi_slope_coefficients[i+6], col=cols[c(4,5,3,6)][i], lwd=4)
abline (group_multi_slope_coefficients[1], 
        group_multi_slope_coefficients[6], lwd=3, lty=3)

################################################################################
### Figure 5.8
################################################################################

par (mfrow = c(1,3), mar = c(4,4,1,1))

plot (f0 ~ pheight_c, data=h95, ylim=c(80,330), xlim=c(-15,15),
      ylab = "f0 (Hz)", pch=16,col=cols[c(4,5,3,6)][h95$group], 
      xlab="Perceived Height (inches)") 
abline (h = group_intercepts_coefficients[2:5], col=cols[c(4,5,3,6)], lwd=4)
abline (h = group_intercepts_coefficients[1], lwd=3, lty=3)


plot (f0 ~ pheight_c, data=h95, ylim=c(80,330), xlim=c(-15,15),
      ylab = "f0 (Hz)", pch=16,col=cols[c(4,5,3,6)][h95$group], 
      xlab="Perceived Height (inches)") 
for (i in 1:4) 
  abline (group_single_slope_coefficients[i+1], 
          group_single_slope_coefficients[6]
          , col=cols[c(4,5,3,6)][i], lwd=4)
abline (group_single_slope_coefficients[1], 
        group_single_slope_coefficients[6], lwd=3, lty=3)

group_multi_slope_coefficients = group_multi_slope_hypothesis[[1]][,2]

plot (f0 ~ pheight_c, data=h95, ylim=c(80,330), xlim=c(-15,15),
      ylab = "f0 (Hz)", pch=16,col=cols[c(4,5,3,6)][h95$group], 
      xlab="Perceived Height (inches)") 
for (i in 1:4) 
  abline (group_multi_slope_coefficients[i+1], 
          group_multi_slope_coefficients[i+6], col=cols[c(4,5,3,6)][i], lwd=4)
abline (group_multi_slope_coefficients[1], 
        group_multi_slope_coefficients[6], lwd=3, lty=3)
```

