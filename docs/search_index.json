[["logistic-regression.html", "Chapter 7 Logistic regression 7.1 Data and research questions 7.2 Generalizing our linear models 7.3 Logistic regression with one predictor 7.4 Answering our research question 7.5 Plot Code", " Chapter 7 Logistic regression In this chapter were going to talk about the prediction of categorical variables. These are variables that can take on a (usually small) number of discrete values. Were only going to talk about dichotomous (i.e. binary) outcomes for now, though the same ideas can be extended to model ordinal (ordered categories such as in 1st, 2nd, 3rd), and multinomial data (unordered categories such as English, French and Spanish). 7.1 Data and research questions Were going to use the data from our perceptual experiment discussed in Chapter 6. Listeners were presented with monosyllabic words, presented at random. For each trial, listeners reported the height of the speaker (in feet and inches) and guessed whether the speaker was a boy, girl, man or woman. library (brms) options (contrasts = c(&quot;contr.sum&quot;,&quot;cont.sum&quot;)) # source data url1 = &quot;https://raw.githubusercontent.com/santiagobarreda&quot; url2 = &quot;/stats-class/master/data/h95_experiment_data.csv&quot; h95 = read.csv (url(paste0 (url1, url2))) # set up colors for plotting devtools::source_url (paste0 (url1, &quot;/stats-class/master/data/colors.R&quot;)) # source functions devtools::source_url (paste0 (url1, &quot;/stats-class/master/data/functions.R&quot;)) In chapter 6 we saw that there is a relationship between perceived height and the f0 of the stimulus. The final model we considered in Chapter 5 suggested that this relationship may vary in a gender-dependent manner. In addition to size, listeners reported the age group (adult or child) and gender (female or male) of the speaker. In Figure 7.1, we see average reported height for each speaker as a function of f0, separately for males and females. Generally, a lower f0 results in the perception of a taller speaker. Figure 7.1: (left) Average perceived height for each speaker as a function of average f0. (middle) The same relationship as the left but only males are plotted. (right) Only females are plotted. Figure 7.2 shows the probability of observing a classification of adult (i.e., man or woman) for each speaker as a function of average f0. Clearly, the perception of size and the perception of adultness are related. This is not surprising as, in general, larger speaker are more likely to be adults. Although superficially they seem similar, we cant directly apply the models weve been using so far to explain variation in the perception of adultness. Figure 7.2: (left) Average adultness classifications for each speaker as a function of average f0. (middle) The same relationship as the left but only males are plotted. (right) Only females are plotted. When we modeled a normally distributed variable, we were predicting the mean but also the outcomes. What I mean by that is that the an observation that equals the mean is a perfectly reasonable prediction (the most likely one in fact!). However, adultness is a binary variable with outcomes of 0 (child, not adult) and 1 (adult) only. What are we predicting using our line? The line in the figure above goes from 0 to 1 but only exactly 0 and 1 (and nothing in between) are possible outcomes. There is no 0.4 child outcome being predicted! We can instead say that we are modeling the probability of observing a certain outcome, instead of the outcome itself. So, we could model the continuous probability (\\(p\\)) of observing an adult response using a line. However, this doesnt work well because the prediction line will predict values outside of 0 and 1, which doesnt work for probabilities. I mentioned in the first chapter that our models are like spherical models for billiards balls: not exactly right but close enough for many purposes. If you do just model \\(p\\) with a line like in Figure 7.2, your model will likely not be close enough for many purposes, potentially leading to major problems. For these reasons, we dont actually use linear models with Gaussian (normally-distributed) errors when modeling dichotomous variables. Instead, we use logistic regression, which is specifically intended to model binary outcomes. In this chapter, we will use a logistic regression model to investigate the perception of adultness from f0 acoustics, and the way that this may vary in a gender-dependent manner. 7.1.1 Dichotomous variables and data Dichotomous variables are ones that can take on only one of two possible discrete outcomes. We can easily think of many examples of this kind of data, perhaps the most obvious being something that is wrong or right. When you can only have two outcomes, you can arbitrarily set one to equal a success. Keep in mind this is not a judgment of any kind, and the other category could be chosen with no important change to your model (all coefficient signs flip, thats it). After we pick a category to be a success we then make all instances of that category equal to 1 and the other category equal to 0. This means we can take all observations of the variable and find the average to calculate the probability of a success. This is actually very obvious: If I am practicing basketball and add up all my made baskets (1 point each) and divide by the total number of shots (averaging), I will clearly get the percent (or probability) of baskets made overall. There are two main distributions used to model dichotomous variables: the binomial distribution and the Bernoulli distribution. The binomial distribution models a batch of multiple dichotomous outcomes. This distribution has two parameters: the probability of a success (\\(p\\)) and the number of trials (\\(n\\)). So for example, if I took 5 free throws and made 3 (3/5=0.6), then I say the data is likely to come from a binomial distribution with a \\(p\\) parameter of 0.6 and an \\(n\\) parameter of 5. If I use this distribution, I am treating all 5 trials as a single observation. The Bernoulli distribution models individual dichotomous outcomes. This distribution has only one parameter: the probability of a success (\\(p\\)). In this case, if I took 5 free throws and made 3 (3/5=0.6), I would still describe the data using a \\(p\\) parameter of 0.6. However, with this distribution, I would model each 5 trial as a separate observation so there is no \\(n\\) parameter (it is always 1). Below we generate random binomial variables (R doesnt specifically make Bernoulli variables). The rbinom function takes parameters in this order number of observations, batch size, probability of success. Below, I generate first a single Bernoulli variable (a Bernoulli trial), and then ten variables with the same probability of success. # a single trial, probability of 0.5 rbinom (1,1,.5) ## [1] 0 # ten single trials, probability of 0.5 rbinom (10,1,.5) ## [1] 1 0 1 1 1 0 0 1 1 0 Below we compare the data generated by the Bernoulli and the binomial distributions. In the top row we get a single number, the total number of successes in the trials. We dont get any information about what happened on any individual trial. In the bottom row we do get information about what happened on each individual trial. # a single batch of 10 trials, probability of 0.5 rbinom (1,10,.5) ## [1] 7 # ten individual trials, probability of 0.5 rbinom (10,1,.5) ## [1] 0 0 1 0 0 0 1 0 1 0 Unlike the normal distribution, these distributions do not directly generate data with values near their mean. Instead, they generate sequences of 1s and 0s, the average of which (as the number of observations approach infinity) is expected to converge on the \\(p\\) parameter of the distribution that generated the data. For example, below I generate sequences of a dichotomous variable with a \\(p\\) parameter of 0.5. In each case, the estimate of the probability of the distribution gets closer as the length of the sample gets longer. set.seed (1) mean (rbinom (10,1,.5)) # the mean of 10 observations ## [1] 0.6 mean (rbinom (100,1,.5)) # the mean of 100 observations ## [1] 0.47 mean (rbinom (1000,1,.5)) # the mean of 1000 observations ## [1] 0.481 mean (rbinom (100000,1,.5)) # the mean of 100000 observations ## [1] 0.50009 So, when we have a binary outcome variable and are talking about individual trials, we model it as being generated by a Bernoulli distribution. This distribution has a single parameter \\(p\\) so that our data model is \\(adult \\sim \\mathrm{Bernoulli} (p)\\). Unlike normally distributed data, there are only two outcomes (0 and 1) and variation in \\(p\\) is bound to be between 0 and 1. If we are modeling multiple Bernoulli trials as a single observation, we can use the binomial distribution and our data model would be \\(adult \\sim \\mathrm{Binomial} (p, n)\\) where \\(p\\) is again bound to be between 0 and 1, and \\(n\\) must be a positive integer. 7.2 Generalizing our linear models The general linear model is the extension of the same general design used in regular regression (featuring normally distributed errors) to models for other kinds of variables. Our regression models consist of a bunch of component parts stuck together. Early on I introduced two general parts: the random components and the systematic component. Our model predicting perceived height from f0 is: pheight ~ g0_s + (g0_s|subj) + (1|speaker) We know that this model is basically this: \\[\\begin{equation} \\begin{split} pheight_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\mu_{[i]} = a_{[i]} + b_{[i]} * \\mathrm{x}_{[i]} \\\\ ... \\end{split} \\tag{7.1} \\end{equation}\\] Which says we expect our variable to be normally distributed, and we expect variation in the mean to vary along a line. These two parts are called the random component and the systematic component respectively. The random component specifies how our data randomly varies given some parameter (\\(\\mu\\) for normal distributions). The systematic component predicts variation in the parameter of interest using shapes like lines and planes. Generalized linear models use what are know as link functions. We havent dealt with this at all yet because the normal distribution does not need one. For normally distributed data we directly model variation in \\(\\mu\\) so there is no need for a link function. We could add a link function in between the random and systematic components as below: \\[\\begin{equation} \\begin{split} pheight_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\mu_{[i]} = f(\\theta_{[i]}) \\\\ \\theta_{[i]} = a_{[i]} + b_{[i]} * \\mathrm{x}_{[i]} \\\\ ... \\end{split} \\tag{7.2} \\end{equation}\\] Where \\(f()\\) represents the link function. Regression models with normally distributed errors use what is called the identity link function, which basically means that they model a parameter that directly translates into the \\(\\mu\\) parameter (\\(f(x)=x\\)). This is equivalent to modeling the \\(\\mu\\) parameter directly in our prediction equation. Why can we directly model variation in \\(\\mu\\) with our prediction equation? The key is that normally distributed data is supposed to be continuous and extend from positive to negative infinity. Of course this is only in principle, but it still means that in principle, you can model \\(\\mu\\) using lines since this might actually vary continuously along them. In contrast, the probability of an event occurring must be between 0 and 1, so it obviously does not extend to infinity. As a result, even if we did switch to the Bernoulli distribution we could not just do something like this: \\[\\begin{equation} \\begin{split} adult_{[i]} \\sim \\mathrm{Bernoulli}(p_{[i]}) \\\\ p_{[i]} = \\theta_{[i]} \\\\ \\theta_{[i]} = a_{[i]} + b_{[i]} * \\mathrm{x}_{[i]} \\\\ \\end{split} \\tag{7.3} \\end{equation}\\] Because this would cause weird things like predictions with negative probabilities. Instead, we need to employ an actual link function in the second step above (e.g., \\(f(\\theta_{[i]})\\)), in order to relate \\(p\\) into something we can model using lines. 7.2.1 Link functions A link function allows you to link variation in a parameter to variation in straight lines. Modeling becomes a three step process like this: Predict variation of some parameter along straight lines (or related shapes), for example \\(\\theta = a + b * \\mathrm{x}\\). Transform the parameter using a link function (\\(p = f(\\theta)\\)). Use the transformed parameter in data generating distribution (e.g., \\(adult \\sim \\mathrm{Bernoulli}(p)\\)) For dichotomous variables, its common to predict outcomes in units called logits (step 3). Then, logits are then turned into probabilities (\\(p\\)) using the logistic link function (step 2). The parameter \\(p\\) can then be used together with an appropriate probability distribution to model the data. 7.2.2 Logits Logits are log-odds, the logarithm of the odds of a success. The odds of a success is defined as: odds = total number of successes / total number of failures Odds of 3/1 indicate that success is three times as likely as a failure. We can turn this into a probability with the following calculation: probability = total number of successes / (total number of successes + total number of failures) So, odds of 3/1 imply a probability of 0.25 (3 / (3+1)). Odds are still bounded by zero so they dont work for linear modeling, but if we take the logarithm of the odds we get a logit: a value that extends continuously from positive to negative infinity. This is because logarithms represent the space from 0 to 1 with values from \\(-\\infty\\) to 0, and values from 1 to \\(+\\infty\\) with values from 0 to \\(+\\infty\\). We can calculate the logit (\\(z\\)) with either of the two equivalent calculations: logit = log (probability of successes / probability of failures) logit = log (probability of successes) - log (probability of failures) I wrote a function that calculates logits from probabilities (ptoz) seen below. Note that I have a special case for situations where \\(p\\) is equal to 0 or 1, where I arbitrarily change those to 0.99 and 0.01. Since the log(0)=\\(-\\infty\\), the logit of probabilities of 0 and 1 and positive and negative infinity respectively. One way to think about this is is that to really know that something is 1 and not, for example 0.9999., you would have to observe an infinite number of successes with no failures, leading to infinite odds (successes/failures) and log odds. Infinite logit values are not useful for us (and cant be plotted), so the ptoz function sets extreme (but manageable) values for probabilities of 0 and 1. # changes probabilities (p) to logits ptoz = function (p){ p[p==1] = .99 # if p=1, change to 0.99 p[p==0] = .01 # if p=0, change to 0.01 (i.e. 1-0.99) log (p) - log(1-p) } Our logistic regression models are going to predict logits. Our lines will describe continuous changes in logits. Our intercepts will describe shifts on the values of logits across several categories. Given a predicted value expressed in logits, we can then use the logistic function (sometimes called the antilogit function) to convert this value into a probability. For this reason, the logistic function is said to be the link function for logistic regression: the logistic function links the prediction made by our model (in logits) to the parameter used in the data-generating function assumed by our model (\\(p\\) in a Bernoulli distribution). 7.2.3 The logistic link function The logistic link function looks like below, for values of \\(z\\) ranging continuously from positive to negative infinity, where \\(e\\) is the mathematical constant used for natural logarithms. \\[ f(z) = \\frac{1}{1 + e^{-z}} \\tag{7.4} \\] Ive written this simple R function that implements the logistic link: # changes logits to probabilities. ztop = function (z) 1 / (1 + exp(-z)) The function may look complicated but its a bit simpler when you remember than any number raised to the power of zero is 1. So \\(e^0=1\\), meaning that when \\(z=0\\), \\[ \\frac{1}{1 + e^{0}}=\\frac{1}{1 + 1}=0.5 \\tag{7.5} \\] When \\(z\\) is a positive value, \\(e^{-z}\\) will be a fraction between 0 and 1, for example if \\(z=3\\) then \\(e^{-z}=e^{-(3)}=0.05\\). This means that when \\(z&lt;0\\), we expect probabilities between 0.5 and 1, as in: \\[ \\frac{1}{1 + e^{-3}}=\\frac{1}{1 + 0.05}=0.95 \\tag{7.6} \\] On the other hand, when \\(z\\) is negative, \\(e^{-z}\\) will be a positive number greater than 1, as in \\(e^{-z}=e^{-(-3)}=e^{3}=20.1\\). Thus, for negative values of \\(z\\) we expect probabilities between 0 and 0.5 as in: \\[ \\frac{1}{1 + e^{3}}=\\frac{1}{1 + 20.1}=0.05 \\tag{7.7} \\] In Figure 7.3 I draw a line with a slope of one and an intercept of 0 (i.e., y = x). We can imagine that this line defines expected changes in logits as a function of some predictor x. For example, where we might directly model change in perceived height (in inches) as a function of continuous variation in f0, we model the change in the logit of the probability of observing an adult response, as a function of f0. In the middle panel, weve applied the logistic function to the line, resulting in a sigmoid curve. If we apply the logit function to this sigmoid curve, the result is a line again. We can see that the logit and sigmoid functions/transformations are just in the inverse of each other. They can be applied to data to go back and forth between a probability or logit interpretation. Figure 7.3: (left) A plot of a line with a slope of 1 and intercept of 0. We can treat the y-axis as logits. (middle) The result of applying the logistic function to every point of the line in the left column. (right) Calculating the logit of each value specified on the curve in the middle turns our sigmoid curve back to a line. In chapter 5 I mentioned that when you do a linear regression you model the data as normal distributions sliding along a line, generating normally distributed data along the line as they move. In logistic regression you model the data as a Bernoulli distribution sliding along the sigmoid curve above, generating 1s and 0s as it moves. This is accomplished by modeling variation in logits (\\(z\\)) using our regression models, and only converting to a probability (\\(p\\)) after prediction. In other words, our model consists of the three elements seen below: the data generation (random component), the link function, and our prediction equation (systematic component). \\[\\begin{equation} \\begin{split} adult \\sim \\mathrm{Bernoulli}(p) \\\\ p = \\mathrm{logistic} (z) \\\\ z = a + b * \\mathrm{x} \\\\ \\end{split} \\tag{7.8} \\end{equation}\\] Keep in mind that we could put the prediction equation directly inside the logistic function: \\[ p = \\frac{1}{1 + e^{-(a + b*x)}} \\tag{7.9} \\] And even put the output of that inside the Bernoulli distribution directly. \\[ adult \\sim \\mathrm{Bernoulli}(\\frac{1}{1 + e^{-(a + b*x)}}) \\\\ \\tag{7.10} \\] When you look at it this way, it shows that our link function really does link our prediction equation (a + b*x) and our data distribution (Bernoulli). The fomulation above is reminiscent of this formulation of our model for normally distributed data: \\[ pheight \\sim \\mathrm{Normal}(a + b*x, \\sigma) \\\\ \\tag{7.11} \\] 7.2.4 Building intuitions about logits The logit (and logistic) functions are non linear functions. In the simplest sense this means that they take lines and make them into non lines, and vice versa. This can be seen below where the left and right panels present the equivalent relation in logits and probabilities. Another consequences of the non-linearity of the transformation is that a given unit increase in probabilities does not equal a given unit increase in logits. The reverse is also true. This can clearly be seen in the figure below which has horizontal lines from 0.1 to 0.9, spaced every 0.1 units of probability. We see that this results in equally-spaced lines on the right, but not on the left. Figure 7.4: (left) A line relating some predictor to logits. (right) A sigmoid curve expressing the probability associated with each value along the line in the left. Horizontal lines are places every 0.1 probability units from 0.1 to 0.9 probability. We can easily check what the logit values are for a set of probabilities using the ptoz function. The top row below contains a sequence of probabilities and the bottom row shows equivalent logits. rbind ( (seq (0.1,.9,.1)), round ( ptoz (seq (0.1,0.9,.1)) , 3) ) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 0.100 0.200 0.300 0.400 0.5 0.600 0.700 0.800 0.900 ## [2,] -2.197 -1.386 -0.847 -0.405 0.0 0.405 0.847 1.386 2.197 Notice that the difference between 0.5 and 0.6 is 0.4 logits, but the difference between 0.7 and 0.9 is about 0.8 logits. Meanwhile the difference between 0.9 and 1 is infinity! Imagine that you keep track of you free throw practice in basketball. You sink 500/1000 free throws, giving you a 0.5 probability of success. Now imagine you take a further 100 and sink them all. Now your probability of success is 600/1100, meaning your amazing streak has increased your probability to 0.54. However, suppose that you had been a 900/1000 shooter, with a probability of 0.9. If you had the same streak of 100 baskets, you would only have increased your probability to 90.1 (1000/1100). We can see that the same increase results in a large increase in one probability (0.5 -&gt; 0.54, almost 10%) and a minuscule change in another (0.9 -&gt; 0.901, about 1%). Basically, as you approach 0 and 1 it gets harder to make large changes in your probabilities. The result of this is that a given logit difference will equal a large probability difference near 0 logits, and a smaller probability difference as the underlying logit values are further from 0. Here are some useful things to keep in mind when interpreting logits: 50% is 0. Positive means more likely to be a success, negative means more likely to be a failure. -3 and 3 are 4.7% and 95.2%. Basically -3 and 3 logits are useful bounds for almost always and almost never\". Since a logit of 3 translates to a \\(p\\) of about 0.95, all of the space between +3 and infinity logits represents the probability space between 0.95 and 1, while logits between 0 and 3 represent the space from 0.5 to 0.95. Logits far beyond 3 might not have much practical significance. A logit of 6 is 99.7%, a 4.5% increase over 3 logits. The logits have doubled but the probability has barely changed for most purposes. Also, it is very difficult to distinguish 95% and 99% in practice since by definition, you will be observing very few mistakes to distinguish the two! Effects can be considered important or not based on how far they got you along -3 to 3 (or -4 to 4). Basically, anything in the +1 range is very likely to matter, while effects smaller than 0.2 or so are likely having only small effects on outcomes. Here is one final thing to keep in mind: you must consider your model effects as logits before transforming them into probabilities. This important constraint follows directly from the fact that a given logit difference can lead to varying differences for different probability values. For example, imagine that we have an intercept of 1 and an effect of 2 for some group, expressed in logits. This results in an expected probability of: ztop (1 + 2) ## [1] 0.9525741 For that group. In contrast, if we had first converted to logits and tried to add after: ztop (1) + ztop(2) ## [1] 1.611856 We get something that is not even a valid probability. 7.3 Logistic regression with one predictor Were going to fit a model to our data that predicts the perception of adultness using logistic regression. 7.3.1 Description of the model The model would be quite familiar to use if it were dealing with normally distributed data. The formula for the model we are using looks like this: padult ~ g0_s*pfemale + (g0_s*pfemale|subj) + (1|speaker) Where padult represents perceived adultness, and pfemale represents perceived female (more on these later). Were going to use scaled log-f0 (g0_s) instead of just the centered version of the predictor. When we scale predictors we first center them and then divide by the standard deviation. This causes our predictor to have a standard deviation of 1 and a mean of 0. g0_m = mean ( log(h95$f0) ) g0_sd = sd ( log(h95$f0) ) h95$g0_s = (log( h95$f0 ) - g0_m) / g0_sd When we scale predictors, the slope coefficients will reflect changes in our dependent variable per 1 standard deviation difference in the predictor (rather than a 1 unit change). Scaling is very useful for logistic regression because it make most parameters have small values (&lt;10 or so) by making the range of any predictor mostly fall within -4 and 4. If you dont scale your continuous predictors, you can end up with slope coefficients like 200 when the predictor has very large values. Since we know that 3 logits is already 95%, coefficient values as large as 200 are difficult to interpret quickly. The formula above says tells our model to predict perceived adultness using scaled log f0 (g0_s), information about whether the speaker was identified as female or male, and the gender-dependent use of f0. It allows for random by-subject effects for all predictors and random intercepts for speaker. Our full model specification is now: \\[\\begin{equation} \\begin{split} \\textrm{Likelihood:} \\\\ padult_{[i]} \\sim \\mathrm{Bernoulli}(p_{[i]}) \\\\ p_{[i]} = \\mathrm{logistic} (z_{[i]}) \\\\ z_{[i]} = a_{[i]} + b_{[i]} * \\mathrm{x}_{[i]} \\\\ a_{[i]} = Intercept + pfemale_{[i]} + \\alpha_{[\\mathrm{subj}_{[i]}]} + \\alpha_{pfemale[\\mathrm{subj}_{[i]}]}+ \\alpha_{[\\mathrm{speaker}_{[i]}]} \\\\ b_{[i]} = g0\\_c + g0\\_s \\colon pfemale_{[i]} + \\beta_{[\\mathrm{subj}_{[i]}]} + \\beta_{g0\\_s \\colon pfemale[\\mathrm{subj}_{[i]}]} \\\\ \\\\ \\textrm{Priors:} \\\\ \\alpha_{speaker} \\sim \\mathrm{Normal}(0,\\sigma_{speaker}) \\\\ \\\\ \\begin{bmatrix} \\alpha_{subj} \\\\ \\alpha_{pfemale,subj} \\\\ \\beta_{subj} \\\\ \\beta_{g0\\_s \\colon pfemale,subj} \\\\ \\end{bmatrix} \\sim \\mathrm{MVNormal} ( \\begin{bmatrix} 0\\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix}, \\Sigma) \\\\ \\\\ Intercept \\sim t(3, 0, 5) \\\\ g0\\_c \\sim t(3, 0, 3) \\\\ pfemale \\sim t(3, 0, 3) \\\\ g0\\_c \\colon female \\sim t(3, 0, 3) \\\\ \\\\ \\sigma_{\\alpha_{speaker}} \\sim t(3, 0, 4) \\\\ \\sigma_{\\alpha_{subj}} \\sim t(3, 0, 4) \\\\ \\sigma_{\\alpha_{pfemale,subj}} \\sim t(3, 0, 4) \\\\ \\sigma_{\\beta_{g0\\_s \\colon pfemale, subj}} \\sim t(3, 0, 4) \\\\ \\sigma_{\\beta_{subj}} \\sim t(3, 0, 4) \\\\ R \\sim \\mathrm{LKJCorr} (2) \\end{split} \\tag{7.12} \\end{equation}\\] Were treating our adultness judgments (1 or 0 for adult and child) as coming from a Bernoulli distribution with a probability that varies trial to trial. The logit of the probability (z) varies along lines. The lines are specified by intercepts and slopes that vary trom trial to trial, and there is a single continuous predictor (g0_s). The intercept of these lines vary based on an overall intercept (the main effect), an overall effect for the perception of a female speaker (pfemale), subject-specific deviations from the mean, subject-specific deviations from the female effect, and speaker-specific deviations from the mean. The slope of these lines vary based on an overall slope (the main effect), subject-specific deviations from the average slope, and subject-specific deviation from the slope based on whether the speaker was identified as female or not. The speaker intercepts were drawn from a normal distribution with a mean of zero and a standard deviation estimated from the data. The subject random effects were drawn from a normal distribution with means of 0 of zero and a covariance matrix estimated from the data. All other effects (e.g., the Intercept, g0_s, etc.) were treated as fixed and drawn from prior distributions appropriate for their expected range of values (e.g., subj ~ t(3,0,3)). That model specification is pretty long, and our models are not even that complicated yet. Going forward Im going to start skipping some or all of the model specification, and provide a verbal description of our models based on the brm function calls we use to fit them. Weve already covered all the components of these models so you can always look back at previous chapters to see how to translate model formulas to their model specifications. For example, here is the function call we need to run the model described above: logistic_g0 = brm (padult ~ g0_s*pfemale + (g0_s*pfemale|subj) + (1|speaker), data=h95, chains=4, cores=4, family=&quot;bernoulli&quot;, warmup=1000, iter = 11000, thin = 10, control = list(adapt_delta = 0.95), prior = c(set_prior(&quot;student_t(3, 0, 5)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 3)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 4)&quot;, class = &quot;sd&quot;), set_prior(&quot;lkj_corr_cholesky (2)&quot;, class = &quot;cor&quot;))) Every piece of information in the model description above is predictable from this function call. The model structure is predictable from the model formula. The family parameter tells brm that our model is analyzing Bernoulli distributed data. When you set family=\"bernoulli\", brm knows to use the logistic link function. The priors can be understood from the call if we remember that: class = \"Intercept\" sets the prior only for the intercept. class = \"b\" sets the prior only for all fixed effect (i.e. what brm calls population level) predictors, other than the intercept. class = \"sd\" sets the prior for standard deviation parameters for our random effects. class = \"corr\" sets the prior for correlation matrices needed to build the covariance matrix for our multivariate normal distributions (discussed in chapter 6). 7.3.2 Fitting the model Were going to fit the model outlined above, except I am just going to use a standard deviation of 3 for everything. These distributions have most of their mass between -9 and +9, which are relatively large values for logits. In general we expect factors and scaled continuous predictors to mostly have values under &lt;10, or for values under 10 to be most meaningful to us. Im also going to set up my padult and pfemale variables. We are going to arbitrarily call adult responses 1 and child responses 0. Below, I create a new vector called padult that equals 1 if the perceived group (pgroup) is woman (w) or man (m) and 0 if the perceived group is girl (g) or boy (b). This will be our depndent variable. h95$padult = as.numeric(h95$pgroup %in% c(&#39;m&#39;,&#39;w&#39;)) as.character (h95$pgroup[1:10]) # original group label ## [1] &quot;g&quot; &quot;g&quot; &quot;b&quot; &quot;g&quot; &quot;w&quot; &quot;b&quot; &quot;b&quot; &quot;w&quot; &quot;b&quot; &quot;w&quot; h95$padult[1:10] # 0,1 dependent variable ## [1] 0 0 0 0 1 0 0 1 0 1 Our model also needs a predictor indicating perceived female gender. Rather than using a factor, we are going to directly sum code this variable ourselves. Since there is only two levels, we can set female responses (g or w) to 1 and male responses (b or m) to -1. The slope associated with this predictor reflects the effect for perceived femaleness, while the negative of the slope reflects the effect for perceived maleness. This is how sum coding is implemented! h95$pfemale = 2*(as.numeric(h95$pgroup %in% c(&#39;g&#39;,&#39;w&#39;)) - 0.5) as.character (h95$pgroup[1:10]) # original group label ## [1] &quot;g&quot; &quot;g&quot; &quot;b&quot; &quot;g&quot; &quot;w&quot; &quot;b&quot; &quot;b&quot; &quot;w&quot; &quot;b&quot; &quot;w&quot; h95$pfemale[1:10] # -1,1 predictor variable ## [1] 1 1 -1 1 1 -1 -1 1 -1 1 We fit the model below: # Fit the model yourself, or # download pre-fit model from: # github.com/santiagobarreda/stats-class/tree/master/models # and load after placing in working directory # logistic_g0 = readRDS (&#39;7_logistic_g0.RDS&#39;) set.seed (1) logistic_g0 = brm (padult ~ g0_s*pfemale + (g0_s*pfemale|subj) + (1|speaker), data=h95, chains=4, cores=4, family=&quot;bernoulli&quot;, warmup=1000, iter = 11000, thin = 10, control = list(adapt_delta = 0.95), prior = c(set_prior(&quot;student_t(3, 0, 3)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 3)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 3)&quot;, class = &quot;sd&quot;), set_prior(&quot;lkj_corr_cholesky (2)&quot;, class = &quot;cor&quot;))) # save model saveRDS (logistic_g0, &#39;7_logistic_g0.RDS&#39;) 7.3.3 Interpreting the model The estimates and credible intervals in the model printout below are being expressed in logits, however, most of this model output is the same as it was for our Gaussian-error models. There are two noteworthy differences: The top of the model now indicates Family: bernoulli and Links: mu = logit. Notice the absence of the Family-Specific parameter section of the model where sigma (i.e., \\(\\sigma_{error}\\)) was usually found. logistic_g0 ## Family: bernoulli ## Links: mu = logit ## Formula: padult ~ g0_s * pfemale + (g0_s * pfemale | subj) + (1 | speaker) ## Data: h95 (Number of observations: 2780) ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~speaker (Number of levels: 139) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 2.77 0.35 2.18 3.52 1.00 2842 3647 ## ## ~subj (Number of levels: 10) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS ## sd(Intercept) 1.32 0.41 0.74 2.34 1.00 3947 ## sd(g0_s) 1.37 0.42 0.71 2.36 1.00 4027 ## sd(pfemale) 0.28 0.22 0.01 0.80 1.00 3934 ## sd(g0_s:pfemale) 1.29 0.47 0.61 2.45 1.00 4124 ## cor(Intercept,g0_s) -0.45 0.26 -0.85 0.13 1.00 3278 ## cor(Intercept,pfemale) 0.16 0.37 -0.60 0.80 1.00 3893 ## cor(g0_s,pfemale) -0.13 0.36 -0.75 0.61 1.00 3833 ## cor(Intercept,g0_s:pfemale) 0.10 0.30 -0.48 0.66 1.00 4077 ## cor(g0_s,g0_s:pfemale) -0.48 0.26 -0.88 0.13 1.00 3762 ## cor(pfemale,g0_s:pfemale) -0.01 0.36 -0.68 0.67 1.00 3818 ## Tail_ESS ## sd(Intercept) 4013 ## sd(g0_s) 3797 ## sd(pfemale) 4056 ## sd(g0_s:pfemale) 3885 ## cor(Intercept,g0_s) 3952 ## cor(Intercept,pfemale) 3925 ## cor(g0_s,pfemale) 4057 ## cor(Intercept,g0_s:pfemale) 3808 ## cor(g0_s,g0_s:pfemale) 3842 ## cor(pfemale,g0_s:pfemale) 3926 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.68 0.56 -1.78 0.46 1.00 2956 3572 ## g0_s -3.29 0.59 -4.50 -2.17 1.00 2940 3404 ## pfemale 2.31 0.25 1.84 2.80 1.00 3297 3617 ## g0_s:pfemale 1.18 0.55 0.12 2.30 1.00 4232 4090 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our model is basically three lines relating perceived adultness and f0: the overall (main effects) line, the line for when subjects indicated hearing a male speaker, and the line for for subjects indicated hearing a female speaker. We recover the parameters for these lines by adding the appropriate terms using the hypothesis function. logistic_g0_hypothesis = hypothesis (logistic_g0, hypothesis = c(&quot;Intercept = 0&quot;, # overall intercept &quot;Intercept + pfemale = 0&quot;, #female intercept &quot;Intercept - pfemale = 0&quot;, # male intercept &quot;g0_s = 0&quot;, # overall slope &quot;g0_s + g0_s:pfemale = 0&quot;, # female slope &quot;g0_s - g0_s:pfemale = 0&quot;) # male slope ) logistic_g0_hypothesis[[1]][,2:5] ## Estimate Est.Error CI.Lower CI.Upper ## 1 -0.6751557 0.5571161 -1.7789047 0.4633089 ## 2 1.6316161 0.6212552 0.4057239 2.8991426 ## 3 -2.9819274 0.5970852 -4.1640600 -1.7486627 ## 4 -3.2941081 0.5917762 -4.5000468 -2.1679406 ## 5 -2.1140444 0.7215239 -3.5229921 -0.6624104 ## 6 -4.4741717 0.8851670 -6.3591675 -2.8303016 We can use these parameters to plot lines predicting the logit of the probability of a male response given f0. Below, we see these lines overall, and separately for voices judged to be male or female. A comparison of each prediction line to the classifications of the voices in our model show that this model offers reasonable predictions of the trends in the data. Figure 7.5: (left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted. Below we see what happens when we apply the logistic transform on the lines above. The result is sigmoid curves that represent expected variation in the \\(p\\) parameter of a Bernoulli distribution as a function of f0. This variation is along straight lines in the logit space but not in the probability space. Obviously, these curves are a much better fit for the data than the lines we originally used in Figure 7.2. Figure 7.6: (left) Curve indicating the relationship between f0 and the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted. 7.4 Answering our research question In the beginning of the chapter, we considered a pretty broad research question: how is f0 used by listeners to determine adultness? How does this vary based on the perception of gender? In this final section were going to try to answer this question using the information presented in our model below, focusing mainly on the fixed effects. Keep in mind, I dont have a right answer or correct interpretation of the data that I am working towards. This is just an example of the way you might approach understanding real data in order to see what you can learn about your research question. fixef (logistic_g0) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.6751557 0.5571161 -1.7789047 0.4633089 ## g0_s -3.2941081 0.5917762 -4.5000468 -2.1679406 ## pfemale 2.3067717 0.2466901 1.8402720 2.8007507 ## g0_s:pfemale 1.1800637 0.5494175 0.1188863 2.2988926 We can consider the fixed effects more easily by plotting them as in the left panel in Figure 7.7. The most important thing to remember when interpreting the coefficients of a logistic model is that positive coefficients push us towards an adult response, negative values push us towards a child response, and a predicted value of 0 means the outcome is 50/50. The right panel in Figure 7.7 shows us a summary of our data, with the main effects classification line drawn on top of it. The negative effect for f0 tells us that our line relating f0 to logits has a negative slope. Therefore, as f0 increases, we are less likely to observe an adult response and more likely to observe a child response. The model intercept is the value of the line when x = 0. We can see this below as the value of y where our diagonal line crosses the vertical dotted line. Since we scaled our f0 predictor, our intercept corresponds to the expected logit value when given a mean log f0. So, the intercept of our model reflects the probability of observing a response of adult at the average log f0. We can see that when scaled log f0 = 0, there is a small negative value, meaning we have a slight expectation of observing a child response. When we use models based on logits, we might actually be interested in the x intercept of our lines (or planes). The x intercept of our line is the value of our line along x where y is equal to zero. Why might we care about this? Well, when y = 0, that means that the probability of classification is 0.5. So, crossing the x intercept one way means you are more likely to see a success, and crossing the other way means you are more likely to see a failure. In other words, the x intercept of these models tells us about the location of the category boundary between our two categories, with respect to the continuous predictor. Figure 7.7: (left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted. The location of the x intercept is presented using a bold vertical line in the 7.7. This is the location where our prediction line intersects with the x axis. We can find the x intercept by setting y=0 in our prediction equation and solving for (i.e. isolating) x, as seen below. For complicated prediction equations (and even for simple ones), I often rely on websites like this one. \\[ \\begin{equation} \\begin{split} y = a + b*x \\\\ 0 = a + b*x \\\\ -a = b*x \\\\ -a/b = x \\end{split} \\tag{7.13} \\end{equation} \\] We the above equation to calculate our category boundary between adult and child classifications. Remember that to do arithmetic operations on our parameters, we have to use the original samples and not the summaries. For example, based on the numbers in our printout above, we expect that the category boundary is at -(a/b) = -(-0.67 / -3.31) = -0.20. Instead, when I do the same calculation using our original samples below, we see that the boundary is actually nearer to -0.22. # get original samples fixef_samples = fixef (logistic_g0, summary = FALSE) # calculate boundary = -a/b boundary = -fixef_samples[,&quot;Intercept&quot;] / fixef_samples[,&quot;g0_s&quot;] posterior_summary (boundary) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] -0.2241527 0.2042887 -0.6675019 0.1332786 The units above are not super useful. After fitting our model, we can convert our parameters back to unscaled log-Hz and then even to regular Hertz values. Below, we calculate the mean and standard deviations of the original variable. We then use these to undo our scaling operation by first multiplying by the standard deviation and then adding the mean (in that order!). We can then exponentiate these unscaled log-Hz values to get Hz ranges from the model parameters. This is basically like, imagine your model was specified in centimeters. If you wanted to understand it in meters, it seems obvious that it would be ok to divide everything by 100. If the model were somehow radically different just because you divided the output by 100, that would obviously be a big problem for us. Scaling and unscaling predictors are these kinds of changes, they dont really hurt our ability to make reliable inferences as long as we do th operation to all of the samples, and then summarize. Below I unscale and exponentiate the data, before using the posterior_summary function to get a summary of the samples. Note that we can use this information to get a point estimate for the boundary, in addition to getting information about uncertainty in the estimate of the boundary (via the credible interval). h95_sd = sd(h95$g0) # calculate data mean h95_mean = mean(h95$g0) # calculate data sd boundary_unscaled = boundary * h95_sd + h95_mean # unscale data boundary_hz = exp (boundary_unscaled) # exponentiate to get hertz posterior_summary (boundary_hz) # check out boundary statistics ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 178.1333 10.55043 155.8174 197.7817 In Figure 7.8 I present a histogram of the posterior distribution of the x-intercept of the line in our model. As we can see, our model suggests that this is likely between 160 and 200, with the mean being at around 180 Hz. We can see this distribution presented along with our data and main effects line in the figure on the right below. Notice that the plot below uses Hz labels rather than log-Hz. Our plots so far had ticks at -2, -2, 0 and 1 scaled log g0. I used the process above to calculate the Hz value corresponding to each of those scaled log-Hz values as shown below: exp ( c(-2,-1,0,1) * h95_sd + h95_mean ) ## [1] 104.7793 141.1276 190.0852 256.0263 And then used these values to make the labels for the figure. Figure 7.8: (left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted. It can be useful to think about category boundaries when interpreting logits because the effects in our model can be interpreted as shifts in these boundaries. For example, consider Figure 7.9 which compares our data with the lines generated by our model. Our model intercept estimate was -0.66, and the effect for pfemale was 2.3. That means that when subjects indicated hearing a female speaker, the lines representing their judgments were shifted up by 2.3 logits. Keep in mind this shift occurs when x=0, so if you havent centered your continuous predictors this is all more complicated, especially if your lines also have different slopes. However, when a line with a fixed negative slope is shifted up its x-intercept must increase in value. This causes the category boundary to move to the right on our plots. We can see this below, the higher intercept for the teal line moves the adult/child boundary higher along f0 for speakers perceived as female. We can see the opposite effect for the male line (in red), where the effect of -2.3 (-pfemale) results in a lower line and a leftward move for the category boundary. Our model slope was -3.3, with the g0_s:pfemale interaction equaling 1.18. This means that when speakers were identified as female, slopes were smaller in magnitude (-3.3 + 1.18) than when speakers were identified as male (-3.3 - 1.18). Increasing the magnitude of our slopes (positive or negative) without changing intercepts does not affect the location of the category boundary. Instead, it results in more categorical, less fuzzy classifications. When lines differ in both slopes and intercepts, the effect on classification boundaries needs to be considered on a case by case basis. However in general it is quite straightforward, one only needs to imagine the effects on our lines and the locations where they will cross 0. Below, we can see that the intercept and slope differences between speakers identified as male and female both serve to increase the f0 threshold which a speaker must cross to be identified as an adult. Figure 7.9: Diagonal lines indicate the relationship between perceived adultness and f0, overall (black), and for women (teal) and men (coral). Bold vertical lines indicate x-axis intercept (category boundary) for each line. In Figure 7.10 I present a final figure that might be the most useful to use if this work were being presented somewhere. It contains the data summarized as probabilities and the x-axis labeled with Hz, both of which help interpret the figure. Figure 7.10: Curves indicate the relationship between the probability of perceived adultness and f0, overall (black), and for women (teal) and men (coral). Bold vertical lines indicate category boundaries suggested by each line. We can calculate the gender-dependent boundaries in the same way we calculated the overall one: # calculate female boundary by reconstructing female slope and intercept # and then finding -a/b boundary_female = -(fixef_samples[,&quot;Intercept&quot;]+fixef_samples[,&quot;pfemale&quot;]) / (fixef_samples[,&quot;g0_s&quot;]+fixef_samples[,&quot;g0_s:pfemale&quot;]) # same for males boundary_male = -(fixef_samples[,&quot;Intercept&quot;]-fixef_samples[,&quot;pfemale&quot;]) / (fixef_samples[,&quot;g0_s&quot;]-fixef_samples[,&quot;g0_s:pfemale&quot;]) # convert to Hz boundary_female_hz = exp (boundary_female * h95_sd + h95_mean) # convert to Hz boundary_male_hz = exp (boundary_male * h95_sd + h95_mean) Below, I summarize the posterior distribution of these boundaries using the posterior_summary function, however I set robust to TRUE. This relies on medians rather than means to provide an estimate of the parameter and its variation. The reason for this is that since we are dividing by a variable that sometimes have very small values, some of our samples result in extremely deviant estimates of the category boundary (i.e., 50,000). So, using the robust option offers protection from these sorts of issues. # overall boundary posterior_summary (boundary_hz, robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 178.8536 9.252209 155.8174 197.7817 # female boundary posterior_summary (boundary_female_hz, robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 238.83 18.20462 204.6917 326.4966 # male boundary posterior_summary (boundary_male_hz, robust = TRUE) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 155.5704 8.456514 131.9241 171.4507 The boundaries, Figure 7.10, and the fixed effects can be used to arrive at the following general conclusions. The overall boundary is at about 180 Hz, although this is not very useful: all adult females would be classified as children given this boundary (see the location of the teal points in Figure 7.10). Instead, listeners appear to use gender-dependent boundaries, a higher threshold for females and a lower one for males. Of course, subjects were not given information about speaker gender so if this were being used, it would have to be guessed from the stimulus just as adultness is being guessed. Finally, in Figure 7.9 the teal line representing female classifications had a slope with a smaller magnitude than the red line representing the male classifications. As noted above this suggests fuzzier, less distinguishable categories. We can see in Figure 7.10 that boys and men are more separable, leading to a more vertical sigmoid curve bentween the categories along the x axis. In contrast, girls and women largely overlap along f0 resulting in a more horizontal sigmoid curve between them, reflecting the co-mingling of categories. 7.5 Plot Code ################################################################################ ### Figure 7.1 ################################################################################ par (mfrow = c(1,3), mar = c(4,2,1,1), oma = c(1,4,0,0)) plot (agg_males$g0, agg_males$pheight, cex =2, col = cols[c(3,5)][agg_males$group], xlim=c(4.5,5.75), pch=1,lwd=2,ylim = c(45,75),xlab = &quot;&quot;, ylab=&quot;Height (inches)&quot;) grid() points (agg_females$g0, agg_females$pheight, cex =2, pch=1,lwd=2, col = cols[c(4,6)][agg_males$group]) legend (4.6,60, legend = c(&quot;Boys&quot;,&quot;Girls&quot;,&quot;Men&quot;,&quot;Women&quot;),lwd=2,lty=0, col = cols[3:6], bty=&#39;n&#39;,pch=1,pt.cex=1.5) abline(lm (pheight ~ g0, data = agg_males)$coefficients, lwd=1,lty=3) abline(lm (pheight ~ g0, data = agg_females)$coefficients, lwd=1,lty=3) abline(lm (pheight ~ g0, data = h95)$coefficients, lwd=3,lty=3, col=1) plot (agg_males$g0, agg_males$pheight, cex =2, col = cols[c(3,5)][agg_males$group], xlim=c(4.5,5.75), pch=1,lwd=2,ylim = c(45,75),xlab = &quot;&quot;, ylab=&quot;Height (inches)&quot;) grid() abline(lm (pheight ~ g0, data = agg_females)$coefficients, lwd=1,lty=3) abline(lm (pheight ~ g0, data = agg_males)$coefficients, lwd=3,lty=3,col=coral) abline(lm (pheight ~ g0, data = h95)$coefficients, lwd=1,lty=3, col=1) plot (agg_females$g0, agg_females$pheight, cex =2, pch=1,lwd=2,ylim = c(45,75), col = cols[c(4,6)][agg_males$group], xlim=c(4.5,5.75),xlab = &quot;&quot;, ylab=&quot;Height (inches)&quot;) grid() abline(lm (pheight ~ g0, data = agg_males)$coefficients, lwd=1,lty=3) abline(lm (pheight ~ g0, data = agg_females)$coefficients, lwd=3,lty=3,col=teal) abline(lm (pheight ~ g0, data = h95)$coefficients, lwd=1,lty=3, col=1) mtext (side=1, text = &quot;f0 (log Hz)&quot;, outer = TRUE, cex = 0.8, line=-1) mtext (side=2, text = &quot;Height (inches)&quot;, outer = TRUE, cex = 0.9, line=2) ################################################################################ ### Figure 7.2 ################################################################################ par (mfrow = c(1,3), mar = c(4,2,1,1), oma = c(1,4,0,0)) plot (agg_males$g0, agg_males$padult, cex =2, col = cols[c(3,5)][agg_males$group], xlim=c(4.5,5.75), pch=1,lwd=2,ylim = c(0,1),xlab = &quot;&quot;, ylab=&quot;&quot;) points (agg_females$g0, agg_females$padult, cex =2, pch=1,lwd=2, col = cols[c(4,6)][agg_males$group]) grid() abline(lm (padult ~ g0, data = agg_males)$coefficients, lwd=1,lty=3) abline(lm (padult ~ g0, data = agg_females)$coefficients, lwd=1,lty=3) abline(lm (padult ~ g0, data = h95)$coefficients, lwd=3,lty=3, col=1) legend (4.6,0.5, legend = c(&quot;Boys&quot;,&quot;Girls&quot;,&quot;Men&quot;,&quot;Women&quot;),lwd=2,lty=0, col = cols[3:6], bty=&#39;n&#39;,pch=1,pt.cex=1.5) plot (agg_males$g0, agg_males$padult, cex =2, col = cols[c(3,5)][agg_males$group], xlim=c(4.5,5.75), pch=1,lwd=2,ylim = c(0,1),xlab = &quot;&quot;, ylab=&quot;&quot;) grid() abline(lm (padult ~ g0, data = agg_females)$coefficients, lwd=1,lty=3) abline(lm (padult ~ g0, data = agg_males)$coefficients, lwd=3,lty=3,col=coral) abline(lm (padult ~ g0, data = h95)$coefficients, lwd=1,lty=3, col=1) plot (agg_females$g0, agg_females$padult, cex =2, pch=1,lwd=2,ylim = c(0,1), col = cols[c(4,6)][agg_males$group], xlim=c(4.5,5.75),xlab = &quot;&quot;, ylab=&quot;&quot;) grid() abline(lm (padult ~ g0, data = agg_males)$coefficients, lwd=1,lty=3) abline(lm (padult ~ g0, data = agg_females)$coefficients, lwd=3,lty=3,col=teal) abline(lm (padult ~ g0, data = h95)$coefficients, lwd=1,lty=3, col=1) mtext (side=1, text = &quot;f0 (log Hz)&quot;, outer = TRUE, cex = 0.9, line=-1) mtext (side=2, text = &quot;P(response=\\&quot;adult\\&quot;)&quot;, outer = TRUE, cex = 0.9, line=2) ################################################################################ ### Figure 7.3 ################################################################################ x = seq (-8,8,.01) y = x par (mfrow = c(1,3), mar=c(4,4,3,1)) plot (x,y, type = &#39;l&#39;,lwd=2, col=deepgreen, xlim=c(-7,7), main = &quot;y = x&quot;, xlab = &quot;Predictor&quot;, ylab = &quot;Logits&quot;) abline (h=0,v=seq(-8,8,2),lty=3) plot (x,ztop (y), type = &#39;l&#39;,lwd=2, col=darkorange, xlim=c(-7,7), main = &quot;y = logistic ( x )&quot;, xlab = &quot;Predictor&quot;, ylab=&quot;Probability&quot;) abline (h=c(0,1,.5),v=seq(-8,8,2),lty=3) plot (x,ptoz(ztop (y)), type = &#39;l&#39;,lwd=2, col=lavender, xlim=c(-7,7), main = &quot;y = logit ( logistic ( x ) )&quot;, xlab = &quot;Predictor&quot;, ylab=&quot;Logits&quot;) abline (h=0,v=seq(-8,8,2),lty=3) ################################################################################ ### Figure 7.4 ################################################################################ x = seq (-3,3,.01) y = x par (mfrow = c(1,2), mar=c(4,4,1,1)) plot (x,y, type = &#39;l&#39;,lwd=3, col=deepgreen, xlim=c(-3,3),xlab=&quot;Predictor (x)&quot;, ylab = &quot;Logits (log (p) - log(1-p))&quot;) abline (h=ptoz(seq(0.1,0.9,.1)),v=c(-9:9),lty=3) abline (h = 0, lwd=2) plot (x,ztop (y), type = &#39;l&#39;,lwd=3, col=darkorange, xlim=c(-3,3), xlab=&quot;Predictor (x)&quot;,ylab=&quot;Probability&quot;) abline (h=seq(0,1,.1),v=(-9:9),lty=3) abline (h = 0.5, lwd=2) ################################################################################ ### Figure 7.5 ################################################################################ cffs = logistic_g0_hypothesis[[1]][,2] par (mfrow = c(1,3), mar = c(4,2,1,1), oma = c(1,4,0,0)) plot (agg_males$g0_s, ptoz(agg_males$padult), cex =2, ylim = c(-5,5),xlab=&quot;&quot;, col = cols[c(3,5)][agg_males$group],pch=1,lwd=2, xlim =range (h95$g0_s)) abline (h=0,lty=3) points (agg_females$g0_s, ptoz(agg_females$padult), cex=2, col=cols[c(4,6)][agg_males$group], pch=1,lwd=2) curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, col = 1, lwd=2) curve ( (cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, col = coral, lwd=1, lty=3) curve ( (cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, col = teal, lwd=1, lty=3) legend (-2,-1, legend = c(&quot;Boys&quot;,&quot;Girls&quot;,&quot;Men&quot;,&quot;Women&quot;),lwd=2,lty=0, col = cols[3:6], bty=&#39;n&#39;,pch=1,pt.cex=1.5) plot (agg_males$g0_s, ptoz(agg_males$padult), cex =2, ylim = c(-5,5),xlab=&quot;&quot;, col = cols[c(3,5)][agg_males$group], pch=1,lwd=2, xlim =range (h95$g0_s)) abline (h=0,lty=3) curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, col = 1, lwd=1, lty=3) curve ( (cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, col = coral, lwd=2) curve ( (cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, col = teal, lwd=1, lty=3) plot (agg_females$g0_s, ptoz(agg_females$padult), cex=2,ylim = c(-5,5),xlab=&quot;&quot;, col=cols[c(4,6)][agg_males$group], pch=1,lwd=2,xlim =range (h95$g0_s)) abline (h=0,lty=3) curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, col = 1, lwd=1, lty=3) curve ( (cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, col = coral, lwd=1, lty=3) curve ( (cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, col = teal, lwd=2) mtext (side=1, text = &quot;f0 (log Hz)&quot;, outer = TRUE, cex = 0.8, line=-1) mtext (side=2, text = &quot;Logit of P(response=\\&quot;male\\&quot;)&quot;, outer = TRUE, cex = 0.9, line=2) ################################################################################ ### Figure 7.6 ################################################################################ cffs = logistic_g0_hypothesis[[1]][,2] par (mfrow = c(1,3), mar = c(4,2,1,1), oma = c(1,4,0,0)) plot (agg_males$g0_s, (agg_males$padult), cex =2, ylim = c(0,1),xlab=&quot;&quot;, col = cols[c(3,5)][agg_males$group], pch=1,lwd=2, xlim =range(h95$g0_s)) abline (h=0.5,lty=3) points (agg_females$g0_s, (agg_females$padult), cex=2, col=cols[c(4,6)][agg_males$group], pch=1,lwd=2) curve ( ztop(cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, col = 1, lwd=2) curve ( ztop(cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, col = coral, lwd=1, lty=3) curve ( ztop(cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, col = teal, lwd=1, lty=3) legend (-2,ztop(-1), legend = c(&quot;Boys&quot;,&quot;Girls&quot;,&quot;Men&quot;,&quot;Women&quot;),lwd=2,lty=0, col = cols[3:6], bty=&#39;n&#39;,pch=1,pt.cex=1.5) plot (agg_males$g0_s, (agg_males$padult), cex =2, ylim = c(0,1),xlab=&quot;&quot;, col = cols[c(3,5)][agg_males$group], pch=1,lwd=2, xlim =range(h95$g0_s)) abline (h=0.5,lty=3) curve ( ztop(cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, col = 1, lwd=1, lty=3) curve ( ztop(cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, col = coral, lwd=2) curve ( ztop(cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, col = teal, lwd=1, lty=3) plot (agg_females$g0_s, (agg_females$padult), cex=2,ylim = c(0,1),xlab=&quot;&quot;, col=cols[c(4,6)][agg_males$group], pch=1,lwd=2,xlim =range (h95$g0_s)) abline (h=0.5,lty=3) curve ( ztop(cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, col = 1, lwd=1, lty=3) curve ( ztop(cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, col = coral, lwd=1, lty=3) curve ( ztop(cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, col = teal, lwd=2) mtext (side=1, text = &quot;f0 (log Hz)&quot;, outer = TRUE, cex = 0.8, line=-1) mtext (side=2, text = &quot;P(response=\\&quot;male\\&quot;)&quot;, outer = TRUE, cex = 0.9,line=2) ################################################################################ ### Figure 7.7 ################################################################################ cffs = logistic_g0_hypothesis[[1]][,2] fixed_effects = fixef(logistic_g0) layout (mat = t(as.matrix(c(1,1,2,2,2)))) par (mar = c(4,4.5,1,1)) brmplot (fixed_effects, grid = TRUE) abline (h=0,lty=3) mtext (side=2, outer = TRUE, text = &quot;Effect (logits)&quot;, line= -1.5, cex=.7) plot (agg_males$g0_s, ptoz(agg_males$padult), ylim = c(-5,5), xlab=&quot;f0 (scaled log Hz)&quot;, type=&#39;n&#39;,ylab=&quot;Logit of P(response=\\&quot;male\\&quot;)&quot;,xlim =range(h95$g0_s)) abline (h=0,v=0,lwd=1, lty=3) points (agg_males$g0_s, ptoz(agg_males$padult), cex =2, col = cols[c(3,5)][agg_males$group], pch=1,lwd=2) points (agg_females$g0_s, ptoz(agg_females$padult), cex=2, col=cols[c(4,6)][agg_males$group], pch=1,lwd=2) curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, col = 1, lwd=2, lty=1) x = -(cffs[1]) / (cffs[4]); abline (v = x, col = 1, lwd=2) legend (-2,-1, legend = c(&quot;Boys&quot;,&quot;Girls&quot;,&quot;Men&quot;,&quot;Women&quot;),lwd=2,lty=0, col = cols[3:6], bty=&#39;n&#39;,pch=1,pt.cex=1.5) ################################################################################ ### Figure 7.8 ################################################################################ layout (mat = t(c(1,2)), widths = c(.35, .65)) par (mar = c(4,4.5,1,1)) den = density(boundary_hz) hist (boundary_hz, col=lightpink, breaks = 60, main = &quot;&quot;, freq = FALSE) lines (den$x, den$y,lwd=2,col=1) plot (agg_males$g0_s, ptoz(agg_males$padult), ylim = c(-5,5),xlab=&quot;f0 (Hz)&quot;, type=&#39;n&#39;, xlim =range (h95$g0_s), xaxt = &#39;n&#39;, ylab=&quot;Logit of P(response=\\&quot;male\\&quot;)&quot;) values = round (exp ( seq(-2,1,1)*sd(h95$g0)+mean(h95$g0) )) axis (side = 1, at = -2:1, labels = values) abline (h=0,lwd=1) points (agg_males$g0_s, ptoz(agg_males$padult), cex =1, col = cols[c(3,5)][agg_males$group], pch=1,lwd=2) points (agg_females$g0_s, ptoz(agg_females$padult), cex=1, col=cols[c(4,6)][agg_males$group], pch=1,lwd=2) legend (-2.5,-0.5, legend = c(&quot;Boys&quot;,&quot;Girls&quot;,&quot;Men&quot;,&quot;Women&quot;),lwd=2,lty=0, col = cols[3:6], bty=&#39;n&#39;,pch=1,pt.cex=1.5) curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, col = 1, lwd=2, lty=3) x = -(cffs[1]) / (cffs[4]); abline (v = x, col = 1, lwd=1) den = density(boundary) lines (den$x, den$y/2,lwd=2,col=1) polygon (den$x, den$y/2,lwd=2,col=lightpink) ################################################################################ ### Figure 7.9 ################################################################################ cffs = logistic_g0_hypothesis[[1]][,2] par (mfrow = c(1,1), mar = c(4,4,1,1)) plot (agg_males$g0_s,ptoz(agg_males$padult),xlab=&quot;f0 (Hz)&quot;,type=&#39;n&#39;, xlim=range (h95$g0_s),ylab=&quot;Logit of P(response=\\&quot;male\\&quot;)&quot;,ylim=c(-5,5)) abline (v= 0,h=0, lty=3) points (agg_males$g0_s, ptoz(agg_males$padult), cex =2, col = cols[c(3,5)][agg_males$group], pch=1,lwd=2) points (agg_females$g0_s, ptoz(agg_females$padult), cex=2, col=cols[c(4,6)][agg_males$group], pch=1,lwd=2) legend (-2.5,-0.5, legend = c(&quot;Boys&quot;,&quot;Girls&quot;,&quot;Men&quot;,&quot;Women&quot;),lwd=2,lty=0, col = cols[3:6], bty=&#39;n&#39;,pch=1,pt.cex=1.5) curve ( (cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, col = 1, lwd=2, lty=3) x = -(cffs[1]) / (cffs[4]); abline (v = x, col = 1, lwd=2) curve ( (cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, col = coral, lwd=2, lty=3) x = -(cffs[3]) / (cffs[6]); abline (v = x, col = coral, lwd=2) curve ( (cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, col = teal, lwd=2, lty=3) x = -(cffs[2]) / (cffs[5]); abline (v = x, col = teal, lwd=2) ################################################################################ ### Figure 7.10 ################################################################################ cffs = logistic_g0_hypothesis[[1]][,2] par (mfrow = c(1,1), mar = c(4,4,1,1)) plot (agg_males$g0_s, (agg_males$padult), ylim = c(0,1),xlab=&quot;f0 (Hz)&quot;,type=&#39;n&#39;, xlim =range (h95$g0_s), xaxt = &#39;n&#39;, ylab = &quot;P(response=\\&quot;male\\&quot;)&quot;) values = round (exp ( seq(-2,1,1)*sd(h95$g0)+mean(h95$g0) )) axis (side = 1, at = -2:1, labels = values) legend (-2.3,0.45, legend = c(&quot;Boys&quot;,&quot;Girls&quot;,&quot;Men&quot;,&quot;Women&quot;),lwd=2,lty=0, col = cols[3:6], bty=&#39;n&#39;,pch=1,pt.cex=1.5) abline (v= 0,h=0.5, lty=3) points (agg_males$g0_s, (agg_males$padult), cex =2, col = cols[c(3,5)][agg_males$group], pch=1,lwd=2) points (agg_females$g0_s, (agg_females$padult), cex=2, col=cols[c(4,6)][agg_males$group], pch=1,lwd=2) curve ( ztop(cffs[1] + cffs[4]*x), xlim =range (h95$g0_s), add = TRUE, col = 1, lwd=2, lty=3) x = -(cffs[1]) / (cffs[4]); abline (v = x, col = 1, lwd=2) curve ( ztop(cffs[3] + cffs[6]*x), xlim =range (h95$g0_s), add = TRUE, col = coral, lwd=2, lty=3) x = -(cffs[3]) / (cffs[6]); abline (v = x, col = coral, lwd=2) curve ( ztop(cffs[2] + cffs[5]*x), xlim =range (h95$g0_s), add = TRUE, col = teal, lwd=2, lty=3) x = -(cffs[2]) / (cffs[5]); abline (v = x, col = teal, lwd=2) "]]
