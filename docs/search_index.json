[["modeling-standard-deviations.html", "Chapter 10 Modeling Standard Deviations 10.1 Data 10.2 Modeling variation in the error 10.3 Modeling group variation in our random effects 10.4 Plot Code", " Chapter 10 Modeling Standard Deviations Our models have included increasingly complicated ways of estimating the value of \\(\\mu\\) from trial to trial. However, weve been assuming that quantities like \\(\\sigma_{error}\\) are fixed from trial to trial. In this chapter were going to build models that relax these restrictions and allow for more variation in their standard deviation parameters. 10.1 Data Were going to continue working with the same data and variables described at the top of the previous chapter. We load the data below: library (brms) options (contrasts = c(&quot;contr.sum&quot;,&quot;cont.sum&quot;)) # data setup ################################################# # source data url1 = &quot;https://raw.githubusercontent.com/santiagobarreda&quot; url2 = &quot;/stats-class/master/data/h95_experiment_data.csv&quot; h95 = read.csv (url(paste0 (url1, url2))) # set up colors for plotting devtools::source_url (paste0 (url1, &quot;/stats-class/master/data/colors.R&quot;)) # source functions devtools::source_url (paste0 (url1, &quot;/stats-class/master/data/functions.R&quot;)) 10.2 Modeling variation in the error In traditional models we assume that our data has what is called a homoscedastic variance. This means that every single data point in our data is drawn from a normal distribution with a fixed variance as below: \\[ \\begin{equation} \\begin{split} y_{[i]} \\sim \\mathrm{Normal}(\\mu_{[i]}, \\sigma_{error}) \\end{split} \\tag{10.1} \\end{equation} \\] So, even though the mean is allowed to vary from trial to trial, the variance is not. This is an important and basic assumption made by all traditional statistical models (including lmer). This assumption had to be made in order to obtain solutions to regression equations using traditional methods. However, Bayesian models based on samplers (like STAN and brms) are not constrained in this way: the variance is just another parameter. Below, we see data drawn from a normal distribution with a varying mean and standard deviation. This distribution slides around the number line generating number as it goes, but also getting narrower and broader while it does so. \\[ \\begin{equation} \\begin{split} y_{[i]} \\sim \\mathrm{Normal}(\\mu_{[i]}, \\sigma_{error[i]}) \\end{split} \\tag{10.2} \\end{equation} \\] These sorts of models are very easy to fit in brms, they can potentially provide better fits to our data, and provide us with useful information. Our model is now going to feature two formulas, one for \\(\\mu\\) and another for \\(\\sigma_{error}\\). brms makes it easy to do this by letting you write multiple formulas and stick them together using the brmsformula function. Below I use the formula from the model we fit at the end of the previous chapter and add a second line (# data formula). The second line says were also going to let the parameter sigma (i.e., \\(\\sigma_{error}\\)) vary in a speaker-dependent manner. model_formula = brmsformula ( # data formula pheight ~ (g0_s + gbar_s) * padult * pgender + vowel + ((g0_s + gbar_s) * padult * pgender + vowel | subj) + (1 | speaker) , # data formula sigma ~ (1|subj) ) In plain English, our model formula above says: Were predicting perceived height using two continuous predictors (g0_s and gbar_s). The slopes of these planes are allowed to vary according to padult, pgender, and the interaction of the two. Our model includes intercept shifts for these plains according to padult, pgender, the interaction of the two, and vowel. All of the aforementioned effects are included as random by-subject effects, and the model includes random by-speaker intercepts as well. In addition, our model features subject-specific error standard deviations, with this variation being treated as a random effect. Note that sigma is modeled as a function of (1|subj). This means that were treating the between-speaker variation in the error terms as a random effect and applying shrinkage to the estimates. The relevant parts of the model structure look like this: \\[\\begin{equation} \\begin{split} y \\sim \\mathrm{Normal}(\\mu_{[i]}, \\sigma_{error[i]}) \\\\ \\mathrm{log}(\\sigma_{error[i]}) = \\sigma_{Intercept}+ \\gamma_{[subj_{[i]}]} \\\\ \\gamma_{[subj_{[i]}]} \\sim \\mathrm{Normal}(0, \\sigma_{\\gamma}) \\\\ \\sigma_{\\gamma} \\sim \\mathrm{t}(3, 0, 6) \\\\ \\end{split} \\tag{10.3} \\end{equation}\\] Where \\(\\gamma\\) (gamma) is just a symbol I picked to represent the random effects for standard deviations. So, the equations above say, top to bottom: the data is distributed normally with a mean and variance that vary form trial to trial. The logarithm of the standard deviation is the sum of an intercept and a subject-specific sigma effects. The subject-specific sigma effects are drawn from a normal distribution with a mean of zero and unknown standard deviation. Finally, the standard deviation of the subject sigma effects is drawn from a t distribution with a mean of 0 and a standard deviation of 6 (values that I set a priori. See the model code below). We could have made this even more complicated, for example, by including differences in expected errors according to perceived adultness. Such a model would look like: sigma ~ padult + (1|subj) Were not going to fit that model, but I just want to point out that your modeling of variance components can also include multiple predictors and random effects, just like the modeling of mean values. 10.2.1 Fitting and interpreting our model Below I prepare the data: # standardize log f0 h95$g0_s = (h95$g0-mean(h95$g0)) / sd(h95$g0) # standardize log geometric mean formant frequency h95$gbar_s = (h95$gbar-mean(h95$gbar)) / sd(h95$gbar) # create gender and adultness variables h95$pgender = c(&#39;m&#39;,&#39;w&#39;)[h95$pgroup %in% c(&#39;g&#39;,&#39;w&#39;)+1] h95$padult = c(&#39;c&#39;,&#39;a&#39;)[h95$pgroup %in% c(&#39;m&#39;,&#39;w&#39;)+1] And fit our model. Notice that instead of supplying the model formula directly in the function call, I supply the object called model_formula that I created above. library (brms) options (contrasts = c(&quot;contr.sum&quot;,&quot;cont.sum&quot;)) set.seed (1) height_perception_het = brms::brm (model_formula, data=h95, chains=4, cores=4, warmup=1000, iter = 6000, thin = 4, control = list(adapt_delta = 0.95), family = &#39;student&#39;, prior = c(set_prior(&quot;student_t(60, 0, 12)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 6)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 6)&quot;, class = &quot;sd&quot;), set_prior(&quot;lkj_corr_cholesky (2)&quot;, class = &quot;cor&quot;))) # save model # saveRDS (height_perception_het, &#39;9_height_perception_het.RDS&#39;) The print statement is way too long to show here, but heres the last chunk. Notice that sigma is no longer in our Family Specific Parameters section. Instead, we have sigma_Intercept in the Population-Level Effects section. Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 61.17 0.65 59.83 62.46 1.00 3347 4014 sigma_Intercept 0.79 0.09 0.61 0.96 1.00 3846 3606 g0_s -1.47 0.36 -2.19 -0.77 1.00 4276 4230 gbar_s -2.88 0.34 -3.57 -2.19 1.00 4486 4186 padult1 3.67 0.58 2.48 4.82 1.00 4131 4361 pgender1 -0.91 0.35 -1.61 -0.22 1.00 4620 4715 vowel1 -0.23 0.07 -0.36 -0.09 1.00 4905 4885 g0_s:padult1 0.02 0.23 -0.45 0.47 1.00 4902 4791 gbar_s:padult1 0.85 0.25 0.38 1.36 1.00 4720 4634 g0_s:pgender1 -0.31 0.18 -0.67 0.04 1.00 4851 4698 gbar_s:pgender1 0.22 0.20 -0.17 0.62 1.00 4312 4705 padult1:pgender1 0.31 0.33 -0.33 0.98 1.00 4787 4663 g0_s:padult1:pgender1 -0.10 0.19 -0.49 0.28 1.00 4682 4429 gbar_s:padult1:pgender1 -0.51 0.19 -0.88 -0.15 1.00 4593 4792 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS nu 7.83 1.30 5.83 10.87 1.00 4947 4764 Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). Our sigma_Intercept parameter represents the average sigma parameter for all subjects. This is the same information that sigma used to give us in our previous models. The reason the value is so low is because brms models standard deviations by log-transforming them. To get the actual mean and credible interval we need to exponentiate the values. # log-transformed standard deviation fixef(height_perception_het, pars=&quot;sigma_Intercept&quot;) ## Estimate Est.Error Q2.5 Q97.5 ## sigma_Intercept 0.7860117 0.09058804 0.6092945 0.9633645 # standard deviation exp ( fixef(height_perception_het, pars=&quot;sigma_Intercept&quot;) ) ## Estimate Est.Error Q2.5 Q97.5 ## sigma_Intercept 2.194626 1.094818 1.839133 2.620498 We can use the hypothesis function to get both the random effects, and the subject-specific sigmas. To do this, you need to set the group parameter to the name of the random effect cluster (in this case subj). Then the scope parameter can be set to ranef to get the random effects, or to coeff to get the sum of the fixed and random effects. # the sigma random effects # sigma_Intercept:subj subj_sigma_ranefs = hypothesis (height_perception_het, &quot;sigma_Intercept=0&quot;, scope = &#39;ranef&#39;, group = &quot;subj&quot;) # the sum of the sigma random effect and the sigma intercept # sigma_Intercept + sigma_Intercept:subj subj_sigmas = hypothesis (height_perception_het, &quot;sigma_Intercept=0&quot;, scope = &#39;coef&#39;, group = &quot;subj&quot;) We can plot these random effect and subject-specific standard deviations below using brmplot. When we just consider the random effects like this, they look a lot like the random slopes we considered in chapter 6. Thats because as far as our model is concerned, they are not that different: they are just subject-specific parameter estimates. Figure 10.1: (left) The subject-specific random sigma deviations (in log units). (middle) Subject specific subject-specific sigmas (random effects + sigma Intercept) on a log scale. (right) The same information as in the middle panel but values have been exponentiated. Looking at the plot above its clear that there is heteroscedastic variation in our model, meaning that there is substantial between-subject variation in our \\(\\sigma_{error}\\) parameter between speakers. But what does this mean? The error standard deviation is the distribution of noise around our lines and planes. So, differing \\(\\sigma_{error}\\) parameters for different subjects reflect different amount of predictability for their data. Each plane below is the overall plane for a different subject in the experiment based on that subjects random effects. Points of the same color are that subjects individual responses. The residuals are actually calculated with respect to different planes based on the other predictors (so, different planes for adult and child responses as shown in the previous chapter). However, you can more or less see that the green points are closer to the green plane than the orange points are to the orange plane, resulting in the larger \\(R^2\\) and smaller \\(\\sigma_{error}\\) for subject 1. Figure 10.2: A comparison of data from different subjects to the overall planes suggested by their subject-specific intercept and slopes terms. To consider whether a subjects responses are more or less systematic, we need to think about the residual standard deviation relative to the total response standard deviation (i.e., error standard deviation / response standard deviation). If we do this using variances (the square of the standard deviation \\(\\sigma^{2}\\)) then we get values that are equivalent to the \\(R^{2}\\) value we get from traditional regression models. As we can see below, the R2 is pretty high for all subjects, indicating that the subject-specific effects are helping us explain a lot of the variation in our data. Note that the shape of the curve for \\(R^{2}\\) is not the same as the shape of the curve for the residual standard deviation. This is because the \\(R^{2}\\) value considers the size of the residual as a function of the amount of variation in the data rather than absolutely. Figure 10.3: Comparison of fixed effects estimates for a normal model (blue), a t model (orange), and the heteroscedastic t model (green). Below we see a comparison of the three similar models weve considered here and in the previous chapter: normally distributed errors, t distributed errors, and heteroscedastic errors. Although there are slight differences, each of the three models provide similar answers for our main research questions (assuming these relate primarily to the fixed effects). Figure 10.4: Comparison of fixed effects estimates for a normal model (blue), a t model (orange), and the heteroscedastic t model (green). 10.3 Modeling group variation in our random effects We can also fit models that ask whether our random effects have different variances based on some grouping factor. The formula below predicts perceived adultness from speech acoustics, perceived gender, and vowel category. Among its random effects are by-speaker random intercepts. These intercepts represent consistent adultness identifications for individual talkers that cannot otherwise be accounted for by the model. brm (padult ~ (g0_s + gbar_s) * pgender + vowel + ((g0_s + gbar_s) * pgender + vowel | subj) + (1 | speaker) To this point, weve been treating random effects as coming from single populations. So, the speaker random effects would be drawn from a population like this: \\[ \\alpha_{speaker} \\sim \\mathrm{Normal} (0, \\sigma_{\\alpha_{speaker}}) \\tag{10.4} \\] However, we could also imagine that the speaker random effects are instead drawn from four populations, one for each group. This would allow the speaker random intercepts to have different distributions for each of the four speaker groups (boy, girl, man, woman). That would correspond to drawing the speaker random effects from populations with differing standard deviations like: \\[ \\alpha_{speaker} \\sim \\mathrm{Normal} (0, \\sigma_{\\alpha_{speaker[group]}}) \\tag{10.5} \\] To implement this change in brm, we simply use the gr (for group) function inside the model formula, on the appropriate clustering factor. For example, the model formula below considers different groups of speaker random effects based on the group predictor: brm (padult ~ (g0_s + gbar_s) * pgender + vowel + ((g0_s + gbar_s) * pgender + vowel | subj) + (1 | gr(speaker, by=group)) In plain English, our model is now: Were predicting the perception of adultness using two continuous predictors (g0_s and gbar_s). The slopes of these planes are allowed to vary according to padult, pgender, and the interaction of the two. Our model includes intercept shifts for these plains according to pgender and vowel. All of the aforementioned effects are included as random by-subject effects, and the model includes random by-speaker intercepts as well. In addition, the by-speaker random effects were modeled as coming from group-specific normal distributions with standard deviations estimated from the data. 10.3.1 Fitting and interpreting our model I fit the model below: library (brms) options (contrasts = c(&quot;contr.sum&quot;,&quot;cont.sum&quot;)) set.seed (1) adultness_group_sds = brms::brm (padult ~ (g0_s + gbar_s)*pgender+vowel + ((g0_s + gbar_s)*pgender+vowel|subj) + (1|gr(speaker, by = group)), data=h95, chains=4, cores=4, warmup=1000, iter = 6000, thin = 4, control = list(adapt_delta = 0.95), family = &quot;bernoulli&quot;, prior = c(set_prior(&quot;student_t(60, 0, 12)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 6)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 6)&quot;, class = &quot;sd&quot;), set_prior(&quot;lkj_corr_cholesky (2)&quot;, class = &quot;cor&quot;))) # save model saveRDS (adultness_group_sds, &#39;10_adultness_group_sds.RDS&#39;) The relevant difference is at the top of the model print statement: Group-Level Effects: ~speaker (Number of levels: 139) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Intercept:groupb) 2.06 0.72 0.90 3.72 1.00 3689 4106 sd(Intercept:groupg) 0.82 0.59 0.05 2.21 1.00 3589 4750 sd(Intercept:groupm) 1.09 0.25 0.64 1.63 1.00 4464 4634 sd(Intercept:groupw) 1.76 0.30 1.25 2.42 1.00 3515 4668 Notice that we now get four separate standard deviation estimates for the intercepts. We can get this information out of the model with the Varcorr function, either as summaries or as posterior samples which we can then use to compare the standard deviations between our groups. # summaries VarCorr(adultness_group_sds)[[&#39;speaker&#39;]] ## $sd ## Estimate Est.Error Q2.5 Q97.5 ## Intercept:groupb 2.0623203 0.7168315 0.9006956 3.716239 ## Intercept:groupg 0.8162568 0.5878581 0.0454276 2.208971 ## Intercept:groupm 1.0918521 0.2460521 0.6423830 1.634528 ## Intercept:groupw 1.7570839 0.2988103 1.2451483 2.415499 # samples of groupwise standard deviations samples = VarCorr(adultness_group_sds, summary = FALSE)[[&#39;speaker&#39;]] str (samples) ## List of 1 ## $ sd: num [1:5000, 1:4] 1.68 1.99 3.6 3.55 1.6 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:4] &quot;Intercept:groupb&quot; &quot;Intercept:groupg&quot; &quot;Intercept:groupm&quot; &quot;Intercept:groupw&quot; It doesnt look like we have very different standard deviations for this data, but the ability to look for this information is very useful. 10.3.2 Two-dimensional response surfaces Below we can see the response surface generated by our model based only on the main effects for f0 and GBAR. The surface below is a plane (in the logit space) which I have transformed from logits to probabilities. As a result, rather than getting a flat plane like we saw in the previous chapter, we get a surface with an s shape like we saw in chapter 7. Its clear that this surface provides a much better fit to the data than a flat plane could have. The color scheme used by the surface is such that red indicates predicted adult responses, blue indicates predicted child responses, and yellow appears at probabilities of 0.5. As a result, the color yellow on the surface below indicates the location of the category boundary. We can see that out curved plane intersects the z=0 plane in a straight line. This straight line is the category boundary between predicted adult and predicted child perceptions. If we turn off the z=0 plane and rotate the cube below to look down the probability axis (P(response=adult)), we can see that the yellow line draws a boundary between the sections of our space corresponding to responses of adult and responses of child. Figure 10.5: A comparison of the probability of being identified as an adult for each speaker with the overall prediction surface suggested by our model. Recall that for logits, values of 0 indicate category boundaries (i.e., response probabilities of 0.5). So, we have a plane where \\(x\\) and \\(y\\) are g0_s and gbar_s respectively, and \\(z\\) is the logit of the probability of observing a response of adult. We need to find the places where our plane goes through the z axis (i.e., \\(z=0\\)). The plane where z=0 (and p=0.5) is shown in teal above. You can see that the intersection of this plane with the z-axis creates a straight line. We can find the equation for this line using the following process. First we determine the formula for our plane. Below, x,y,z refer to the axis coordinates, a is the intercept, b is the x-axis slope and c is the y-axis slope. \\[ z = a+b*x+c*y \\tag{10.6} \\] To find the equation for the line where z=0, we first need to isolate y in our equation. This is because our category boundary line will define y as a function of x, as is usually done. To isolate \\(y\\) on the left hand side (I use this website). The result is: \\[ y = -(b*x)/c-a/c+z/c \\tag{10.7} \\] After setting z to zero and simplifying, we get the equation for a line below. \\[ y = -a/c + -(b/c)*x \\tag{10.8} \\] So, the intercept of the boundary line is a/c and the slope is -b/c. We can then use this to plot our data along with the boundary line as below. # get fixed effects fixefs = fixef(adultness_group_sds)[,1] # transfer parameter values to variable names a = fixefs[1] b = fixefs[2] c = fixefs[3] # calculate slope and intercept of boundary line slope = -b/c intercept = -a/c Notice that the plot on the right resembles the view down the cube above above. Figure 10.6: A comparison of data from different subjects to the overall planes suggested by their subject-specific intercept and slopes terms. Of course, our boundary is not that good overall, and our model actually suggests different boundaries based on the perceived gender of the speaker. Below, I calculate gender-specific boundaries by incorporating the intercept shifts indicated by the pgender fixed effect. This results in a parallel shift in category boundaries as seen below, which results in better categorization overall. Figure 10.7: (left) Each point represents a single speaker. The yellow line represents the category boundary. (middle) Only male speakers are plotted, and the yellow line is the gender-specific boundary. The dotted line represents the overall boundary. (right) The same as the middle panel, except only female speakers, and female-specific boundaries, are plotted. This general procedure can be a very useful way to visualize and understand what your model parameters suggest about your data and listener behavior when working with categorical variables. 10.4 Plot Code ################################################################################ ### Figure 10.1 ################################################################################ par (mfrow = c(1,3), mar = c(4.5,4.5,1,1)) brmplot (subj_sigma_ranefs[[1]][,3:6], col = cols,cex.lab=1.3, ylab=&quot;log(sigma) Random Effects&quot;) abline (h = 0, col = skyblue, lwd=3) brmplot (subj_sigmas[[1]][,3:6], col = cols,ylab=&quot;log(sigma)&quot;,cex.lab=1.3) abline (h = 0.78601175, col = skyblue, lwd=3) brmplot (exp (subj_sigmas[[1]][,3:6]), col = cols,ylab=&quot;sigma&quot;,cex.lab=1.3) abline (h = exp (0.78601175), col = skyblue, lwd=3) mtext (side=1,text = &quot;Subject&quot;, outer = TRUE, -1.1, cex=0.9) ################################################################################ ### Figure 10.2 ################################################################################ library (plotly) normal_g0_gbar = readRDS (&#39;../../models/9_height_perception_multi.RDS&#39;) fixefs = brms::fixef (normal_g0_gbar)[,1] h95$padult = h95$padult == &#39;a&#39; h95$group = factor (h95$group) h95$g0_s = (h95$g0-mean(h95$g0)) / sd(h95$g0) h95$gbar_s = (h95$gbar-mean(h95$gbar)) / sd(h95$gbar) agg_data = aggregate (cbind ( pheight, g0,g0_s,gbar,gbar_s,padult) ~ speaker+group, data = h95, FUN = mean) intercepts = hypothesis (height_perception_het, &quot;Intercept=0&quot;, scope = &#39;coef&#39;, group = &quot;subj&quot;) g0_s = hypothesis (height_perception_het, &quot;g0_s=0&quot;, scope = &#39;coef&#39;, group = &quot;subj&quot;) gbar_s = hypothesis (height_perception_het, &quot;gbar_s=0&quot;, scope = &#39;coef&#39;, group = &quot;subj&quot;) n=100 x = seq (min(agg_data$g0_s),max(agg_data$g0_s), length.out=n) y = seq (min(agg_data$gbar_s),max(agg_data$gbar_s), length.out=n) z0 = seq (0,1,length.out=n) xx = rep (x, each = n) yy = rep (y, n) z = intercepts[[1]][1,3] + g0_s[[1]][1,3]*xx + gbar_s[[1]][1,3]*yy zz0 = matrix (z,n,n) z = intercepts[[1]][6,3] + g0_s[[1]][6,3]*xx + gbar_s[[1]][6,3]*yy zz1 = matrix (z,n,n) fig3 &lt;- plot_ly(agg_data, x = ~g0_s, y = ~gbar_s, z = ~pheight, showlegend=TRUE) fig3 &lt;- fig3 %&gt;% add_surface (x=x,y=y,z = zz0, showscale = FALSE,name=&quot;Subj 1&quot;, colorscale = &quot;Greens&quot;) fig3 &lt;- fig3 %&gt;% add_surface (x=x,y=y,z = zz1, showscale = FALSE,name=&quot;Subj 6&quot;, colorscale = &quot;YlOrRd&quot;) fig3 &lt;- fig3 %&gt;% layout(title = &quot;&quot;,showlegend = TRUE, scene = list(domain=list(x=c(0,1),y=c(0,1)), aspectmode=&#39;cube&#39;, xaxis = list(title = &#39;log f0&#39;), yaxis = list(title = &#39;log GBAR&#39;), zaxis = list(title = &#39;Perceived Height (inches)&#39;))) tmp = h95[h95$subj==1,] fig3 &lt;- fig3 %&gt;% add_markers(data = tmp, x = ~g0_s, y = ~gbar_s, marker = list(color = deepgreen),name=&quot;Subj 1&quot;, type=&quot;scatter3d&quot;, mode=&quot;markers&quot;) tmp = h95[h95$subj==6,] fig3 &lt;- fig3 %&gt;% add_markers(data = tmp, x = ~g0_s, y = ~gbar_s, marker = list(color = darkorange),name=&quot;Subj 6&quot;, type=&quot;scatter3d&quot;, mode=&quot;markers&quot;) fig3 ################################################################################ ### Figure 10.3 ################################################################################ y_pred = predict (height_perception_het)[,1] ysd = tapply (h95$pheight, h95$subj, sd) esd = tapply (h95$pheight - y_pred, h95$subj, sd) par (mfrow = c(1,2), mar=c(4.5,4.5,1,1)) plot (ysd, type=&quot;b&quot;, pch=16, ylim = c(0,10), ylab = &quot;Standard Deviation&quot;, xlab=&quot;Subject&quot;, lwd=2,cex=2) lines (esd, type=&quot;b&quot;, pch=16, col = 4, lwd=2,cex=2) plot (1 - esd^2 / ysd^2, type=&quot;b&quot;, pch=16, ylab = expression(R^2), lwd=2,cex=2, xlab=&quot;Subject&quot;, ylim = c(.7,1)) ################################################################################ ### Figure 10.4 ############################################################################### height_perception_multi = readRDS (&#39;../../models/9_height_perception_multi.RDS&#39;) height_perception_student = readRDS (&#39;../../models/9_height_perception_student.RDS&#39;) par (mfrow = c(1,1), mar = c(4,4,1,1)) brmplot (fixef(height_perception_multi)[-1,], col = skyblue,xs=(1:12)-.15, ylim = c(-4.5,5)) brmplot (fixef(height_perception_student)[-1,], col=darkorange, add = TRUE,xs=(1:12)+0, labels=&quot;&quot;) brmplot (fixef(height_perception_het)[-(1:2),], col=deepgreen, add = TRUE,xs=(1:12)+.15, labels=&quot;&quot;) ################################################################################ ### Figure 10.5 ################################################################################ adultness_group_sds = readRDS (&#39;../../models/10_adultness_group_sds.RDS&#39;) fixefs = brms::fixef (adultness_group_sds)[,1] h95$pfemale = (h95$pgender == &#39;w&#39; | h95$pgender == &#39;g&#39;) h95$group = factor (h95$group) h95$g0_s = (h95$g0-mean(h95$g0)) / sd(h95$g0) h95$gbar_s = (h95$gbar-mean(h95$gbar)) / sd(h95$gbar) agg_data = aggregate (cbind ( pheight, g0,g0_s,gbar,gbar_s,padult,pfemale) ~ speaker+group, data = h95, FUN = mean) n=100 x = seq (min(agg_data$g0_s),max(agg_data$g0_s), length.out=n) y = seq (min(agg_data$gbar_s),max(agg_data$gbar_s), length.out=n) z0 = seq (0,1,length.out=n) xx = rep (x, each = n) yy = rep (y, n) z = fixefs[1] + fixefs[2]*xx + fixefs[3]*yy zz0 = matrix (z,n,n) zz1 = zz0 * 0 fig3 &lt;- plot_ly(agg_data, x = ~g0_s, y = ~gbar_s, z = ~padult, showlegend=TRUE) fig3 &lt;- fig3 %&gt;% add_surface (x=x,y=y,z = ztop(-zz0), showscale = FALSE, colorscale = &quot;Portland&quot;,showlegend = TRUE, opacity = .8, name=&quot;Prediction&quot;) fig3 &lt;- fig3 %&gt;% add_surface (x=x,y=y,z = ztop(-zz1), showscale = FALSE, colorscale = &quot;YlGnBu&quot;,showlegend = TRUE, opacity = 1, name=&quot;z=0&quot;) fig3 &lt;- fig3 %&gt;% add_markers(color = ~group) fig3 &lt;- fig3 %&gt;% layout(title = &quot;&quot;,showlegend = TRUE, scene = list(domain=list(x=c(0,1),y=c(0,1)), aspectmode=&#39;cube&#39;, xaxis = list(title = &#39;log f0&#39;), yaxis = list(title = &#39;log GBAR&#39;), zaxis = list(title = &#39;P(response=\\&#39;adult\\&#39;)&#39;))) fig3 ################################################################################ ### Figure 10.6 ################################################################################ agg_data = aggregate (cbind ( pheight, g0,g0_s,gbar,gbar_s,padult,pfemale) ~ speaker+group, data = h95, FUN = mean) cols2 = c(deepgreen, darkorange,skyblue ,lavender) fixefs = fixef(adultness_group_sds)[,1] intercept_0 = fixefs[1] slope_x = fixefs[2] slope_y = fixefs[3] slope = -slope_x/slope_y intercept = -intercept_0/slope_y par (mfrow = c(1,2), mar = c(4,4,1,1)) plot (agg_data$g0_s, agg_data$gbar_s, pch=16,cex=2,col = cols2[agg_data$group], xlab=&#39;g0_s&#39;, ylab=&#39;gbar_s&#39;) #points (agg_data$g0_s, agg_data$gbar_s, pch=1,cex=2,col = 1) abline (intercept, slope, lwd=3, col=yellow) plot (agg_data$g0_s, agg_data$gbar_s, pch=16,cex=2,col = cols2[agg_data$group], xlab=&#39;g0_s&#39;, ylab=&#39;gbar_s&#39;) xl1 = par(&quot;usr&quot;)[1] xl2 = par(&quot;usr&quot;)[2] xl1 = xl1 - 2*diff(c(xl1,xl2)) xl2 = xl2 + 2*diff(c(xl1,xl2)) mat = rbind( c(xl1, xl1*slope+intercept), c(xl1, xl2*slope+intercept), c(xl2, xl2*slope+intercept)) polygon (mat[,1], mat[,2], col=coral) mat = rbind( c(xl1, xl1*slope+intercept), c(xl2, xl1*slope+intercept), c(xl2, xl2*slope+intercept)) polygon (mat[,1], mat[,2], col=skyblue) abline (intercept, slope, lwd=5, col=yellow) points (agg_data$g0_s, agg_data$gbar_s, pch=16,cex=2,col = cols2[agg_data$group]) #points (agg_data$g0_s, agg_data$gbar_s, pch=1,cex=2,col = 1) ################################################################################ ### Figure 10.7 ################################################################################ fixefs = fixef(adultness_group_sds)[,1] a = fixefs[1] b = fixefs[2] c = fixefs[3] slope0 = -b/c intercept0 = -a/c a = fixefs[1] + fixefs[4] b = fixefs[2] c = fixefs[3] slope = -b/c intercept = -a/c par (mfrow = c(1,3), mar = c(4,4,1,1)) plot (agg_data$g0_s, agg_data$gbar_s, pch=16,cex=2,col = cols2[agg_data$group], xlab=&#39;g0_s&#39;, ylab=&#39;gbar_s&#39;,xlim=c(-2.5,2),ylim=c(-2,2)) abline (intercept0, slope0, lwd=3, col=yellow) use = agg_data$group %in% c(&#39;m&#39;,&#39;b&#39;) plot (agg_data$g0_s[use], agg_data$gbar_s[use], pch=16,cex=2, col = cols2[agg_data$group[use]], xlab=&#39;g0_s&#39;, ylab=&#39;gbar_s&#39;, xlim=c(-2.5,2),ylim=c(-2,2)) abline (intercept, slope, lwd=3, col=yellow) abline (intercept0, slope0, lwd=2, lty=3) a = fixefs[1] - fixefs[4] b = fixefs[2] c = fixefs[3] slope = -b/c intercept = -a/c use = agg_data$group %in% c(&#39;w&#39;,&#39;g&#39;) plot (agg_data$g0_s[use], agg_data$gbar_s[use], pch=16,cex=2, col = cols2[agg_data$group[use]], xlab=&#39;g0_s&#39;, ylab=&#39;gbar_s&#39;,xlim=c(-2.5,2),ylim=c(-2,2)) abline (intercept, slope, lwd=3, col=yellow) abline (intercept0, slope0, lwd=2, lty=3) "]]
