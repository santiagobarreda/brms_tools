[["random-slopes.html", "Chapter 6 Random slopes 6.1 Data and research questions 6.2 Repeated measures and speaker-dependent parameter values 6.3 Random effects and the multivariate normal distribution 6.4 Random slopes 6.5 More predictors and more random slopes 6.6 Answering our research questions 6.7 Frequentist corner 6.8 Exercises", " Chapter 6 Random slopes To this point weve been fitting realistic, but relatively simple models. Our models so far have only included a single random effect: the \\(\\alpha_{[speaker]}\\) parameter that represents between-speaker variation in the intercept. What about between-speaker variation in all of the other effects weve discussed? For example, consider the experiment regarding coffee and reading times that has been mentioned in the previous chapters. What if we did a within-subjects design and tested everyone in the coffee and water conditions. Maybe we expect large differences in the effect for coffee from person to person. Maybe theres even interesting patterns in these effects? These random between-speaker differences in effects are sometimes called random slopes (for continuous predictors) or random effects (for factors) when they are included in your model. This is because since speakers are selected randomly and vary randomly, speaker-dependent effects will also tend to be random variables. For example, if the average speaking rate of different people is a random variable (a random by-subject intercept), then the between-speaker change in this rate induced by the consumption of coffee will also be a random variable (a random by-subject effect for coffee). In this chapter were going to learn to build models that include terms that will model consistent between-speaker variation in the parameters in our models. Fortunately, weve already discussed all of the component parts that make up our models. Well see that more complicated models are just made up of many smaller components, all working together. 6.1 Data and research questions Random by-subject effects for different predictors is not so much a data design as it is an aspect of your model. What I mean by that is that often, the same data could potentially be analyzed with or without the random by-subject effect being included in the model. However, there is one absolutely essential limitation to this: if you want to test the random effect of a predictor according to any other factor (i.e., listener), that predictor must be crossed with that factor. For example, if you want to test the random effect for coffee on subjects, you need to test subjects in both water and coffee conditions. This should be obvious: how can you talk about the effect of coffee on specific individuals when you did not observe those specific individuals across both conditions? We already saw an example of this in our Hillenbrand et al. production data. It is impossible to include adult as a by-speaker random effect. What I mean by this is that you cannot test for the individual effect of adultness on each speaker since you only observed them either as adults or as children. Speaker is not crossed with adult and so the random by-subject effect for adultness simply cannot be estimated. If something is mysteriously wrong with your model, you may be trying to estimate effects that cannot be estimated. Were going to continue analyzing the results of a listening experiment carried out using the Hillenbrand et al. data, presented in Chapter 5. As described in the previous chapter, ten listeners heard productions of hod and heed produced by all 139 speakers in the dataset, presented at random. For each trial, listeners reported the height of the speaker (in feet and inches) and guessed whether the speaker was a boy, girl, man or woman. Were going to analyze the full data, meaning we have 239 observations per listener and 2390 observations overall. In this chapter, were going to flip the dependent and independent variables from last chapter and consider variation in perceived height as a function of f0 (and other predictors). This model makes more sense as linguists because it corresponds to a question linguists actually ask: How do you know what kind of person is speaking from their voice? A model predicting perceived height from f0 helps us understand how continuous variation in f0 leads to continuous variation in the size of the speaker, which goes some way towards understanding the perception of what are called the indexical characteristics of the speaker (things like gender, age, size, and so on). url1 = &quot;https://raw.githubusercontent.com/santiagobarreda/stats-class/master/data/&quot; url2 = &quot;h95_experiment_data.csv&quot; h95 = read.csv (url(paste0 (url1, url2))) Below we can see the distribution of perceived height plotted according to f0, individually for each listener. Clearly, there is a general tendency for perceived height to decrease as f0 increases resulting in a negative slope in the relationship. This relationship is not really a straight line for most subjects, but is linear enough to try this model as a first step. Figure 6.1: Each plot shows responses from a single subject. In Figure 6.2, we compare the data from all subjects using the same colors as above. There is clearly quite a bit of general agreement between listeners. However, we can also clearly see that there is between-subject variation in responses. Figure 6.2: Distribution of perceived height responses as a function of f0 for all listeners. 6.2 Repeated measures and speaker-dependent parameter values In Chapter 5 we saw that in order to encode group-specific regression lines in our model, we need to include the group predictor and an interaction between the group predictor and our continuous predictor. We also know that a predictor like listener/subject/participant is just a factor like any other. This suggests that to encode listener-specific lines in our model, we many need to include a predictor for listener and the interaction between listener and our continuous predictors. In fact, this is precisely what random effects are: the interactions between your predictors and the random variables in your design. Random by-listener effects represent our listener effects, and random by-subject slopes represent the interaction between listener and our continuous predictor. In the context of our Bayesian multilevel models, we call them random effects because they are estimated with partial-pooling. This means that these interaction terms are treated as coming from a distribution whose characteristics are estimated from the data itself (discussed in Chapter 2). To highlight the nature of random effects and their similarity to interactions, were going to consider two approaches to including subject-specific slopes and intercepts in our model: one with random slopes by subject, and one with a fixed \\(predictor \\colon subject\\) interaction. 6.2.1 Description of the model Were going to predict perceived height (in inches) as a function of centered log-f0 (the logarithm of the fundamental frequency), which will be referred to as \\(g0\\_c\\). Our model formula is: pheight ~ g0_c * subj + (1|speaker) As we saw in the last chapter, including an interaction between our continuous predictor and a factor allows for arbitrarily different lines according to the factor. So, the inclusion of g0_c * subj allows the relationship between f0 and perceived height to vary arbitrarily according to listener (subj). In addition, the model also includes a random effect for speaker, meaning we allow the average height response to potentially vary across speakers. The model formula above says model perceived height as a function of centered log-f0 (g0_c) and include subject effects (i.e. subject-specific intercepts). Also, allow subject-specific use of centered log-f0 in the perception of height (using the g0_c:subj interaction). We can build this model up from the equation for a single line using the information outlined in the previous chapter. Recall that the formula for a line is: \\[ \\mu = a + b \\times x \\tag{6.1} \\] Where our predicted value (\\(\\mu\\)) varies along a line with an intercept of \\(a\\) and a slope of \\(b\\). We can decompose the intercept and slope terms in an ANOVA-like decomposition as below, given predictors \\(A\\) and \\(B\\): \\[ \\mu = (Intercept + A + B + ...) + (Slope + Slope \\colon A + Slope \\colon B + ...) \\times x \\tag{6.2} \\] Above, the line intercept is broken up into a model intercept, and effects for A and B. The slope is broken up into a main effect for slope (basically a slope intercept) and the factor by slope interactions (e.g., \\(Slope \\colon A\\)). Below, the equation is expanded further by removing the parenthesis and multiplying each slope term by our continuous predictor (\\(\\mathrm{x}\\)). \\[ \\mu = Intercept + A + B + ... + Slope \\times x + slope \\colon A \\times x + Slope \\colon B \\times x + ... \\tag{6.3} \\] As our models get bigger and bigger, expressions like the one above can be difficult to interpret. A presentation like the one below can be clearer and easier to interpret (once you get used to it). The top line reminds you that you are modeling a line. The second and third lines provide information about expected variation in the intercept and the slope. \\[\\begin{equation} \\begin{split} \\mu = a + b * \\mathrm{x} \\\\ a = Intercept + A + B + ... \\\\ b = Slope + slope \\colon A + Slope \\colon B + ... \\\\ \\end{split} \\tag{6.4} \\end{equation}\\] If you prefer the expanded version of the model equation its easy enough to get this. You simply place all of the components of the \\(a\\) and \\(b\\) equations on the same line, and multiply each term in the \\(b\\) equation by the appropriate continuous predictor. Below is the structure for our model that treats subject as a fixed effect (just like \\(group\\) in the previous chapter). This model includes subject-specific intercepts for our lines (based on the \\(subj\\) term) and subject-specific slopes for our lines (based on the \\(g0\\_c \\colon subj\\) term). This model includes random intercepts for speakers (\\(\\alpha_{speaker}\\)), seen in the intercept equation. Note that our model does not include any random effects in the slopes equation. \\[\\begin{equation} \\begin{split} \\textrm{Likelihood:} \\\\ pheight_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\mu_{[i]} = a_{[i]} + b_{[i]} * \\mathrm{x}_{[i]} \\\\ a_{[i]} = Intercept + subj_{[\\mathrm{subj}_{[i]}]} + \\alpha_{[\\mathrm{speaker}_{[i]}]} \\\\ b_{[i]} = g0\\_c + g0\\_c \\colon subj_{[\\mathrm{subj}_{[i]}]} \\\\ \\\\ \\textrm{Priors:} \\\\ \\alpha_{speaker} \\sim \\mathcal{N}(0,\\sigma_{speaker}) \\\\ \\\\ Intercept \\sim t(3, 60, 12) \\\\ g0\\_c \\sim t(3, 0, 50) \\\\ g0\\_c \\colon subj \\sim t(3, 0, 50) \\\\ subj \\sim t(3, 0, 12) \\\\ \\sigma_{error} \\sim t(3, 0, 12) \\\\ \\sigma_{speaker} \\sim t(3, 0, 12) \\\\ \\end{split} \\tag{6.5} \\end{equation}\\] Heres a description of the model in plain English: Perceived height is normally distributed with a mean that varies trial to trial but a fixed standard deviation. The mean (expected value) varies along lines. The lines are specified by intercepts and slopes that vary trom trial to trial, and there is a single continuous predictor (g0_c). The intercept of these lines vary based on an overall intercept (the main effect), subject-specific deviations from the mean, and speaker-specific deviations from the mean. The slope of these lines vary based on an overall slope (the main effect) and subject-specific deviations from the average slope. The speaker intercepts were drawn from a normal distribution with a mean of zero and a standard deviation estimated from the data. All other effects (e.g., the Intercept, g0_c, etc.) were treated as fixed and drawn from prior distributions appropriate for their expected range of values (e.g., subj ~ t(3,0,12)). 6.2.2 Fitting the model We fit the model that treats subject as a fixed effect: # Fit the model yourself, or # download pre-fit model from: # github.com/santiagobarreda/stats-class/tree/master/models # and load after placing in working directory # fixed_slopes_model = readRDS (&#39;6_fixed_slopes_model.RDS&#39;) set.seed (1) fixed_slopes_model = brm (pheight ~ g0_c * subj + (1|speaker), data = h95, chains=4, cores=4, warmup=1000, iter = 7500, thin = 4, control = list(adapt_delta = 0.95), prior = c(set_prior(&quot;student_t(3, 60, 24)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 0.24)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 24)&quot;, class = &quot;sd&quot;))) # save model # saveRDS (fixed_slopes_model, &#39;6_fixed_slopes_model.RDS&#39;) 6.2.3 Interpreting the model Were going to focus on the model fixed effects. Since were calculating subject-specific intercepts and slopes, we need 20 coefficients to represents all the lines for our ten subjects. Because we are using sum coding, the Intercept and g0_c parameters represent our mean overall intercept and slope across all subjects. The subj parameters represent subject-specific deviations from the mean intercept for a given subject, while the g0_c:subject interactions represent subject specific deviations from the overall slope. As a result, the line representing subject 8s responses can be found by calculating Intercept + subj8 for the intercept and g0_c + g0_c:subj8 for the slope. Recall (from Chapter 4) that the average value of a predictor is called the main effect. In contrast, the effect of a predictor at one specific level of another other factor is called a simple effect. So, the value of the intercept for a particular subject can be thought of at the simple effect of that predictor for that level of factor. # inspect main effects fixef (fixed_slopes_model) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 60.98678449 0.4784301 60.0484335 61.9134125 ## g0_c -6.28597349 1.4033403 -8.9682111 -3.5214787 ## subj1 1.16432587 0.2191837 0.7370882 1.5905736 ## subj2 1.15897715 0.2178242 0.7268558 1.5884516 ## subj3 0.08484842 0.2191453 -0.3439572 0.5161977 ## subj4 3.19209439 0.2196449 2.7737538 3.6216057 ## subj5 -1.99350871 0.2183408 -2.4207174 -1.5651102 ## subj6 0.03808015 0.2185083 -0.3916037 0.4690840 ## subj7 -1.35476612 0.2171118 -1.7884342 -0.9376645 ## subj8 -0.18965272 0.2172589 -0.6200814 0.2400315 ## subj9 -1.53045427 0.2166533 -1.9543467 -1.1057361 ## g0_c:subj1 0.97782028 0.7314227 -0.4547865 2.3773882 ## g0_c:subj2 -1.04728394 0.7340484 -2.4842509 0.3662594 ## g0_c:subj3 1.33942054 0.7252348 -0.1290505 2.7669004 ## g0_c:subj4 7.67663353 0.7400229 6.2212793 9.1142621 ## g0_c:subj5 2.00732223 0.7318437 0.5799876 3.4735872 ## g0_c:subj6 -4.77805879 0.7323175 -6.1988376 -3.3397408 ## g0_c:subj7 2.18304348 0.7355534 0.7304213 3.6183430 ## g0_c:subj8 -2.05792955 0.7389462 -3.4984697 -0.6021956 ## g0_c:subj9 -4.34455418 0.7365353 -5.7989383 -2.8925264 Since were treating subject as a factor and using sum coding, notice that we dont get the final level for the subject intercepts or slopes (i.e. there is no subj10). Unlike with a frequentist analysis, the value of the missing coefficients does not need to remain a mystery. We can recover this using the hypothesis function, though this can be a bit tedious when there are many levels for a factor. I wrote a couple of functions that can help with recovering missing factor levels. First, there is a function called divide_factors. This function takes in a brm model and returns a list of matrices. Each matrix represents the samples for a single main effect or interaction term in your model. We can apply this function to the model we fit above to inspect the output of the divide_factors function. Using the names function shows us that our output has four matrices corresponding to the effects for (Intercept),g0_c, subj, and g_c:subj. # get individual factor matrices from brm model factors = divide_factors (fixed_slopes_model) # check out names of factors names (factors) ## [1] &quot;(Intercept)&quot; &quot;g0_c&quot; &quot;subj&quot; &quot;g0_c:subj&quot; We can use the str function to inspect the output. We can see that (Intercept) and g0_c are vectors (i.e. single columns) of length 6500 (our number of samples), while subj and g_c:subj are matrices with 6500 rows, but 9 columns (number of subjects - 1). str (factors) ## List of 4 ## $ (Intercept): num [1:6500] 61.3 61 61.4 60.9 61 ... ## $ g0_c : num [1:6500] -6.31 -7.25 -7.65 -6.45 -7.75 ... ## $ subj : num [1:6500, 1:9] 1.017 1.185 1.395 1.298 0.962 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:9] &quot;subj1&quot; &quot;subj2&quot; &quot;subj3&quot; &quot;subj4&quot; ... ## $ g0_c:subj : num [1:6500, 1:9] 0.2202 -0.0829 0.1943 1.3944 1.5306 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:9] &quot;g0_c:subj1&quot; &quot;g0_c:subj2&quot; &quot;g0_c:subj3&quot; &quot;g0_c:subj4&quot; ... We can use a function I wrote called add_missing which will add missing levels to single factors (it doesnt work for interactions for now). The process is very simple. Recall from Chapter 3 that the missing factor level will equal the negative sum of the other factors. From this we know that each sample of the missing factor level must equal the negative sum of the estimates for the 9 levels that are present. We have a matrix with 9 columns, and we would like a tenth column representing the coefficients we dont have. Recall from Chapter 2 that we are allowed to combine our parameter samples to answer questions about their values or combinations of these. This suggests we can add up the 9 columns we have in a row-wise manner (i.e. across rows), resulting in a single column representing the sum of the other columns. We flip the sign on this column and we now have a column that represents the negative sum of the other parameters. We stick this column onto the end of our existing matrix and we now have a ten-column matrix representing estimates for all of our factor levels. The above summarizes the function of the add_missing function, and below it is used to recover the missing intercept and slope terms (those of subj10). # add missing subject intercept effect factors[[&quot;subj&quot;]] = add_missing (factors[[&quot;subj&quot;]]) # add missing subject slope effect factors[[&quot;g0_c:subj&quot;]] = add_missing (factors[[&quot;g0_c:subj&quot;]]) I then use brmplot to plot the speaker intercept and slope effects, including the final recovered set of parameters: par (mfrow = c(1,2), mar = c(4,4,1,1)) brmplot (factors[[&quot;subj&quot;]], col = cols) ; abline (h = 0, lty = 3) brmplot (factors[[&quot;g0_c:subj&quot;]], col = cols) ; abline (h = 0, lty = 3) Figure 6.3: (left) Fixed-effects estimates of subject intercept effects. (right) Fixed-effects estimates of subject slope effects. If we want to recover the actual speaker-specific intercepts and slopes (the simple effects), we need to add the speaker effects to their corresponding main effects terms. We can do this by adding the column representing each main effect to the matrix representing each set of subject interactions, as below. We could also do this with the hypothesis function but this way we can add all ten subjects slopes in a single operation, instead of having to write (or even copy) ten lines of code. # add overall intercept to subject effects subj_intercepts = factors[[&quot;(Intercept)&quot;]] + factors[[&quot;subj&quot;]] # add overall slope to subject slope effects subj_slopes = factors[[&quot;g0_c&quot;]] + factors[[&quot;g0_c:subj&quot;]] I want to pause for a moment to highlight that everything to this point has involved the original samples from the posterior, not summaries of the samples. Any manipulations done to parameters (including any comparisons) need to be carried out on the samples, and then summarized (never summarized, and then compared). For example, subj_intercepts, the sum of the overall intercept and the subject-specific intercept effects is still a matrix with ten columns and 6500 rows, representing the individual samples from the posterior distribution of each parameter. head( round ( subj_intercepts , 3 ) ) ## subj1 subj2 subj3 subj4 subj5 subj6 subj7 subj8 subj9 subj10 ## [1,] 62.339 62.790 61.641 64.905 59.008 61.347 59.750 60.958 59.741 60.750 ## [2,] 62.161 62.134 61.071 64.067 59.011 60.838 59.832 60.910 59.144 60.587 ## [3,] 62.785 62.419 61.316 64.498 59.232 61.654 60.327 61.181 59.824 60.667 ## [4,] 62.180 62.219 60.886 64.023 59.134 60.946 59.616 60.716 58.898 60.195 ## [5,] 61.964 62.323 61.121 64.120 59.261 61.075 59.532 60.542 59.761 60.321 ## [6,] 61.905 61.601 60.894 63.631 58.602 61.029 59.257 60.500 59.197 60.035 Only after we are done working with it, we can summarize these matrices: # find summary statistics for the posterior distributions of our parameters subj_intercepts_summary = posterior_summary (factors[[&quot;(Intercept)&quot;]] + factors[[&quot;subj&quot;]]) subj_slopes_summary = posterior_summary (factors[[&quot;g0_c&quot;]] + factors[[&quot;g0_c:subj&quot;]]) Resulting in a summary of the matrix where each row corresponds to a column from the subj_intercepts matrix above. subj_intercepts_summary ## Estimate Est.Error Q2.5 Q97.5 ## subj1 62.15111 0.5290596 61.10673 63.16930 ## subj2 62.14576 0.5255985 61.09036 63.15278 ## subj3 61.07163 0.5283570 60.02235 62.10693 ## subj4 64.17888 0.5274135 63.14280 65.19576 ## subj5 58.99328 0.5243383 57.96506 60.01118 ## subj6 61.02486 0.5266767 59.98223 62.03750 ## subj7 59.63202 0.5237791 58.60875 60.64282 ## subj8 60.79713 0.5233061 59.76867 61.81433 ## subj9 59.45633 0.5243279 58.39773 60.45063 ## subj10 60.41684 0.5247678 59.37266 61.42216 We can plot the subject effects and the actual subject-specific parameter estimates side by side as in Figure 6.4. Clearly, the pattern is the same except for two key differences. First, there are shifts along the y axis due to the addition of the main effects. Second, the error around the estimates is larger when the main effects are added in. This is because the estimates on the right represent the sum of the uncertainty in each individual effect, and the uncertainty in the corresponding main effect. par (mfrow = c(2,2), mar = c(4,4,1,1)) brmplot (factors[[&quot;subj&quot;]], col = cols) ; abline (h = 0, lty = 3) brmplot (subj_intercepts_summary, col = cols) brmplot (factors[[&quot;g0_c:subj&quot;]], col = cols) ; abline (h = 0, lty = 3) brmplot (subj_slopes_summary, col = cols) ; abline (h = 0, lty = 3) Figure 6.4: (top left) Fixed-effects estimates of subject intercept terms. (top left) Fixed-effects estimates of subject intercept (main effect + subject effect). (bottom left) Fixed-effects estimates of subject slope terms. (bottom right) Fixed-effects estimates of subject slopes (main effect + subject effect). In in Figure 6.5 we again see the distribution of perceived height plotted according to f0 (centered log-f0), individually for each subject. We can now add lines indicating predicted perceived height based on our model parameters. Figure 6.5: Each plot shows responses from a single subject. Lines indicate best fit line relating variables, as indicated by our fixed slopes model. In Figure in Figure 6.6, we again compare the data from all subjects using the same colors as above. This time, we add the regression lines for each subject so that we can compare the similarities/differences between them. The fit provided by this model is not great: they make predictions (the lines) that dont match the data well for anyone, suggesting that our model is really missing important information with respect to size perception. We will return to these issues later but will not worry about that for now. Figure 6.6: Distribution of perceived height responses as a function of f0 for all listeners. Lines indicate best-fit lines for each subject. 6.3 Random effects and the multivariate normal distribution Recall from Chapter 2 that when you place a predictor in the formula in parenthesis and on the right-hand-side of a pipe, like this (effect|predictor), you tell brm that you expect data to be clustered according to each category represented in the grouping vector. Further, whatever you put in the left-hand-side of the parentheses ( in here | predictor ) is the model for each subcluster. However, to this point we have not considered putting anything other than a 1 on the left of the pipe in the section where we define random effects. If subject is our clustering factor, this means that each individual subject (i.e. each level) gets their own model. So, a (1|speaker) term says were going to have an intercept for every level of speaker in our model. In the same way, a (g0_c|subj) term says were going to have an intercept and a g0_c slope for every level of subject in our data (remember the intercept is assumed even if you dont include a 1). So we may write a model formula such as: pheight ~ g0_c + (g0_c |subj) + (1|speaker) Our model formula could conceivably be written like this (though this wont work because the syntax is wrong): pheight ~ 1 + g0_c + (pheight ~ 1 + g0_c |subj) + (pheight ~ 1|speaker) Again, this wont actually work, but it might be helpful to think of your formulas this way. Notice that the formula inside the subject parenthesis is the same as the formula outside the parenthesis. In each case we are just estimating perceived height according to a slope and an intercept. The equation outside the parentheses (pheight ~ 1 + g0_c) tells our model to estimate an overall slope and intercept, and the part inside the parentheses ((pheight ~ 1 + g0_c |subj)) tells our model to do the same thing individually for each subject. Normally, we omit the 1 for the intercept, and we only include the dependent variable (and the ~) in the outside formula. However, the formula above is an accurate representation of what our model formula is really doing. So, a model formula like this y ~ g0_c (g0_c | subject) tells brm to estimate a random intercept for each subject, and also a random effect for g0_c for each speaker. Thus, each level of the clustering factor (subject) is represented by two random parameters, the intercept and the slope for g0_c. Random effects in multilevel models are usually treated as draws from multivariate normal distributions. What I mean by this is that the random intercept and slope for each speaker are treated as a single multidimensional variable, rather than as two independent variables. The main difference between treating our random coefficients as a single variable rather than two variables is that when we do this, we also estimate the correlation between them. The easiest way to imagine this is by drawing a bivariate (2-dimensional) normal variable and plotting it. This is what Ive done below, with simulated intercept and slope parameters drawn at random from a multivariate normal distribution. In the left column below I compare three bivariate normal variables along the two dimensions. In the absence of any correlation between variables, a plot of this distribution will be spherical (or circular in 2 dimensions). When there is a correlation between the two dimensions, the distribution starts looking more and more like a straight line. When there is a negative correlation, the line just points down rather than up. Note the the marginal (independent) distributions of the variables (the left and right histograms) dont change as the correlation changes. The correlation is a reflection of the joint variation in the two variables and will not necessarily be evident in the marginal distributions of each variable. Figure 6.7: 10,000 bivatiate normal draws of simulated intercept and slope coefficients from distributions with a mean of 0 and a standard deviation of 1. The correlation of the variables is 0 (top), 0.5 (middle) and 0.9 (bottom). The left column presents both variables together, the middle column presents intercepts and the right column presents slopes. When our dimensions are uncorrelated they are independent. The value of one does not help you understand the other. However, when the dimensions are correlated we can use this to make better predictions using our data. For example, an intercept of 2 in the bottom row in the figure above is very likely to be paired with a slope of 2, but extremely unlikely to be seen with a slope of -2. In contrast, in the top row a slope of 2 and -2 seem about equally likely given an intercept of 2. So, when we use multiple random predictors per grouping factor, we are really drawing from a multivariate normal distributions that acknowledges the relationships between random predictors in our data, within-cluster (e.g. subject/participant/speaker). For example, consider the experiment regarding coffee and speaking rate. Perhaps people who speak fast normally get an even larger boost to their speaking rate from coffee. On the other hand, maybe since they already speak fast, the effect for coffee is diminished in these speakers. In other case, the relationship between the intercept for these speakers (baseline rate) and their coffee effect would not be independent. The shape of the multivariate normal distribution (i.e. how much it looks liek a circle vs an ellipse) is determined by a covariance matrix called sigma (\\(\\Sigma\\)). This matrix is a square \\(n\\) x \\(n\\) matrix for a variable with \\(n\\) dimensions. When we dealt with unidimensional normal distributions for our previous random effects, we specified priors for the (unidimensional) standard deviations using t distributions. The specification of priors for our covariance matrix is only slightly more complicated. In our models, we wont actually include priors for \\(\\Sigma\\) directly. This is because brms (and STAN) build up \\(\\Sigma\\) for us from the components we do specify. This is more information that you really need, but it helps to understand why the priors are specified the way they are for our random effects. Consider two random effects, a random by subject intercept \\(\\alpha_{[subj]}\\), and a random by-subject slope called _{[subj]}. The covariance matrix for our random effects is created by multiplying the standard deviations of our individual dimensions by a correlation matrix (\\(R\\)) specifying the correlations between each dimension. The operation is like this: \\[\\begin{equation} \\begin{split} \\Sigma = \\begin{bmatrix} \\sigma_{\\alpha_{[subj]}} &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{[subj]}} \\\\ \\end{bmatrix} \\times R \\times \\begin{bmatrix} \\sigma_{\\alpha_{[subj]}} &amp; 0 \\\\ 0 &amp; \\sigma_{\\beta_{[subj]}} \\\\ \\end{bmatrix} \\\\ \\end{split} \\tag{6.6} \\end{equation}\\] The values in the outside matrices are the the standard deviations of the random intercepts (\\(\\sigma_{\\alpha_{[subj]}}\\)) and slopes (\\(\\sigma_{\\beta_{[subj]}}\\)) individually. The correlation matrix \\(R\\) contains information about the correlation between the dimensions of the variable (e.g., \\(\\rho_{\\alpha_{[subj]} \\beta _{[subj]}}\\)). So, when we have multiple random effects we have a multidimensional variable, and we need to specify priors for each dimension and for the correlation between all dimensions (but not for \\(\\Sigma\\) directly). We provide priors for the standard deviations of the individual dimensions in the same way as we do for unidimensional random effects (like \\(\\alpha_{[speaker]}\\)). The correlation matrix \\(R\\) will look something like below (for two dimensions). It will contain only values of 1 on the main diagonal and have mirrored values between -1 and 1 off of the diagonal (since the correlation of a and b equals the correlation of b and a). \\[\\begin{equation} \\begin{split} R = \\begin{bmatrix} x &amp; y \\\\ y &amp; z \\\\ \\end{bmatrix} \\\\ \\\\ \\end{split} \\tag{6.7} \\end{equation}\\] We specify priors for variables of this type using the \\(LKJCorr\\) distribution in brms. This distribution has a single parameter that determines how peaked the distribution is around 0. Basically, higher numbers make it harder to find larger correlations (and therefore yield more conservative estimates). See here for an example. \\[\\begin{equation} \\begin{split} R \\sim \\mathrm{LKJCorr} (2) \\end{split} \\tag{6.8} \\end{equation}\\] The above was a full explanation of what information the model needs and why it needs it. You dont need to understand any of the above to use random effects correctly. The important take away is that whenever you are estimating any random effects above and beyond a random intercept, you need to: Specify priors for the standard deviation of each dimension. Specify a prior for the correlation matrix for the multivariate normal used for the random parameters. and brm (and STAN) will do the rest. 6.4 Random slopes The initial model we considered was a demonstration that served primarily as a comparison for the model we are going to fit now. No one would actually include subjects as a fixed effect, nor would they include the \\(g0_c\\) by \\(subject\\) interaction as a fixed effect. In both cases, researchers would tend to include these predictors as random effects. Here, were going to refit the model as a random slopes model, and talk about how this is similar/different to our previous approach of treating subjects as fixed effects. 6.4.1 Description of the model Our previous model formula was: pheight ~ g0_c * subj + (1|speaker) Our new model formula moves g0_c into the parenthesis with subject like this: pheight ~ g0_c + ( g0_c |subj) + (1|speaker) As noted above, this means our model has subject-specific slopes and intercepts now. You may be thinking subject specific intercepts and slopes, isnt that what we did in our last model?. The answer is yes, it is what we did in our last model! As well see below, our random and fixed effects models are largely the same thing, and provide very similar information. However, there are a few very (very) important differences. The model description for our random slopes model is given below. The differences relative to our previous model lie in the replacement of our \\(subj\\) predictor with an \\(\\alpha_{[\\mathrm{subj}]}\\) random effect, and the \\(g0 \\_ c \\colon subj\\) predictor with a \\(\\beta_{[\\mathrm{subj}]}\\) random effect. In addition, the inclusion of these random effects requires that we provide a prior for our correlation term (i.e. \\(\\mathrm{LKJCorr} (2)\\)). However, note that the priors for the standard deviations of the random effects are still being specified in the same way. \\[\\begin{equation} \\begin{split} \\textrm{Likelihood:} \\\\ y_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\mu_{[i]} = a_{[i]} + b_{[i]} * \\mathrm{x}_{[i]} \\\\ a_{[i]} = Intercept + \\alpha_{[\\mathrm{subj}_{[i]}]} + \\alpha_{[\\mathrm{speaker}_{[i]}]} \\\\ b_{[i]} = g0\\_c + \\beta_{[\\mathrm{subj}_{[i]}]} \\\\ \\\\ \\textrm{Priors:} \\\\ \\alpha_{speaker} \\sim \\mathrm{Normal}(0,\\sigma_{speaker}) \\\\ \\\\ \\begin{bmatrix} \\alpha_{subj} \\\\ \\beta_{subj} \\end{bmatrix} \\sim \\mathrm{MVNormal} ( \\begin{bmatrix} 0 \\\\ 0 \\\\ \\end{bmatrix}, \\Sigma) \\\\ \\\\ Intercept \\sim t(3, 60, 12) \\\\ g0\\_c \\sim t(3, 0, 50) \\\\ \\\\ \\sigma_{error} \\sim t(3, 0, 100) \\\\ \\sigma_{\\alpha_{speaker}} \\sim t(3, 0, 100) \\\\ \\sigma_{\\alpha_{subj}} \\sim t(3, 0, 12) \\\\ \\sigma_{\\beta_{subj}} \\sim t(3, 0, 12) \\\\ R \\sim \\mathrm{LKJCorr} (2) \\end{split} \\tag{6.9} \\end{equation}\\] Heres a description of the model in plain English: Perceived height is normally distributed with a mean that varies trial to trial but a fixed standard deviation. The mean (expected value) varies along lines. The lines are specified by intercepts and slopes that vary from trial to trial, and there is a single continuous predictor (g0_c). The intercept of these lines vary based on an overall intercept (the main effect), subject-specific deviations from the mean, and speaker-specific deviations from the mean. The slope of these lines vary based on an overall slope (the main effect) and subject-specific deviations from the average slope. The speaker intercepts were drawn from a normal distribution with a mean of zero and a standard deviation estimated from the data. The subject intercepts and slopes were drawn from a bivariate normal distribution with means of 0 of zero and a covariance matrix estimated from the data. All other effects (e.g., the Intercept, g0_c, etc.) were treated as fixed and drawn from prior distributions appropriate for their expected range of values (e.g., subj ~ t(3,0,12)). Note that the prediction equation in our last model: \\[\\begin{equation} \\begin{split} \\mu_{[i]} = a_{[i]} + b_{[i]} * \\mathrm{x}_{[i]} \\\\ a_{[i]} = Intercept + subj_{[\\mathrm{subj}_{[i]}]} + \\alpha_{[\\mathrm{speaker}_{[i]}]} \\\\ b_{[i]} = g0\\_c + g0\\_c \\colon subj_{[\\mathrm{subj}_{[i]}]} \\\\ \\\\ \\end{split} \\tag{6.10} \\end{equation}\\] Is just like the one for this model, save for a one-to-one replacement of the terms \\(\\alpha_{[\\mathrm{subj}]}\\) and \\(\\beta_{[\\mathrm{subj}]}\\) for \\(subj_{[\\mathrm{subj}]}\\) and \\(g0\\_c \\colon subj_{[\\mathrm{subj}]}\\): \\[\\begin{equation} \\begin{split} \\mu_{[i]} = a_{[i]} + b_{[i]} * \\mathrm{x}_{[i]} \\\\ a_{[i]} = Intercept + \\alpha_{[\\mathrm{subj}_{[i]}]} + \\alpha_{[\\mathrm{speaker}_{[i]}]} \\\\ b_{[i]} = g0\\_c + \\beta_{[\\mathrm{subj}_{[i]}]} \\\\ \\\\ \\end{split} \\tag{6.11} \\end{equation}\\] Although the prediction equations are largely the same, in the previous model we treated subject as a fixed effect. Remember that in our multilevel Bayesian models, this means that the prior distribution for these was determined entirely a priori and was not estimated from the data. For example our subject effects were drawn from a population of \\(subj \\sim t(3, 0, 12)\\) and the subject by g0_c interaction was drawn from a population of \\(g0\\_c \\colon subj \\sim t(3, 0, 50)\\). In contrast, random effects are drawn from populations whose standard deviation is estimated from the data. This is the way our current (and previous) models treat the speaker effects. Notice that we estimate, rather than stipulate, the standard deviation for the population of speaker effects (\\(\\sigma_{speaker}\\)): \\[\\begin{equation} \\begin{split} \\alpha_{speaker} \\sim \\mathrm{Normal}(0,\\sigma_{speaker}) \\\\ \\\\ \\sigma_{speaker} \\sim t(3, 0, 100) \\\\ \\end{split} \\tag{6.12} \\end{equation}\\] We might have treated our subject intercepts in the same way, except for the fact that we are also estimating random slopes for subjects. Since we are drawing two random variables for each person, we need to also model the correlation between the variables. So, when we have multiple random effects (e.g., intercepts and/or slopes) for a predictor, we draw this from a multivariate normal distribution where each predictor is a different dimension of the variable. This requires that we estimate a standard deviation for each predictor, and a correlation between each pair of predictors. As seen below, we draw our predictors from a two-dimensional normal distribution. This distribution has a mean of zero for each dimension, and a covariance matrix equal to \\(\\Sigma\\). \\[ \\begin{bmatrix} \\alpha_{subj} \\\\ \\beta_{subj} \\\\ \\end{bmatrix} \\sim \\mathrm{MVNormal} ( \\begin{bmatrix} 0 \\\\ 0 \\\\ \\end{bmatrix}, \\Sigma) \\\\ \\\\ \\tag{6.13} \\] 6.4.2 Fitting the model We now fit the model that includes random intercepts and by-subject slopes for f0. Notice that my set_prior section now includes a new category of parameter cor for which I provide a prior using the lkj_corr_cholesky distribution. # Fit the model yourself, or # download pre-fit model from: # github.com/santiagobarreda/stats-class/tree/master/models # and load after placing in working directory # random_slopes_model = readRDS (&#39;6_random_slopes_model.RDS&#39;) set.seed (1) random_slopes_model = brm (pheight ~ g0_c + (g0_c|subj) + (1|speaker), data=h95, chains=4, cores=4, warmup=1000, iter = 7500, thin = 4, control = list(adapt_delta = 0.95), prior = c(set_prior(&quot;student_t(3, 60, 12)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 50)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 12)&quot;, class = &quot;sd&quot;), set_prior(&quot;lkj_corr_cholesky (2)&quot;, class = &quot;cor&quot;))) # save model # saveRDS (random_slopes_model, &#39;6_random_slopes_model.RDS&#39;) 6.4.3 Interpreting the model When we look at the print statement for our model, we now see multiple entries in the Group-Level Effects section. Under ~speaker we see sd(Intercept) representing the standard deviation of the talker intercepts. This tells us that we are only estimating random intercepts for our 139 speakers. These intercepts represent systematic variability in perceived height that is independent of the linear effect for f0. ## Group-Level Effects: ## ~speaker (Number of levels: 139) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 5.52 0.47 4.65 6.51 1.00 1643 2982 ## ## ~subj (Number of levels: 10) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.81 0.54 1.09 3.18 1.00 4776 5714 ## sd(g0_c) 4.21 1.22 2.54 7.25 1.00 5435 5917 ## cor(Intercept,g0_c) 0.34 0.27 -0.27 0.77 1.00 5602 5807 We see that there is also a section for ~subj, containing our by-subject random effects. This section has three elements. the first is sd(Intercept), representing the standard deviation of our subject intercepts. These intercepts represent differences in the average height responses of different subjects that are independent of f0. The second is sd(g0_c) representing the standard deviation of subject-dependent slopes. This represents variation in by-subject slopes, analogous to the \\(g0_c \\colon subj\\) interaction in our fixed effects model. The third item iscor(Intercept,g0_c), representing the correlation of subject intercepts and subject slopes. Notice that we get means but also credible intervals areound all these parameters. Below, we can compare the fixed effect estimates of our random slopes model: ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 60.93 0.76 59.44 62.43 1.00 1539 3029 ## g0_c -6.34 1.94 -10.17 -2.63 1.00 2883 4064 To those of the fixed slopes model. ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 60.99 0.48 60.05 61.91 1.00 713 1735 ## g0_c -6.29 1.40 -8.97 -3.52 1.00 1963 3303 The estimates are quite close in value, though their credible intervals vary. The difference in the credible intervals comes across more clearly when we plot them to compare: Figure 6.8: (left) Comparison of random effect (RE) and fixed effect (FE) estimates of the intercept main effect. (right) Comparison of random effect (RE) and fixed effect (FE) estimates of the slope main effect. We can get the random effects (slopes and intercepts) from our model using the ranef function, and asking for the subj random effects. # get random effects random_effects = ranef (random_slopes_model)$subj # inspect their structure str (random_effects) ## num [1:10, 1:4, 1:2] 1.164 1.155 0.116 3.174 -1.914 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : chr [1:10] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## ..$ : chr [1:2] &quot;Intercept&quot; &quot;g0_c&quot; When we have a look at the output of the str function, we can see that this is a 3-dimensional matrix. When we look at this matrix along the third dimension (e.g., random_effects[,,in here]), we get a series of 2-d matrices that are a summary of a single random effect. Below we see that the first matrix (random_effects[,,1]) corresponds to the random intercepts, and the second matrix (random_effects[,,2]) corresponding to the random slopes. Youll note that we actually get all ten subject effects and there is no omitted value. This is because when you use partial pooling to estimate parameters, you actually can estimate all levels of a factor (for technical reasons related to shrinkage). random_effects ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 1.16377527 0.6268821 -0.04942324 2.46153571 ## 2 1.15505273 0.6275566 -0.03510937 2.42001233 ## 3 0.11623493 0.6231278 -1.10121480 1.38309357 ## 4 3.17445575 0.6258360 1.99622247 4.49472090 ## 5 -1.91413822 0.6259466 -3.12790488 -0.63922912 ## 6 0.04515396 0.6257749 -1.18640204 1.32431108 ## 7 -1.28080815 0.6267446 -2.50106938 -0.03644058 ## 8 -0.16633822 0.6317497 -1.39801195 1.11656287 ## 9 -1.47968132 0.6320220 -2.71728703 -0.21591965 ## 10 -0.53282781 0.6300846 -1.74720063 0.75334092 ## ## , , g0_c ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 1.0157799 1.577586 -2.117585 4.117939 ## 2 -0.9027227 1.566411 -4.070686 2.179692 ## 3 1.3279988 1.577451 -1.798952 4.516177 ## 4 7.4737121 1.591885 4.444100 10.670002 ## 5 1.8695128 1.569712 -1.300681 5.008267 ## 6 -4.4944108 1.569323 -7.674207 -1.418619 ## 7 2.0829323 1.582772 -1.030887 5.365565 ## 8 -1.9113901 1.578336 -5.155632 1.199296 ## 9 -4.1408383 1.578127 -7.315184 -1.035547 ## 10 -1.8307910 1.561144 -4.991040 1.268622 In Figure 6.9, we see a comparison of the subject intercept and slope terms provided by the random and fixed slopes models. We can see that the effects are extremely similar, however, again the credible intervals are substantially wider for the estimates provided by the random effects model. par (mfrow = c(2,1), mar = c(4,4,1,1)) # plot random intercepts brmplot (xs = (1:10)-.2, random_effects[,,1], col=cols, labels = &quot;&quot;, pch=15) # plot fixed intercepts brmplot (xs = (1:10)+.2, factors[[&quot;subj&quot;]], add = TRUE, col=cols) abline (h=0, lty=3) # plot random slopes brmplot (xs = (1:10)-.2, random_effects[,,2], col = cols, labels = &quot;&quot;, pch=15) # plot fixed slopes brmplot (xs = (1:10)+.2, factors[[&quot;g0_c:subj&quot;]], add = TRUE, col=cols) abline (h=0, lty=3) Figure 6.9: (top) Comparison of random effects (squares) and fixed effect (circles) estimates for speaker intercept effects. (bottom) Same as above but for the slope terms. In Figure 6.10, we compare the random and fixed effects estimates for the subject intercepts. Note that the difference between the random and fixed effect estimates is largest for the effects with the largest magnitude. For example, on the left edge of the right figure below we see that the green effect with a fixed effect estimates near -2 has a random effect that is nearly 0.1 larger than that (near to -1.9). Figure 6.10: (left) Comparison of fixed and random estimates for subject effects (intercept terms). (right) Plot of the difference between the estimates for each parameter, plotted against the value of the fixed-effect estimate of the same parameter. The same pattern is evident in Figure 6.11, : more extreme values are shrunk towards the mean. This is partial-pooling and shrinkage in action! Because parameters in a random effect are jointly estimated (to some extent), extreme values can be pulled towards the mean when they are weakly supported. Here we see a tiny bit of shrinkage indicating that: 1) the values were not so extreme, and 2) the extreme values had a reasonable amount of support. Figure 6.11: (left) Comparison of fixed and random estimates for g0_c:subject effects (slope terms). (right) Plot of the difference between the estimates for each parameter, plotted against the value of the fixed-effect estimate of the same parameter. 6.5 More predictors and more random slopes 6.5.1 Adding another random slope Imagine we were to add another continuous predictor to our model. We can use the centered logarithm of F1 (g1_c) as an example. Inclusion of random slopes for this predictor would mean our equation now looks like: pheight ~ g0_c + g1_c (g0_c + g1_c|subj) + (1|speaker) In turn, this means that the likelihood section of our model now looks like below. Basically, we have just added a new continuous predictor (\\(\\mathrm{x}_{2[i]}\\)) and slope term (\\(b_{2}\\)), with its own corresponding decomposition into a main effect and a random subject effect. \\[\\begin{equation} \\begin{split} y_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\mu_{[i]} = a_{[i]} + b_{1[i]} * \\mathrm{x}_{1[i]} + b_{2[i]} * \\mathrm{x}_{2[i]} \\\\ a_{[i]} = Intercept + \\alpha_{[\\mathrm{subj}_{[i]}]} + \\alpha_{[\\mathrm{speaker}_{[i]}]} \\\\ b_{1[i]} = g0\\_c + \\beta_{1{[\\mathrm{subj}_{[i]}]}} \\\\ b_{2[i]} = g1\\_c + \\beta_{2{[\\mathrm{subj}_{[i]}]}} \\\\ \\end{split} \\tag{6.14} \\end{equation}\\] The two random slopes (\\(\\beta_{1{[\\mathrm{subj}]}},\\beta_{2{[\\mathrm{subj}]}}\\)) and the random intercept (\\(\\alpha_{[\\mathrm{subj}]}\\)) are all drawn from a multivariate normal distribution: \\[\\begin{equation} \\begin{split} \\begin{bmatrix} \\alpha_{[subj]} \\\\ \\beta_{1[subj]} \\\\ \\beta_{2[subj]} \\\\ \\end{bmatrix} \\sim \\mathrm{MVNormal} ( \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix}, \\Sigma) \\\\ \\end{split} \\tag{6.15} \\end{equation}\\] Just as for our first random slopes model, we just need to worry about specifying the priors for the effects standard deviations (\\(\\sigma_{\\alpha_{[\\mathrm{speaker}]}}, \\sigma_{\\beta_{[\\mathrm{speaker}]}}\\)) and the correlation matrix (\\(R\\)) (like below) and brm does the rest of the work for us. \\[\\begin{equation} \\begin{split} \\sigma_{\\alpha_{[speaker]}} \\sim t(3, 0, 100) \\\\ \\sigma_{\\beta_{1[subj]}} \\sim t(3, 0, 100) \\\\ \\sigma_{\\beta_{2[subj]}} \\sim t(3, 0, 100) \\\\ R \\sim \\mathrm{LKJCorr} (2) \\end{split} \\tag{6.16} \\end{equation}\\] 6.5.2 Adding random factors We can also add random effects for factors. For example, we could include adult inside our subj parentheses in the model formula. This would tell our model to calculate a subject-specific effect for adult: pheight ~ g0_c + g1_c + adult (g0_c + g1_c + adult |subj) + (1|speaker) We discussed before that the random effect of adult cannot be estimated for speaker. However, since subjects were tested for both adult and child voices, we can estimate the random effect for adultness according to subject. The likelihood section of our model will now look like below. Note that the random effect associated with adultness is added to the intercept equation. This is because this effect does not interact with our continuous predictors. Since there are no interactions between our continuous predictors and the adult effect, the slopes cannot vary based on adultness. As a result, the adultness parameter can only affect the intercepts of the shapes being drawn and not their slopes. \\[\\begin{equation} \\begin{split} y_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\mu_{[i]} = a_{[i]} + b_{1[i]} * \\mathrm{x}_{1[i]} + b_{2[i]} * \\mathrm{x}_{2[i]} \\\\ a_{[i]} = Intercept + \\alpha_{[\\mathrm{subj}_{[i]}]} + \\alpha_{adult[\\mathrm{subj}_{[i]}]} + \\alpha_{[\\mathrm{speaker}_{[i]}]} \\\\ b_{1[i]} = g0\\_c + \\beta_{1{[\\mathrm{subj}_{[i]}]}} \\\\ b_{2[i]} = g1\\_c + \\beta_{2{[\\mathrm{subj}_{[i]}]}} \\\\ \\\\ \\end{split} \\tag{6.17} \\end{equation}\\] Since we now have 4 random effects for subject, we draw our subject random effects from a four-dimensional normal distribution like this: \\[\\begin{equation} \\begin{split} \\begin{bmatrix} \\alpha_{[subj]} \\\\ \\alpha_{adult[subj]} \\\\ \\beta_{1[subj]} \\\\ \\beta_{2[subj]} \\\\ \\end{bmatrix} \\sim \\mathrm{MVNormal} ( \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix}, \\Sigma) \\\\ \\\\ \\end{split} \\tag{6.18} \\end{equation}\\] 6.5.3 The independence of continuous predictors Its important to note that each continuous predictor is treated independently in our model. We could, for example, include an interaction between adultness and F1 in our model. This would make the formula look like this: pheight ~ g0_c + g1_c + adult + g1_c:adult + (g0_c + g1_c + adult + g1_c:adult|subj) + (1|speaker) This model says model variation as a function of f0, F1, adultness, and the interaction of F1 and adultness. Note that this only causes a change for one of our slopes (\\(b_2\\)). It causes no change at all for our intercept or the other slope: \\[\\begin{equation} \\begin{split} y_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\mu_{[i]} = a_{[i]} + b_{1[i]} * \\mathrm{x}_{1[i]} + b_{2[i]} * \\mathrm{x}_{2[i]} \\\\ a_{[i]} = Intercept + \\alpha_{[\\mathrm{subj}_{[i]}]} + \\alpha_{adult[\\mathrm{subj}_{[i]}]} + \\alpha_{[\\mathrm{speaker}_{[i]}]} \\\\ b_{1[i]} = g0\\_c + \\beta_{1{[\\mathrm{subj}_{[i]}]}} \\\\ b_{2[i]} = g1\\_c + \\beta_{2{[\\mathrm{subj}_{[i]}]}} + \\beta_{2,adult[\\mathrm{subj}_{[i]}]} \\\\ \\end{split} \\tag{6.19} \\end{equation}\\] Since we now have five random effects, we now draw our subject random effects from a five-dimensional normal distribution like: \\[\\begin{equation} \\begin{split} \\begin{bmatrix} \\alpha_{[subj]} \\\\ \\alpha_{adult[subj]} \\\\ \\beta_{1[subj]} \\\\ \\beta_{2[subj]} \\\\ \\beta_{2,adult[\\mathrm{subj}_{[i]}]} \\\\ \\end{bmatrix} \\sim \\mathrm{MVNormal} ( \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{bmatrix}, \\Sigma) \\\\ \\\\ \\end{split} \\tag{6.20} \\end{equation}\\] Keep in mind that the correlation matrix for this distribution is a 5x5 matrix with 25 elements, meaning we have to estimate 10 correlation parameters and 5 variance parameters in order to estimate these random effects. Many of the convergence problems that lmer has seem to relate to the estimation of the correlation parameters for random slopes. Since our Bayesian models have prior distributions on these correlation parameters, they can do a much better job of investigating the random effects correlations and can therefore easily (but perhaps slowly) find solutions for models with even large numbers of random effects. 6.6 Answering our research questions Lets return to the output of our random slopes model to see where we stand with respect to our research question: How does f0 relate to the perception of speaker size? random_slopes_model ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: pheight ~ g0_c + (g0_c | subj) + (1 | speaker) ## Data: h95 (Number of observations: 2780) ## Samples: 4 chains, each with iter = 7500; warmup = 1000; thin = 4; ## total post-warmup samples = 6500 ## ## Group-Level Effects: ## ~speaker (Number of levels: 139) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 5.52 0.47 4.65 6.51 1.00 1643 2982 ## ## ~subj (Number of levels: 10) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.81 0.54 1.09 3.18 1.00 4776 5714 ## sd(g0_c) 4.21 1.22 2.54 7.25 1.00 5435 5917 ## cor(Intercept,g0_c) 0.34 0.27 -0.27 0.77 1.00 5602 5807 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 60.93 0.76 59.44 62.43 1.00 1539 3029 ## g0_c -6.34 1.94 -10.17 -2.63 1.00 2883 4064 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 3.83 0.05 3.73 3.94 1.00 5831 6083 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). There is clearly an effect for f0 on perceived height, and our credible intervals suggest that this effect is unlikely to be zero or even a very small value. If we were so inclined we could leave it at that and conclude f0 predicts perceived height. However, we can do a very simple form of posterior prediction by considering the lines generated by our models. In the figure below (recreated from above) we can see that the lines are actually doing a pretty terrible job of predicting where the data is. In most cases, there is no data where the line actually is! Are you happy with a model whose predictions are almost entirely different from your data? Figure 6.12: Each plot shows responses from a single subject. Lines indicate best fit line relating variables, as indicated by our fixed slopes model. When we look at the model statement above, we can see that the residual error (sigma, \\(\\sigma_{error}\\)) is only 3.8 inches, meaning our model can predict perceived height with an expected error of 3.8 inches. That doesnt seem that bad. However, if we look at the standard deviation for the speaker intercepts (sd(Intercept) under ~speaker, \\(\\sigma_{\\alpha_{[speaker]}}\\)) we can see that this is 5.5 inches. In other words, our model contains large amounts of speaker specific variation in perceived height. This variation is explained by the random effects in our model, but is not being explained by f0. We can inspect these below: Figure 6.13: Speaker random intercepts and credible intervals colored by group (red = boys, yellow = girls, green = men, blue = women). Remember, these random effects are being modeled as being normally distributed with a mean of 0. The distribution of random effects clearly shows a systematic pattern according to speaker group. In general, boys and girls have negative coefficients meaning they are perceived as smaller than expected given their f0. In contrast, the positive coefficients for adult males and females indicate that the speakers in these groups were perceived as taller than expected given their f0. The speaker intercepts above show a remarkable amount of consistency within group. However, keep in mind that listeners were not told the group the speaker belonged to, so either 1) listener guessed speaker group and used this to guess size, or 2) there are other acoustic cues that vary systematically between groups, and listeners used this to estimate size. In either case, this suggests that our model needs to change in order to really capture how listeners are arriving at size judgments for these speakers. As a result, we could say that this model definitely suggests a relationship between f0 and perceived size. However, there number of discrepancies between our data and our model as currently implemented suggests that we should be looking to make adjustments to the structure of this model. 6.7 Frequentist corner 6.7.1 Bayesian multilevel modesl vs. lmer We can fit a random slopes model with lmer using the code below: model = lme4::lmer (pheight ~ g0_c + (g0_c|subj) + (1|speaker), data = h95) Below I recreate the middle part of the lmer model print statement, except I added numbers to some rows. This is because lmer and brm present much of the same information but with different labels and in a different order. ## Random effects: ## Groups Name Variance Std.Dev. Corr ## (1) speaker (Intercept) 29.903 5.468 ## (2) subj (Intercept) 2.326 1.525 ## (3) g0_c 12.908 3.593 (7) 0.52 ## (4) Residual 14.649 3.827 ## Number of obs: 2780, groups: speaker, 139; subj, 10 ## ## Fixed effects: ## Estimate Std. Error t value ## (5) (Intercept) 60.9939 0.6731 90.621 ## (6) g0_c -6.3488 1.5340 -4.139 Below I show the random_slopes_model print statement with numbers that match the labels in the print statement above. We see that both models provide reasonably similar estimates for our parameters, with the brm model providing more information about parameter intervals. ## Group-Level Effects: ## ~speaker (Number of levels: 139) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## (1) sd(Intercept) 5.52 0.47 4.65 6.51 1.00 1643 2982 ## ## ~subj (Number of levels: 10) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## (2) sd(Intercept) 1.81 0.54 1.09 3.18 1.00 4776 5714 ## (3) sd(g0_c) 4.21 1.22 2.54 7.25 1.00 5435 5917 ## (7) cor(Intercept,g0_c) 0.34 0.27 -0.27 0.77 1.00 5602 5807 ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## (5) Intercept 60.93 0.76 59.44 62.43 1.00 1539 3029 ## (6) g0_c -6.34 1.94 -10.17 -2.63 1.00 2883 4064 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## (4) sigma 3.83 0.05 3.73 3.94 1.00 5831 6083 Below is a comparison of the subject intercepts and slopes fit by both approches: Figure 6.14: (left) Subject random intercepts and credible intervals estimated using brms models. Crosses indicate random effects estimated by lmer. (right) Same as right but for random slopes. And the speaker random intercepts estimates by both approaches. In both cases we see that we arrive at basically the same results using either analysis method. Figure 6.15: Speaker random intercepts and credible intervals estimated using brms models. Crosses indicate random effects estimated by lmer. Im also going to fit a model that was described but not fit above. Below we see a model predicting perceived height as a function of centered log F1 and f0. It also included an effect for adultness and an adultness by F1 interaction. The model includes random intercepts for subject and by subject slopes for all predictors. A random by-speaker intercept was also included. # make variable that indicates if the talker is an adult h95$adult = &quot;&quot; h95$adult[h95$group %in% c(&#39;w&#39;,&#39;m&#39;)] = &quot;adult&quot; h95$adult[h95$group %in% c(&#39;g&#39;,&#39;b&#39;)] = &quot;child&quot; # make centered log F1 h95$g1_c = log(h95$f1) - mean (log(h95$f1)) formula = pheight ~ g0_c + g1_c + adult + g1_c:adult + (g0_c + g1_c + adult + g1_c:adult|subj) + (1|speaker) model_2 = lme4::lmer (formula, data = h95) Notice that lmer also treats our subject random effects as draws from a 5-dimensional normal distribution. It provides estimates of the standard deviations of the five dimensions (under Random effects: and subj), and also estimates of the correlations between the dimensions. summary (model_2) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: pheight ~ g0_c + g1_c + adult + g1_c:adult + (g0_c + g1_c + adult + ## g1_c:adult | subj) + (1 | speaker) ## Data: h95 ## ## REML criterion at convergence: 15283.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -4.0394 -0.5447 0.0323 0.6016 4.7085 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## speaker (Intercept) 3.1496 1.7747 ## subj (Intercept) 2.8338 1.6834 ## g0_c 12.9514 3.5988 0.25 ## g1_c 0.6226 0.7891 0.75 -0.33 ## adult1 2.0034 1.4154 -0.41 0.31 -0.21 ## g1_c:adult1 0.8725 0.9341 0.35 -0.04 0.55 0.12 ## Residual 12.5969 3.5492 ## Number of obs: 2780, groups: speaker, 139; subj, 10 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 59.4982 0.5618 105.897 ## g0_c -11.1546 1.2865 -8.670 ## g1_c -2.4530 0.3384 -7.249 ## adult1 4.1865 0.4920 8.509 ## g1_c:adult1 -1.2790 0.3709 -3.448 ## ## Correlation of Fixed Effects: ## (Intr) g0_c g1_c adult1 ## g0_c 0.179 ## g1_c 0.501 -0.154 ## adult1 -0.402 0.345 -0.091 ## g1_c:adult1 0.286 -0.034 0.176 0.071 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see ?isSingular 6.8 Exercises "]]
