[["bayesian-anova-and-interpreting-complicated-models.html", "Chapter 8 Bayesian ANOVA and interpreting complicated models 8.1 Data and research questions 8.2 Deciding on a model 8.3 Fitting the models 8.4 Comparing the maximal and final models 8.5 Bayesian Analysis of Variance 8.6 Applying a Bayesian ANOVA to a more-complicated model 8.7 Plot Code", " Chapter 8 Bayesian ANOVA and interpreting complicated models At this point we have enough model components to build very large and complicated models. Traditionally, models with many predictors have had three general problems: A model may return spurious values for the extra predictors, leading to incorrect conclusions. The model may not fit/converge, meaning you cant get the model coefficients. It can be difficult to interpret a model with hundreds of parameters. In this chapter were going to cover a Bayesian approach to working with large models. Were going to discuss how working with multilevel Bayesian models can naturally help us with problems (1) and (2) above, and were going to discuss an easy way to approach the third problem also. 8.1 Data and research questions Were going to change course slightly and focus on entirely new data, called kb07. The data is from this paper: Kronmüller, E., &amp; Barr, D. J. (2007). Perspective-free pragmatics: Broken precedents and the recovery-from-preemption hypothesis. Journal of Memory and Language, 56, 436455. This chapter will focus on the comparison of analyses for the kb07 data presented in this paper: Bates, D., Kliegl, R., Vasishth, S., &amp; Baayen, H. (2015). Parsimonious mixed models. arXiv preprint arXiv:1506.04967 The above paper discusses what kinds of random effects structures to include in our models. The paper focuses on use of the lmer function. I havent discussed lmer very much, although it is extremely popular and extremely useful. Think of it like this: lmer is basically a special case of brm. There is an equivalent brm model for every lmer model but not vice versa. However, generally speaking, brm and lmer should provide the same answers when fitting the same models. One major difference between lmer and brms is in the way the lmer estimates parameter values. Rather than provide a set of samples, lmer returns point estimates representing the best values of parameters. This can cause a problem when parameters are bounded, or grow without bound. For example, standard deviation parameters (e.g., \\(\\sigma\\)) cannot be 0 or negative, but sometimes they are very, very small. So, when lmer tries to find the best value, it can get closer, and closer, and closer to zero, leading to problems involving calculations with very small values (e.g., as \\(x\\) approaches \\(0\\), \\(1/x\\) approaches \\(\\infty\\)). In contrast, Bayesian models dont try to find the single best value but instead collect a series of samples. As a result, the models are less likely to encounter problems when they try to estimate values that are very close to boundaries (they just bounce of the boundary as they randomly walk!). A second difference between the lmer estimation method and that of our Bayesian multilevel models is that lmer doesnt use prior probabilities for all of its parameters. This can cause some problems when estimating a large number of parameters without enough data. In contrast, brms applies shrinkage to all its parameters (at least in principle), which can help avoid some of the problems encountered by lmer. Just to be clear, I hope to compare in this chapter is just two different estimation/fitting methods, rather than two different philosophies. The models fit by lmer and brms are much more similar than they are different. This chapter will simply serve to highlight how using Bayesian multilevel models can help us resolve some of the difficult issues that can present when we fit complicated models using approaches more similar to lmer, in addition to discussion how we can interpret models with large numbers of parameters. 8.1.1 The experiment We first load the lme4 and brms packages, and the data. library (lme4) library (brms) options (contrasts = c(&quot;contr.sum&quot;,&quot;cont.sum&quot;)) url1 = &quot;https://raw.githubusercontent.com/santiagobarreda&quot; # source colors devtools::source_url (paste0 (url1, &quot;/stats-class/master/data/colors.R&quot;)) # source functions devtools::source_url (paste0 (url1, &quot;/stats-class/master/data/functions.R&quot;)) The data comes from an experiment by Kronmüller and Barr (2007). The version of the data Im using is hosted here. You need to download it, place it in your working directory, and then load it: load (&quot;kb07_data.RDA&quot;) Kronmüller and Barr ran an experiment where subjects had to select one of several visual objects on a monitor, with a cursor. Heres information about the experimental structure: The dependent variable is reaction times measures in milliseconds (RTtrunc). 56 subjects responded to 32 items (n = 1790, 2 missing observations). The experiment involved several manipulations. These are our predictors, because they are the things the authors did in order to affect reaction times. The manipulations were: Speaker (\\(S\\)): Instructions presented by a new or old speaker (a factor with two levels: new and old). Precedent (\\(P\\)): Instructions maintained or broke a precedent of reference (a factor with two levels: maintained and broke). Cognitive Load (\\(C\\)): the task had to be performed without or with a cognitive load (a factor with two levels: with and without). The experimental conditions were crossed. This means that all possible combinations were presented. Since we have 3 factors with two levels each we can call this a 2 x 2 x 2, crossed design. A 2x2x2 crossed design results in 8 (2x2x2=8) unique conditions (combination of factor levels). Since were using sum coding, remember that we can only calculate 1 parameter for every 2-level factor. A model that included all main effects and interactions for our three factors would include the following terms: Main effects: \\(Intercept, S, P, C\\). 2-way interactions: \\(SP, SC, PC\\). 3-way interactions: \\(SPC\\). Our model requires 8 parameters to represent the 8 combinations of the three factors. By using these predictors (\\(Intercept, S, P, C, SP, SC, PC, SPC\\)) rather than modeling group means directly, our model is able to estimate the independent effect of each predictor (main effects), in addition to any interaction effects. The model also includes three random sources of variation: \\(Subject\\): the individual subject/listener in the experiment. \\(Item\\): the item presented to the subject on the trial. \\(Error\\): the random trial to trial variation (i.e., \\(\\sigma_{error}\\)). 8.2 Deciding on a model The authors manipulated listening conditions for a task and observed variation in reaction times as a result of the manipulations. The main goal of this experiment is to see what does (and does not) affect reaction times for this task. The most complicated possible answer to this question is that everything matters. You have unpredictable variation across all groups, meaning you need all eight fixed parameters (i.e., Int., S, P, C, SP, SC, PC, SPC) to distinguish the groups. Furthermore, the way that these main effects and interactions affect reaction times also varies unpredictably from subject to subject. And on top of all that, these effects also vary substantially and unpredictably from item to item. The statement above represents whats sometimes called the maximal model, the model containing all possible random slopes and intercepts. The maximal model for this experiment is: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + ( 1 + S + P + C + SP + SC + PC + SPC | subj) + ( 1 + S + P + C + SP + SC + PC + SPC | item) This model estimates speaker and item-dependent effects for all predictors, basically a lot of effect:subject and effect:item interactions. We could compare this to a minimal model so small it fits on one line: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + ( 1 | subj) + ( 1 | item) This model does not contain any random effects other than intercepts. This means this model acts as if we do not have any systematic/substantial variation in our predictors by item or subject (random or otherwise). We could take this even further and make the model like this: RTtrunc ~ 1 + S + P + C + ( 1 | subj) + ( 1 | item) Which basically says we expect only main effects and no interactions. We may know instinctively that its a bad idea to start with the last model above. If you assume almost nothing matters how can you find out if it does? But is the opposite necessary, it is necessary to always use the maximal model? There are at least two reasons why people may not use the maximal model as their final model: It may not be practical. These models can take a long time to fit since they are complicated. For example, since the random effects are treated as draws from a multivariate normal distribution (as discussed in chapter 6), we need to calculate correlations between all dimensions. This means calculating 8 standard deviations and 28 (!) correlations for the random slopes for subject, and then doing the same for the random slopes for item. When many of these standard deviations or correlations are near zero and/or there is not very much data, lmer can have problems converging on a solution. It may not be necessary. If some predictors really have no effect on our outcomes, and especially when these interfere with our ability to fit or interpret our models, it may not be necessary to leave them in the model. In Bates et al., the authors outline a way to select a final model based on an initial maximal model. The authors propose an: iterative method that reduces model complexity to arrive at an optimal [linear mixed model] for this experiment. [] We qualify this procedure at the outset: We do not claim that this is the only way to proceed, but the strategy has consistently yielded satisfactory results for all data sets we have examined so far p. 8. Notice that the authors are clear that this is not the best or only approach, but simply an approach, albeit one that is based on sound reasoning and with a proven track record. Im not going to discuss the process outlined by the authors in detail. However, the general strategy can be summarized as: Fit the maximal model. Investigate which random effects likely dont matter. This is done based on which ones have very small variances and/or a small amount of independent variation between the dimensions of the random effects. Fit a smaller model without the unimportant random effects. In this chapter, Im going to present a slightly different, but largely analogous approach to dealing with large models, as described in this paper: Gelman, A. (2005). Analysis of variancewhy it is more important than ever. Annals of statistics, 33(1), 1-53. And in Chapter 22 of this book: Gelman, A., &amp; Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge university press. Basically, Gelman and colleagues suggest that we inspect the variance components (the \\(\\sigma\\) terms) of our multilevel models in order to figure out what does and doesnt matter. This is analogous to step 2 in the process outlined above. Once you have established which parameters are not a source of variance in your data, you can re-fit a smaller model, or you can sometimes just keep your maximal model. 8.3 Fitting the models 8.3.1 The maximal models First we fit the maximal brm model # Fit the model yourself, or # download pre-fit model from: # github.com/santiagobarreda/stats-class/tree/master/models # and load after placing in working directory # maximal_brms = readRDS (&#39;8_maximal_brms.RDS&#39;) set.seed (1) maximal_brms = brm (RTtrunc ~ 1+S+P+C+SP+SC+PC+SPC + (1+S+P+C+SP+SC+PC+SPC|subj) + (1+S+P+C+SP+SC+PC+SPC|item), data = kb07, chains=4, cores=4, warmup=1000, iter = 3500, thin = 2, prior = c(set_prior(&quot;student_t(3, 2000, 500)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 500)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 500)&quot;, class = &quot;sd&quot;), set_prior(&quot;lkj_corr_cholesky (2)&quot;, class = &quot;cor&quot;))) # save model # saveRDS (maximal_brms, &#39;8_maximal_brms.RDS&#39;) And then the analogous, maximal lmer model. I got the parameter settings used for the model from this webpage. # Fit the model yourself, or # download pre-fit model from: # github.com/santiagobarreda/stats-class/tree/master/models # and load after placing in working directory # maximal_lmer = readRDS (&#39;8_maximal_lmer.RDS&#39;) maximal_lmer &lt;- lmer(RTtrunc ~ 1+S+P+C+SP+SC+PC+SPC + (1+S+P+C+SP+SC+PC+SPC|subj) + (1+S+P+C+SP+SC+PC+SPC|item), kb07, REML=FALSE, start=thcvg$kb07$m0, control=lmerControl(optimizer=&quot;Nelder_Mead&quot;, optCtrl=list(maxfun=50000L), check.conv.grad=&quot;ignore&quot;, check.conv.hess=&quot;ignore&quot;), verbose=TRUE) # save model # saveRDS (maximal_lmer, &#39;8_maximal_lmer.RDS&#39;) 8.3.2 Fitting the final models in lmer and brms We can also fit the final model as determined by Bates et al. using the selection process proposed by the authors. This model includes only random by-subject intercepts, and random intercepts and \\(P\\) effects for item. First, we fit the final model in brms: set.seed (1) final_brms = brm (RTtrunc ~ 1+S+P+C+SP+SC+PC+SPC + (1|subj) + (1+P|item), data = kb07, chains=4, cores=4, warmup=1000, iter = 3500, thin = 2, prior = c(set_prior(&quot;student_t(3, 2000, 500)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 500)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 500)&quot;, class = &quot;sd&quot;), set_prior(&quot;lkj_corr_cholesky (2)&quot;, class = &quot;cor&quot;))) # save model #saveRDS (final_brms, &#39;final_brms.RDS&#39;) And then fit the final model in lmer: final_lmer &lt;- lmer(RTtrunc ~ 1+S+P+C+SP+SC+PC+SPC + (1|subj) + (1+P|item), kb07, REML=FALSE, control=lmerControl(optimizer=&quot;Nelder_Mead&quot;, optCtrl=list(maxfun=50000L), check.conv.grad=&quot;ignore&quot;, check.conv.hess=&quot;ignore&quot;), verbose=TRUE) # save model # saveRDS (final_lmer, &#39;final_lmer.RDS&#39;) 8.4 Comparing the maximal and final models Bates et al. compare the maximal and final models fit in lmer with the same models fit using STAN (the language brms uses to fit its models). Were going to make the same comparisons that Bates et al. make, and recreate some of the figures in their paper (with some modifications). Instead of using STAN directly, were going to fit models in brms and compare these to the equivalent models fit in lmer. In Figure 8.1 you can see a recreation of Figure 1 from Bates et al., which compares fixed-effect parameter estimates and intervals for the different models. Clearly, the four models provide essentially the same output for these predictors. Thats reassuring! If results changed dramatically as a result of our model selection, we would need to think very carefully about the different models were considering and what might have caused the changes. Figure 8.1: Comparison of means and confidence/credible intervals for fixed-effects parameter estimates by the four models being considered. In Figure 8.2, I recreate Figure 2 from Bates et al., which compares the standard deviations estimates for the random effects included in the different models. Its worth stopping to think about what these standard deviations are. Recall that random effects like subject-specific random intercepts are modeled as coming from a normal distribution with a mean of 0 and a standard deviation based on the data, like this: \\[ \\alpha_{[subj]} \\sim \\mathrm{Normal}(0, \\sigma_{\\alpha_{[subj]}}) \\tag{8.1} \\] So, the \\(\\sigma_{\\alpha_{[subj]}}\\) reflects the variation in the subject-specific intercepts in the data. Similarly, we draw the subject-specific effects for \\(P\\) (define) from a normal distribution with a standard deviation of \\(\\sigma_{\\alpha_{[subj]}}\\). \\[ \\alpha_{P[subj]} \\sim \\mathrm{Normal}(0, \\sigma_{\\alpha_{P[subj]}}) \\tag{8.2} \\] These standard deviations directly reflect the amount of variation in these parameters in our model. Parameters that vary a lot reflect large effects on our data. These will be represented by large standard deviations. In contrast, parameters that do not vary much will be represented by small standard deviations do not have a large effect on our data (since they do not vary much). So, the individual values of \\(\\sigma_{\\alpha_{[subj]}}\\) and \\(\\sigma_{\\alpha_{P[subj]}}\\) in our model will tell us whether the subject intercepts and the subject-specific effect for \\(P\\) are important sources of variation in our model. We can take one look at Figure 8.2 and tell that there are really four dominant sources of variance in our random effects: the item-specific intercepts and effects for \\(P\\), the subject-specific intercepts, and the random error (sigma). Figure 8.2: Comparison of random effects standard deviation estimates for the frou models being considered. As you can see, the models all provide very similar standard deviation estimates. Note that the brm models provide credible intervals for all parameters, while the lmer models only provide point estimates for these parameters. The lack of intervals on parameter estimates makes it difficult to rule out parameters since they will never equal exactly zero. So, we will always have non-zero numbers for these parameters, and secretly some of these are zero or nearly zero. We can use our credible intervals to figure out which variance components are unlikely to matter: Variance components whose credible intervals are concentrated near zero. There are several such components in the Figure below. In the best case, many of these components reflect a tiny amount of systematic variation in our outcomes. In Figure 8.3 we compare the correlations for the subject random effects, recreating Figure 3 from Bates et al. These are the correlations between the dimensions of the 8-dimensional variable representing the subject random effects in the maximal model. As I noted earlier, there are 28 of these correlations. Note that the plot below only contains information from the maximal model because the final models included only a single subject random effect (subject random intercepts), and so did not involve correlations. As with the standard deviations, brms gives us intervals for these parameters while lmer returns point estimates. We see that the lmer estimates vary substantially around 0 while the brms estimates are all close to zero, and have intervals that include zero. Here we see two advantages of our Bayesian model: Credible intervals for all parameters let us accept values of zero as likely values for the correlations (this cant be done with point estimates). The prior on the covariance matrix pulls weakly-supported correlations to zero, thereby protecting against spurious results. Thus, we can actually use the results below to conclude that the most likely value for these correlations is zero or something close to zero. Figure 8.3: Compraison of subject random effect correlation estimates for brm (green) and lmer (red). Figure 8.4 shows the correlations between the dimensions of the item random effects (a recreation of the other half of Figure 3 in Bates et al.). Recall that that final model included by-item intercepts and by-item effects for \\(P\\), and so involved a single correlation parameter (Int:P in the figure below). Notice that the lmer estimates for the maximal model all vary around zero. There is nothing qualitatively different about the correlation that is found to actually be real as opposed to the other ones. In fact, the value of the Int:P correlation is about the same magnitude across both lmer models. The fact that the correlation mainted in the final model looks about the same as the ones that are omitted from the final model is a problem for us, when we rely on lmer. In the absence of intervals, it is difficult to judge parameters based on magnitudes alone. The method outlined in Bates et al. is a proposed solution to this problem. However, note that our Bayesian models do let us distinguish the correlations that matter from the ones that dont. Even in the maximal model, the correlation that is included in the final model is qualitatively different from the others: its credible interval is substantially narrower and does not include 0. Furthermore, the correlations that do not represent reliable variation in the data are pulled toward zero, allowing us to establish that these are likely not important to our understanding of the data. So, in effect, our maximal Bayesian model give us the same information as the final model, without having to ever refit the model. It allows us to rule out the same variance components, and provides the same parameter estimates and intervals for the fixed effects. Figure 8.4: Compraison of item random effect correlation estimates for the maximal brm (green) and lmer (red) models, and the final brm (blue). Bates et al. conclude that: The Bayesian analysis shows two important things. First, the estimates of the fixed effects in the lme4 model and the Bayesian model are nearly identical. This shows that the maximal lmm fit using lme4 is essentially equivalent to fitting a Bayesian lmm with regularizing priors of the sort described above. (p. 13) and that: Second, the relevant variance component parameters that were identified above using principal components analysis (pca) and likelihood ratio tests (lrts) are exactly the parameters that clearly dominate in the Bayesian analysis. (p. 13) As a result of this general equivalence, fitting a maximal model using brms and focusing on the relative magnitudes of individual variance components can be a way to do model selection without necessarily selecting your model. 8.5 Bayesian Analysis of Variance Heres what Gelman and Hill have to say about the analysis of variance using multilevel models: When moving to multilevel modeling, the key idea we want to take from the analysis of variance is the estimation of the importance of different batches of predictors (components of variation in ANOVA terminology). As usual, we focus on estimation rather than testing: instead of testing the null hypothesis that a variance component is zero, we estimate the standard deviation of the corresponding batch of coefficients. If this standard deviation is estimated to be small, then the source of variation is minorwe do not worry about whether it is exactly zero. In the social science and public health examples that we focus on, it can be a useful research goal to identify important sources of variation, but it is rare that anything is truly zero.\" (p. 490) the standard deviation of a set of coefficients gives a sense of their predictive importance in the model. An analysis-of-variance plot, which shows the relative scale of different variance components, can be a useful tool in understanding a model. (p. 492) Gelman and Hill distinguish between two types of standard deviations for a random effect with \\(J\\) levels (p. 459):  The superpopulation standard deviation [\\(\\sigma_{\\alpha_{[subj]}}\\)], which represents the variation among the modeled probability distribution from which the [\\(\\alpha_{[subj]}\\)] were drawn, is relevant for determining the uncertainty about the value of a new group not in the original set of J.  The finite-population standard deviation [\\(\\mathrm{sd}(\\alpha_{[subj]})\\)] of the particular J values of [\\(\\alpha_{[subj]}\\)] describes variation within the existing data. The superpopulation standard deviation estimates correspond to \\(\\sigma\\) terms actually estimated by our model. The finite-population standard deviation terms are estimated by us using our model parameter estimates. The authors note that (p. 464): The superpopulation [] and finite-population [s] standard deviations are not two different statistical estimators of a common quantity; rather, they are two different quantities that can both be estimated from the multilevel model. We can get a point estimate and uncertainty intervals for both. In general, the point estimates of  and s will be similar to each other, but s will have less uncertainty than . That is, the variation is more precisely estimated for the finite population than the superpopulation. This makes sense because we have more information about the units we observe than the full population from which they are sampled.\" Gelman and colleagues suggest the following general process: Fit the model with the structure you think it required to capture the variation in the data. Calculate the superpopulation and finite-population standard deviations for predictors or groups of predictors. Make an ANOVA plot comparing the magnitudes of different predictors, and of the uncertainty in the estimates. Us the ANOVA plot to make inferences about the relative importance of your predictors, and to guide your analysis. 8.5.1 Getting the superpopulation standard deviations from our models The superpopulation standard deviation is our models estimate of the standard deviation of the population from which our random effects are drawn. We can extract the superpopulation standard deviations for subj and item using the VarCorr function and the code below. # subject random effect standard deviations subj_ranefs_super = VarCorr(maximal_brms)[[&quot;subj&quot;]][[&quot;sd&quot;]] # item random effect standard deviations item_ranefs_super = VarCorr(maximal_brms)[[&quot;item&quot;]][[&quot;sd&quot;]] We can use the same VarCorr function to get the superpopulation estimate of sigma, the residual error in our model. # superpopulation residual estimate sigma_super = VarCorr(maximal_brms)$residual$sd # give it a name because its unnamed by default rownames(sigma_super)=&quot;sigma&quot; We can then take these three matrices representing our standard deviations estimates and stick them together: # all superpopulation effects together superpopulation_effects = rbind (subj_ranefs_super, item_ranefs_super, sigma_super) superpopulation_effects ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 309.09416 35.53155 246.8724335 386.22851 ## S 52.82829 25.84257 4.5430876 103.37759 ## P 56.48286 26.14443 6.2896184 107.05695 ## C 71.57066 26.91964 12.0642003 121.35244 ## SP 78.33043 27.22390 17.3887223 128.61832 ## SC 29.93360 20.47755 1.3632664 75.99486 ## PC 71.32010 27.32401 12.0298328 123.19472 ## SPC 37.38488 23.35164 1.7069461 87.59383 ## Intercept 373.33136 50.32515 288.4503545 487.84638 ## S 31.38349 20.97907 1.5702143 77.30445 ## P 259.40602 37.96109 196.3904691 342.17678 ## C 46.16567 27.18196 2.7059299 101.91536 ## SP 20.62007 15.54163 0.9615122 58.42353 ## SC 34.24261 23.47740 1.7819649 86.05518 ## PC 58.00565 28.52386 4.3813078 112.79112 ## SPC 54.96571 27.19562 4.8733710 109.68920 ## sigma 653.35867 13.06193 628.5384992 679.31325 And plot this in what Gelman and colleagues call a Bayesian Anova plot. This allows us to quickly asses the large sources of variance in the data, and allows us to compare this to the residual error in our model. Figure 8.5: Estimates of superpopulation standard deviations for different random effects in our maximal KB07 model. 8.5.2 Getting the finite-population standard deviations from our models Gelman actually suggests using the finite-sample standard deviation estimates, and comparing the magnitude of these across our fixed effects, random effects, and residual error. When we focus on the finite-population standard deviation estimates we can compare the relative magnitudes of difference sources of variance on a more or less equal playing field. To calculate the standard deviations of different groups of parameters you need to calculate the standard deviation for each bundle, for each sample. This means you end up with 5000 (or however many) samples of the standard deviation based on each set of posterior samples. The code below shows how to calculate the finite-population standard deviations for item based on the random effects parameter estimates. # extract matrix representing all random effects from our model item_ranefs_finite = ranef(maximal_brms, summary = FALSE)[[&quot;item&quot;]] # the output is a 3d matrix. dimensions are: # 1) sample number (1-5000) # 2) item number (1-32) # 3) random effect number (1-8) str (item_ranefs_finite) ## num [1:5000, 1:32, 1:8] -353 -434 -469 -468 -365 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : NULL ## ..$ : chr [1:32] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## ..$ : chr [1:8] &quot;Intercept&quot; &quot;S&quot; &quot;P&quot; &quot;C&quot; ... # we need to find the standard deviation for the item random effects for each # random effect, for each set of samples. The function below calculates the # standard deviation along the 1st and 3rd dimensions of our 3d matrix, # collapsing the second (item) dimension item_ranefs_finite = apply (item_ranefs_finite[,,],c(1,3),sd) # the output is a 2d matrix. dimensions are: # 1) sample number (1-5000) # 2) random effect number (1-8) str (item_ranefs_finite) ## num [1:5000, 1:8] 365 361 361 366 368 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:8] &quot;Intercept&quot; &quot;S&quot; &quot;P&quot; &quot;C&quot; ... # we summarize the output into a matrix where each row represents a single # finite-population random effect estimate item_ranefs_finite = posterior_summary (item_ranefs_finite) For the fixed effects, we use the absolute value of the parameters because they are each a single degree of freedom (i.e. a single parameter). The code below shows how to get the fixed effects standard deviation estimates. # get individual parameter samples fixefs_finite = fixef(maximal_brms, summary = FALSE) # summarize absolute value fixefs_finite = posterior_summary (abs (fixefs_finite)) Finally, we can find the prediction error (the residuals) with the residuals function, and then calculate the standard deviation of the residuals for each set of samples. # get residuals sigma_finite = residuals (maximal_brms, summary = FALSE) # find standard deviation for each set of samples sigma_finite = apply (sigma_finite, 1, sd) # summarize sigma_finite = posterior_summary (sigma_finite) # name row, because it has no name by default row.names(sigma_finite) = &#39;sigma&#39; 8.5.3 Using the banova function I wrote a simple function that does the above steps for you. You can choose whether you want the finite-population or superpopulation standard deviations (though it can only ever use the finite-population for the fixed effects). The output is a single dataframe that contains a summary of all fixed and random effects, and sigma if the model contains normally-distributed error. maximal_banova = banova (maximal_brms) # inspect output maximal_banova ## Estimate Est.Error Q2.5 Q97.5 cluster ## sigma 653.09494 6.595197 640.3732775 666.17096 sigma ## Intercept 2172.28858 78.430699 2020.3507829 2324.35811 fixefs ## S 66.94226 18.129586 30.7611733 101.79869 fixefs ## P 329.20511 49.402456 235.3017587 429.06277 fixefs ## C 79.03061 20.412822 39.7167610 119.10522 fixefs ## SP 23.96781 16.158583 1.0133929 58.71126 fixefs ## SC 21.42102 14.563791 0.9812082 54.11046 fixefs ## PC 17.80075 13.449979 0.7310491 49.68008 fixefs ## SPC 26.09428 16.864884 1.2687924 62.72075 fixefs ## Intercept:item 366.92509 16.770748 333.8218755 399.86701 item ## S:item 29.87270 19.509138 1.5404471 71.39403 item ## P:item 255.40122 19.336650 218.8252527 293.91863 item ## C:item 43.50635 24.599073 2.6019614 91.63498 item ## SP:item 19.60249 14.311265 0.8789549 53.60589 item ## SC:item 32.32763 21.328790 1.7038979 77.96548 item ## PC:item 54.28645 25.396590 4.3513090 99.94241 item ## SPC:item 52.00120 24.461426 4.7274922 98.07662 item ## Intercept:subj 302.17599 18.044662 267.0665426 339.00046 subj ## S:subj 51.67352 24.683656 4.4734142 98.15312 subj ## P:subj 55.14963 24.893009 6.2876688 101.85838 subj ## C:subj 70.04796 25.308165 11.9181579 113.55929 subj ## SP:subj 76.33887 25.383581 16.8116679 120.32346 subj ## SC:subj 29.11213 19.640120 1.3413590 72.14067 subj ## PC:subj 69.74025 25.820290 11.5482570 116.14938 subj ## SPC:subj 36.44247 22.377150 1.6708271 82.42874 subj The output of the banova function can be used to make a Bayesian ANOVA plot of our model. If we were to do this right after fitting the model, we would have a pretty good idea of what matters and what doesnt in our data. Figure 8.6: Standard deviation estimates for each component of our KB07 model. Lines indicate 95% credible intervals. The process Gelman proposes is potentially more complicated that what Im doing here. For example, consider the random effects for a factor like vowel category. Imagine there are four categories, so four levels, for each of 50 listeners. The process Ive described treats each of the 50 random effects for each vowel separately (i.e., 4 groups of 50 vowel random effects). The process described by Gelman would treat the 200 vowel random effects (across the four vowels) as one batch of coefficients. This single batch would reflect all of the \\(Listener \\colon Vowel\\) interaction. The way we are approaching instead separates each \\(Listener \\colon Vowel\\) simple effect and treats it separately. The main reason to do it the way way Ive shown above is because it can be done easily for all models, and you get roughly equivalent information from the analysis. If you do want to investigate the variation associated with entire clusters of multiple predictors at a time, please see the Gelman articles linked to above, as there are a few important details that are not discussed here (e.g., the need to recover missing parameters, the need to calculate degrees of freedom, etc.). 8.5.4 Applying the banova function to our gender perception model We can load the model we fit in Chapter 7 (logistic_g0) and apply the banova function to it. logistic_banova = banova (logistic_g0) # inspect output logistic_banova ## Estimate Est.Error Q2.5 Q97.5 cluster ## Intercept 0.7416706 0.4648814 0.045743928 1.7789047 fixefs ## g0_s 3.2941081 0.5917762 2.167940600 4.5000468 fixefs ## pfemale 2.3067717 0.2466901 1.840272045 2.8007507 fixefs ## g0_s:pfemale 1.1873830 0.5334107 0.196241521 2.2988926 fixefs ## Intercept:speaker 2.7511229 0.2986038 2.254733741 3.4057535 speaker ## Intercept:subj 1.1743633 0.1925801 0.816857707 1.5575303 subj ## g0_s:subj 1.3204798 0.2889046 0.821822086 1.9690607 subj ## pfemale:subj 0.2319804 0.1629290 0.009011726 0.5947397 subj ## g0_s:pfemale:subj 1.1496191 0.3009733 0.617755680 1.7851284 subj We can then take the output and plot it using another simple function I wrote called banovaplot, which will plot the output of the banova function. This plot shows us which fixed effects have the largest magnitudes, and also lets us compare the magnitude of fixed effects to their between-subject variation. For example, notice that although the effect for \\(pfemale\\) is relatively large, there is almost no between-subject variation in this parameter. In contrast, between-subject variation in intercepts is larger than the overall model intercept. par (mfrow = c(1,1), mar = c(5.2,9.2,1,1)) banovaplot (logistic_banova, xlim=c(0,5), xaxs=&#39;i&#39;, horizontal = FALSE, xlab = &quot;Logits&quot;) Figure 8.7: Banova plot of our logistic model from Chapter 7. 8.6 Applying a Bayesian ANOVA to a more-complicated model 8.6.1 Description of the model Were going to fit our most complicated model yet. The model includes the following variables: padult: a dichotomous variable, 1=adult response, 0=child response. g0_s: a continuous predictor, scaled log-f0. gbar_s: a continuous predictor, scaled log-mean formant frequency (a measure of vocal-tract length). pfemale: a factor indicating whether subjects indicated hearing a female speaker, pfemale1=female, -pfemale1=female. vowel: a factor with two levels, vowel1=a, -vowel1=i. subj: a factor indicating subject/listener (n=10). speaker: a factor indicating speaker (n=139). The model formula looks like this: padult ~ ( g0_s + gbar_s) * pfemale + vowel + (( g0_s + gbar_s) * pfemale + vowel | subj ) + ( 1 | speaker ) Its broken up into three lines only so that it will fit on the page. Ive aligned the line to make the structure of the three components (main effects, subject effects, speaker effects) clearer. The model says: \"Model the perception of adultness using the continuous predictors g0 and gbar. These predictors interact with pfemale. This tells us that in addition to intercept shifts for the perception of femaleness, there can also be differences in the slopes of our continuous predictors. We also include vowel category as a predictor. Since this does not interact with our continuous predictors, it represents only interecept shifts. Our entire model is replicated inside the subj cluster, meaning we are fitting a maximal model with respect to subjects. We are only including random intercepts for speakers. Notice that we are not using a really maximal model since we are not including random effects for all speakers. This is because I dont think our data supports the inclusion of random slopes for speaker! With only two different tokens per speaker we dont have much information about f0 variation within speaker. We also have no repetitions of the vowel for each person, so our speaker-specific measurements of vowel effects would be imprecise. Finally, many voices were likely not perceived as both male or female often enough to really test for the effect of perception of perceived femaleness, within-speaker. 8.6.2 Fitting the model We load and set up the data below: # source data url1 = &quot;https://raw.githubusercontent.com/santiagobarreda&quot; url2 = &quot;/stats-class/master/data/h95_experiment_data.csv&quot; h95 = read.csv (url(paste0 (url1, url2))) # padult = 1 for adult responses, 0 for child responses h95$padult = as.numeric (h95$pgroup %in% c(&#39;w&#39;,&#39;m&#39;)) # factor with f for female responses and m for male responses h95$pfemale = &quot;&quot; h95$pfemale[h95$pgroup %in% c(&#39;w&#39;,&#39;g&#39;)] = &quot;f&quot; h95$pfemale[h95$pgroup %in% c(&#39;m&#39;,&#39;b&#39;)] = &quot;m&quot; h95$pfemale = factor(h95$pfemale) # standardize log f0 h95$gbar_s = (h95$g0-mean(h95$g0)) / sd(h95$g0) # standardize log geometric mean formant frequency h95$gbar_s = (h95$gbar-mean(h95$gbar)) / sd(h95$gbar) And then fit the model: set.seed (1) logistic_g0_gbar = brm (padult ~ (g0_s+gbar_s) * pfemale + vowel + ((g0_s+gbar_s) * pfemale + vowel | subj) + (1|speaker), data=h95, chains=4, cores=4, family=&quot;bernoulli&quot;, warmup=1000, iter = 3000, thin = 2, control = list(adapt_delta = 0.95), prior = c(set_prior(&quot;student_t(3, 0, 3)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 3)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 3)&quot;, class = &quot;sd&quot;), set_prior(&quot;lkj_corr_cholesky (2)&quot;, class = &quot;cor&quot;))) # save model # saveRDS (logistic_g0_gbar, &#39;8_logistic_g0_gbar.RDS&#39;) 8.6.3 Interpreting the model After inspecting the model to check for convergence and an adequate ESS, we can make a Bayesian ANOVA for the model, and plot the results. logistic_g0_gbar_banova = banova (logistic_g0_gbar) Figure 8.8: Banova plot for a multivariate regression model predicting the perception of adultness from f0 and formant frequencies. Were going to leave the interpretation of this model for the next chapter. However, the ANOVA plot lets us easily consider the important sources of variation in our data, and can guide our interpretation of the model. If predictors are only small sources of variance, and especially if these are accompanied by credible intervals that are wide relative to their small values, these may be very small and meaningless values (even if not equal to exactly zero). As a result, it makes sense to begin interpreting a model based on the largest sources of variance. 8.7 Plot Code ################################################################################ ### Figure 8.1 ################################################################################ par (mfrow = c(1,1), mar = c(4,4,1,1)) shift = -.3 brmplot (xs = (1:7)+shift, fe_f[8:2,], horizontal = TRUE, col = deepgreen, xlim = c(.5,7.5), xlab = &#39;Predictors&#39;, ylab = &#39;RT (msec)&#39;,labels=&quot;&quot;) abline (h = 0, lty=3) shift = -.15 points ((7:1)+shift, sumf[[10]][2:8,1], col=coral,lwd=2,cex=1.5,pch=0) arrows ((7:1)+shift, sumf[[10]][2:8,1]+sumf[[10]][2:8,2]*2,(7:1)+shift, sumf[[10]][2:8,1]-sumf[[10]][2:8,2]*2, lwd=2,col=coral,length=0) shift = +.0 brmplot (xs = (1:7)+shift, fe_m[8:2,], horizontal=TRUE, add=TRUE, col=skyblue) abline (h = 0, lty=1) shift = +.15 points ((7:1)+shift, summ[[10]][2:8,1], pch=0, col=teal,lwd=2,cex=1.5) arrows ((7:1)+shift, summ[[10]][2:8,1]+summ[[10]][2:8,2]*2, (7:1)+shift,pch=0, summ[[10]][2:8,1]-summ[[10]][2:8,2]*2, lwd=2,col=teal,length=0) legend (1, -150, bty=&#39;n&#39;, pch=c(16,0,16,0), pt.cex=1.5, col=c(deepgreen,coral,skyblue,teal),lwd=2, legend = c(&quot;Maximal brm&quot;,&quot;Maximal lmer&quot;,&quot;Final brm&quot;,&quot;Final lmer&quot;)) ################################################################################ ### Figure 8.2 ################################################################################ par (mfrow = c(1,1), mar = c(7,4,1,1)) labels = paste0 (c(&quot;&quot;,rep(&quot;Item:&quot;,8),rep(&quot;Subj:&quot;,8)), rownames(vars_m)) brmplot (xs = (1:17), vars_m, horizontal = TRUE, col = deepgreen, xlab = &#39;&#39;, yaxs=&#39;i&#39;, ylab=&#39;RT (msec)&#39;,ylim = c(0,750),las=2, labels = labels) grid() points ((1:17), lmer_vars_m, pch=0, col=coral,lwd=3,cex=2) points (c(1,7,9,17)+.3, lmer_vars_f, pch=0, col=teal,lwd=3,cex=2) brmplot (xs = c(1,7,9,17)+.3, vars_final, horizontal = TRUE, col = skyblue, add = TRUE) abline (h=0) legend (11, 600, bty=&#39;n&#39;, pch=c(16,0,16,0), pt.cex=1.5, col=c(deepgreen,coral,skyblue,teal),pt.lwd=2, legend = c(&quot;Maximal brm&quot;,&quot;Maximal lmer&quot;,&quot;Final brm&quot;,&quot;Final lmer&quot;)) ################################################################################ ### Figure 8.3 ################################################################################ subjrhom = attr(VarCorr(maximal_lmer)[[1]],&#39;correlation&#39;) par (mfrow = c(1,1), mar = c(5,5.5,3,1)) corsuse = c(2:8,(3:8)+8,(4:8)+16,(5:8)+24,(6:8)+32,(7:8)+40,(8)+48) blabels = c(&quot;SPC&quot;,&quot;PC&quot;,&quot;SC&quot;,&quot;SP&quot;,&quot;C&quot;,&quot;P&quot;,&quot;S&quot;,&quot;Int&quot;) labs = expand.grid(blabels,blabels)[corsuse,] labs = paste(labs[,1],labs[,2],sep=&quot; : &quot;) corrs_ms = VarCorr(maximal_brms)$subj$cor corrs_ms = rbind (corrs_ms[7:1,,8], corrs_ms[6:1,,7], corrs_ms[5:1,,6], corrs_ms[4:1,,5], corrs_ms[3:1,,4], corrs_ms[2:1,,3], corrs_ms[1,,2]) brmplot (corrs_ms, ylim = c(-1,1), main = &quot;Subject Correlation Estimates&quot;, yaxs=&quot;i&quot;, cex.main=1, las=2, ylab = &quot;Correlation&quot;, labels = labs, col=deepgreen) abline (h = 0, lty = 3) points (28:1, subjrhom[corsuse], pch=0, col=coral,lwd=2,cex=1.5) ################################################################################ ### Figure 8.4 ################################################################################ brmplot (corrs_mi, ylim = c(-1,1), main = &quot;Subject Correlation Estimates&quot;, yaxs=&quot;i&quot;, las=2, labels = labs, col = deepgreen) abline (h = 0, lty = 3) points (28:1, itemrhom[corsuse], pch=0, col=coral,lwd=2,cex=1.5) points (27+.4,corrs_fi[1], cex = 1.5, col = skyblue, pch=16) segments (27+.4, corrs_fi[3], 27+.4, corrs_fi[4], lwd=2, col=skyblue) points (27+.4, itemrhof[2], col=teal,lwd=3, pch = 0, cex = 1.5) ################################################################################ ### Figure 8.5 ################################################################################ rownames(sigma_super)=&quot;sigma&quot; superpopulation_effects = rbind (item_ranefs_super, subj_ranefs_super, sigma_super) par (mfrow = c(1,1), mar = c(5,4,3,1)) brmplot (superpopulation_effects, yaxs=&#39;i&#39;, ylim = c(0,750), main=&quot;Fixed Effects&quot;, cex.main=1.2,las=2, col = c(rep(skyblue,8),rep(lavender,8),darkorange)) text (5,500, label = &quot;Item Effects&quot;,col=skyblue, font = 2) text (12,500, label = &quot;Subject Effects&quot;,col=lavender, font = 2) ################################################################################ ### Figure 8.6 ################################################################################ par (mfrow = c(1,1), mar = c(7,4.2,1,1)) banovaplot (maximal_banova[-2,], yaxs = &#39;i&#39;, ylim=c(0,880), main=&quot;Fixed Effects&quot;, las=2, ylab = &quot;Reaction Time (ms)&quot;) text (5,500, label = &quot;Fixed Effects&quot;,col=coral, font = 2) text (12,500, label = &quot;Item Effects&quot;,col=deepgreen, font = 2) text (20,500, label = &quot;Subject Effects&quot;,col=teal, font = 2) "]]
