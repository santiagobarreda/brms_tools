[["index.html", "A Quick Introduction to Multilevel Bayesian Models for Linguistic Researchers Introduction", " A Quick Introduction to Multilevel Bayesian Models for Linguistic Researchers Santiago Bareda 2021-01-05 Introduction Im working on this book for a stats class in the Linguistics department at UC Davis. If you are reading this then I am currently working on it, and it may contain errors, inconsistencies, etc. This book will focus on a conceptual introduction to multilevel Bayesian models. I am going to talk about math as little as possible, so I am going to avoid going into details unless necessary. My goal is to convey some of the more important and useful concepts necessary to understand these models and to use them in your own work. I hope to set you up in a position to fill in the blanks as necessary later. Im going to try to provide intuitive explanations for statistical models that rely on understanding the figures we use represent and interpret our data and models. The book assumes some understanding of R (only to understand the code), but none of statistics. I dont really explain many basic things, for example, how to calculate a standard deviation? Why is it calculated like it is? There are two reasons for this. First, there are many better resources for that (many written by real statisticians!). And second, I am trying to make this quick, to go from 0 to understanding and interpreting Bayesian multilevel models in 10 weeks. The book is specifically designed for linguists only because it focuses on the sorts of research designs frequently used by linguists. However, the models and principles outlined in this book are used, and are useful, in many domains. If you have any comments or suggestions please let me know, and if you find any errors definitely let me know! "],["inspecting-a-single-group-of-observations.html", "Chapter 1 Inspecting a single group of observations 1.1 Data and research questions 1.2 Probability Distributions 1.3 Probabilities of events and likelihoods of parameters 1.4 Bayesian models 1.5 Posterior distributions 1.6 Plot Code", " Chapter 1 Inspecting a single group of observations In this chapter I am going to present an introduction to some fundamental statistical concepts (i.e, probability, likelihood). We will discuss how to use these concepts to make inferences about our observations. The things I talk about in this chapter will be come up in every chapter, so if things dont all make snese right now thats fine, things will make more sense bit by bit as we learn how to use more and more complicated models. 1.1 Data and research questions Basically the simplest question a researcher can ask is: what is the value of a single number? What is the average ________ ? As an example of this, we are going to investigate variation in voice fundamental frequency (f0) in a sample of speakers. The f0 of a voice is the primary determinant of perceived pitch, and is a very important cue in speech communication. It relates to phoneme identification, prosody, and to the communication of social and indexical information (speaker gender, age, etc.). We are going use a well-known data set, the Hillenbrand et al. (1995) data of Michigan English. We are going to focus on a single vector (f0), representing the f0 produced by a set of female speakers. url1 = &quot;https://raw.githubusercontent.com/santiagobarreda&quot; url2 = &quot;/stats-class/master/data/h95_vowel_data.csv&quot; ## read data from my Github page h95 = read.csv (url(paste0 (url1, url2))) # select the &#39;f0&#39; vector, for women only (speaker type = &#39;w&#39;) f0 = h95[[&#39;f0&#39;]][h95$group == &#39;w&#39;] These speakers represent a sample from a larger population. The sample is a finite set of observations that you actually have. The population is the (hypothetical) larger group of all possible observations that you are actually interested in. For example, Hillenbrand et al. collected this data not to study these speakers in particular, but instead to make inferences about Michigan speakers more generally. Similarly, we want to answer a few basic questions about the population of female speakers from Michigan, not about the sample itself: What is the average f0 of the whole population likely to be? Can we set bounds on likely mean f0 values based on the data we collected? The second point is crucial. First, our sample will never exactly match the population. But it should be representative of it, meaning it should not be too far off from the real mean. If it is too far off, then is it really a representative sample from that population? For example, the mean f0 in the data we will discuss below is 220 Hz. This seems to suggest that, for example, a true mean of 100 Hz is unlikely. Is a true mean of 150 Hz also unlikely? What about 190 Hz? 1.1.1 Inspecting the central location and spread of values We can easily find descriptive statistics like the sample mean (\\(\\bar{x}\\)), the sample standard deviation (\\(s_x\\)), and important quantiles for this sample of f0 values. The quantiles below correspond to the values of ordered observations, like in the right plot in Figure 1.1. The 0% quantile is the smallest (leftmost) observation, while 100% is the highest (rightmost) observation. Any other quantile is found by ordering the observations and selecting the observation that is higher than x% of the sample values. For example, the 50% quantile (the median) is higher than 50% of values, and the 25% quantile is higher than 1/4 of the values in the sample. par (mfrow = c(1,2), mar = c(1,4,1,1)) plot (f0, xaxt=&#39;n&#39;,xlab=&#39;&#39;, ylab = &#39;f0&#39;, pch = 16, col = skyblue) plot (sort (f0), xaxt=&#39;n&#39;,xlab=&#39;&#39;,ylab=&#39;f0 (sorted)&#39;, pch=16, col = deepgreen) Figure 1.1: (left) Plot of values in the order they appear in the original data. (right) Observations ordered by increasing value. ## calculate basic descriptive statistics mean (f0) ## [1] 220.401 sd (f0) ## [1] 23.22069 quantile (f0) ## 0% 25% 50% 75% 100% ## 149.00 207.00 220.00 236.25 307.00 We can look at the distribution of productions of f0 in several ways, as seen in Figure 1.2. In the top row, points indicate individual productions, and are jittered along the y axis to make them easier to see. In the middle row we see a histogram of the same data. The histogram divides the x axis into a set of discrete sections (bins), and gives you the count of observations in each bin. In the bottom row we see a box plot of the same data. The edges of the box correspond to the 25 and 75% quantiles of the distribution, and the line in the middle of it corresponds to the median. As a result, 50% of observations are contained in the box. par (mfrow = c(3,1), mar = c(1,4,1,1), oma = c(4,0,0,0)) plot (f0, jitter (rep(1,length(f0))), xlim = c(140, 320), ylim = c(.95,1.05), yaxt=&#39;n&#39;,ylab=&#39;&#39;, pch = 16, col = yellow) hist (f0,main=&quot;&quot;, col = teal) boxplot (f0, horizontal = TRUE, ylim = c(140, 320), col = coral) mtext (side =1, outer = TRUE, text = &quot;f0&quot;, line = 2.5) Figure 1.2: Different ways to consider our distribution of f0 values. 1.2 Probability Distributions Histograms are particularly useful to understand because of how they relate to probability distributions. For our purposes, the probability is the number of times an event is expected to occur, out of all the other observed events and outcomes. This can also be thought of as the percent of times an event is expected to occur. The total probability of all events is always equal to 1. This is like using 100 to communicate percent, its just easier that way. As a result of this convention, you know that a probability of 0.5 means something is expected to occur half the time (i.e., on 50% of trials). For example, suppose we want to know the probability of being an adult female in our sample who produces an f0 under 175 Hz. Finding the probability of observing this event is easy: # the evaluation in the parenthesis will return 1 if true, 0 if false sum (f0 &lt; 175) ## number of observations the fall below threshold ## [1] 22 sum (f0 &lt; 175) / length (f0) ## divided by total number of events ## [1] 0.03819444 mean (f0 &lt; 175) ## a shortcut to calculate probability, mean = total/length ## [1] 0.03819444 The top value is the frequency of the occurrence. This is not so useful because this number can mean different things given different sample sizes (e.g., 22/23, 22/10000). The middle and bottom values have been divided by the total number of observations. As a result, these now represent a proportion, or probability. Histograms can also show this difference between total counts and probabilities. Below, the histogram on the left shows the number of observations in each bin. The histogram on the right shows density on the y axis. When you see density on the y axis, that means that y axis values have been scaled to make the area under the curve equal to 1. This has two benefits: It lets you compare the distribution of values across different sample sizes. It makes the histogram more comparable to a probability distribution. par (mfrow = c(1,2), mar = c(4,4,1,1)) hist (f0, main=&quot;&quot;, col = lavender) hist (f0, freq = FALSE, main = &quot;&quot;, col = deepgreen) Figure 1.3: (left) A histogram of our f0 data showing counts in each bin. (right) A histogram of our f0 data showing densities. The density is just the thickness of the distribution at a certain location. In probability theory, the sum of the probabilities of all possible outcomes is 1, by definition. So, the fact that the area under the curve of a density is equal to 1 means that the density contains all your stuff, all the possible outcomes of the variable we are discussing. Imagine a circle like in a Venn diagram that contains all possible productions of female f0. This circle has an area of 1 since it contains all possible instances of the variable. Imagine we spread out this circle along the x axis so that its shape reflected the relative frequencies of different values of the variable. For example, if some outcomes were 5 times more probable than others, the shape should be 5 times taller there, and so on. If we managed to do this, the height (or density) of this shape would exactly correspond to a probability distribution like that seen in the right plot above. Below Ive repeated the data, doubling the counts. Notice that the y axis in the right panel does not change. This is because increasing the number of observations changes your counts but not the relative frequencies of observations. For instance, increasing the number of coin flips will not change the fact that 50% will be heads, but it will change the number of heads observed. par (mfrow = c(1,2), mar = c(4,4,1,1)) hist (c(f0,f0), breaks = 10, main = &quot;&quot;, col = lavender) hist (c(f0,f0), freq = FALSE, breaks = 10, main = &quot;&quot;, col = deepgreen) Figure 1.4: The counts have been doubled relative to above. 1.2.1 The normal distribution The distribution of many variables (including f0) follows whats called a normal distribution. This means if you take a random sample of a variable and arrange observations into bins, they will tend to resemble the shape of a normal distribution. This distribution is also called a Gaussian distribution and has a familiar, bell-shaped curve. The normal distribution has the following important characteristics. The distribution is approximately symmetrical - i.e., producing a higher or lower than average f0 is about equally likely. The probability of observing a given value decreases as you get further from the mean. Its easy to work with, very well understood, and naturally arises in basically all domains. Normal distributions have two parameters. This means they vary from each other in only two ways. These parameters are: A mean, \\(\\mu\\), which determines where the distribution is located along the x axis. The mean is the 50% halfway point of the mass of the distribution. If the distribution were an physical object, its mean would be its center of gravity. A standard deviation, \\(\\sigma\\), that determines its spread along the x axis. Since every distribution has an area under the curve equal to one (they all have the same volume), the smaller the variance the higher the peak of the density along the y axis must be. Below, I compare the histogram of f0 values to the density of a normal distribution with a mean equal to our sample mean (\\(\\mu = \\bar{f0}\\)) and a standard deviation equal to our sample standard deviation (\\(\\sigma = s_{f0}\\)). The density was drawn using the dnorm function. This function will help draw a curve representing the shape of a theoretical normal distribution with a given mean and standard deviation. par (mfrow = c(1,1), mar = c(4,4,1,1)) hist (f0, freq = FALSE, main = &quot;&quot;, breaks = 20, col = deepgreen) abline (v = 63.8, lwd = 2, col = 2, lty=3) ## plots the normal density (red line) using stats calculated form our sample. curve (dnorm (x, mean(f0), sd(f0)), from = 100, to = 300, lwd=3, col = coral, add = TRUE) Figure 1.5: A comparison of the data distribution with a theoretical normal distribution. When you are dealing with normally-distributed data, summary statistics can tell you a lot about the shape of your distribution, and about where you can expect the bulk of the density/distribution to lie. The left panel shows the locations of quantiles (0%, 25%, 50%, 75%, 100%), the right panel shows you the mean and standard deviations from the mean (-3, -2, 0, +1, +2, +3). Notice that ±2 standard deviations enclose most of the distribution (around 95%), and ±3 standard deviations enclose almost all of it (99%). par (mfrow = c(1,2), mar = c(4,4,1,1)) hist (f0, main = &quot;&quot;, col = skyblue) abline (v = quantile (f0), lwd = 2, col = deepgreen) hist (c(f0,f0), freq = FALSE, breaks = 10, main = &quot;&quot;, col = yellow) abline (v = seq (mean(f0)-3*sd(f0),mean(f0)+3*sd(f0),sd(f0)), lwd = 2, col = coral) Figure 1.6: Quantiles and standard deviations help understand the shape of a distribution. 1.2.2 Referring to the normal distribution to make inferences In general, it is impossible to know what the true data distribution is, so that perfect inference is not possible. As a result, scientists often use theoretical probability distributions to make inferences about real-life populations and observations. Notice that our real life measurements follow the shape predicted by the theoretical normal distribution. This suggests that we may be able to use the characteristics of an appropriate normal distribution to make inferences about female f0 (and other variables). Using a normal distribution to make inferences about your data is like using a mathematical model for spheres to understand the behavior of billiard balls. In reality the balls are not perfect spheres. However, their shapes will be spherical enough to allow us to make useful predictions based on the simplified model. In general, it is useful to keep in mind that reality will never exactly conform to our model. This can result in unpredictable errors in our conclusions. In general, the things you dont know you dont know are the things that will cause the most problems. If you know where your model went wrong, you would have fixed it! Since we expect the distribution of f0 values to have the shape of the normal distribution, we can use the shape of the normal distribution to make inferences about the distribution of f0 values, even the ones we did not observe. For example, we can use the theoretical normal density to estimate the probability of observing a female production with an f0 of under 175 Hz, from among all possible observable productions of f0 in this population. We do this by referring to the proportion of values expected to be less 175 Hz in the normal distribution that has the same shape as our sample. This can be found by finding the area under the curve of the probability density to the left of that point (the red area below). Since the total area is always equal to 1, the area of the red portion below corresponds to a percentage/probability. Below, I use the function pnorm to find the proportion of values that are expected to be greater/less than 175 Hz. I use the parameters estimated form our sample to run the pnormfunction, as these are our best guesses of the population parameters. As we can see, this value is reasonably close to our empirical proportion, which was 0.038 (3.8%). par (mfrow = c(1,1), mar = c(4,4,1,1)) hist (f0, freq = FALSE, main = &quot;&quot;, breaks = 20, col = deepgreen) abline (v = 175, lwd = 2, col = 2, lty=3) ## plots the normal density (red line) using stats calculated form our sample. curve (dnorm (x, mean(f0), sd(f0)),from=100, to=300, lwd=2, col = 2, add=TRUE) x = c(140,seq(140,175,length.out = 100),175) y = c(0,dnorm(seq(140,175,length.out = 100), mean (f0), sd (f0)),0) polygon(x, y, col=&#39;2&#39;) abline (v = 63.8, lwd = 2, col = 2, lty=3); abline (v = 70, lwd = 2,col=1,lty=3) Figure 1.7: The read area relects the distribution of outcomes that satisfy f0 &lt; 175 Hz. ## probability of observing a production below 175 Hz pnorm (175, mean (f0), sd(f0)) ## [1] 0.02527988 ## probability of observing a production greater than 175 Hz 1 - pnorm (175, mean (f0), sd(f0)) ## [1] 0.9747201 Imagine you had 1 pound of clay and I asked you to make a shape exactly like the normal density (red curve) above with a constant depth. The area under the curve would just correspond to the amount of clay in a certain area. So, if you made the density just right and I took a knife and cut the shape left of 175 Hz (the red part) and we weighed it, it should weigh 2.5% of a pound. So, the area under the curve, the probability, is just the amount of the stuff in the density that falls below/above a certain point, or between two points. Since the total number of observations is always one. This helps us compare across many different actual numbers of observations. The probability above suggests the following: ## probability of observing a production with an f0 under 175 Hz pnorm (175, mean (f0), sd(f0)) ## [1] 0.02527988 ## expected count pnorm (175, mean (f0), sd(f0)) * length (f0) ## [1] 14.56121 ## actual count sum (f0 &lt; 175) ## [1] 22 We can also use this theoretical distribution to think about other possible outcomes: min (f0) ## [1] 149 pnorm (149, mean (f0), sd(f0)) # probability of observing our smallest value ## [1] 0.001052907 pnorm (140, mean (f0), sd(f0)) # probability of observing a smaller value ## [1] 0.0002676171 ## predicted number of tokens below 175 Hz if we was had 5500 observations pnorm (175, mean (f0), sd(f0)) * 5500 ## [1] 139.0394 1.3 Probabilities of events and likelihoods of parameters We are going to switch from talking about probabilities to talking about likelihoods. A probability is the odds of observing some data/event/outcome, given some parameter(s). A likelihood places odds on different parameters given some observed data. Every parameter for every probability distribution has a likelihood function, given some data. I am only going to talk about the likelihood of the normal mean parameter, \\(\\mu\\), in detail. The likelihood function is a curve showing the relative likelihoods of different parameter values, given a fixed set of data. The likelihood function tells you what values are believable given your data. If a value is very unlikely, that means that it is not supported by your data. In other words, unlikely parameter estimates represent conclusions that your data is rejecting as not viable. Here are three useful properties of the likelihood functions of \\(\\mu\\), the mean parameter of the normal distribution: The likelihood function of \\(\\mu\\) will tend to be a normal distribution. The mean (and peak) of the likelihood function of \\(\\mu\\) given some sample \\(x\\) is equal to the arithmetic mean of the sample (\\(\\bar{x}=mean(x)\\)). The standard deviation of the likelihood of \\(\\mu\\) is equal to the standard deviation of the data (\\(s_x=sd(x)\\)), divided by the square root of N (the sample size). The first point tells us that we can use the normal distribution to make inferences about likely, and unlikely values for means, given some data. The second point says that if you are wondering what the best (most likely) estimate of \\(\\mu\\) is given your sample, the answer is the arithmetic mean of your sample (\\(\\bar{x}\\)). The third point means that the likelihood function for \\(\\mu\\) will tend to be much narrower than the distribution of our original data. This is because a mean based on, for example, 50 samples will contain many positive and negative deviations from the average that will tend to cancel out. As a result, the more data you have the more precise your estimates are, and the less uncertainty is associated with any estimate. The left panel in Figure 1.8 (code at end of chapter) shows the likelihood function for \\(\\mu\\) based on the first 10 observations of our f0 vector, shown by the blue points at the bottom of the plot. I chose this small sample just to make this example clearer. Notice that the most likely mean values of \\(\\mu\\) for these points like over the bulk of the sampled values. The vertical dotted lines show three possible mean values that will be highlighted. The likelihood of any parameter estimate (e.g., \\(\\mu\\) = 175 Hz in the right panel of Figure 1.8) is equal to the product of the density of each observation in the sample, if we assume that the estimate were true. For example, to calculate the likelihood that \\(\\mu=175\\), we: Assume that the data is generated by a normal distribution with a \\(\\mu\\) equal to 175 Hz, and \\(\\sigma\\) equal to the sample standard deviation (\\(s_{f0}\\)). Find the the height of the curve of the probability distribution (the density) over each point (indicated by lines in the right panel below). The likelihood is the product of all of these densities (heights). In practice, the logarithms of the individual probabilities are added together, yielding the log-likelihood. This is because multiplying together too many fractions can lead to numbers so small computers have a hard time representing them, and adding logarithms is equivalent to multiplying to original values. Imagine I follow the steps above for each position along the x axis, recording the likelihood values I calculate. I then plot the product of the densities for each corresponding x value. If I do this I have just plotted a likelihood function for \\(\\mu\\) given our data. Figure 1.8: (Left) The likelihood of the population mean given the blue points in the figure. (right) The probability of the points given an assumed mean of 175 Hz. In the right panel in Figure 1.8 we see that a normal distribution with a \\(\\mu\\) of 175 Hz is very unlikely to generate this data. Many points are extremely improbable and have densities close to zero. As a result, the product of these values (the heights of the lines) will be a very small number. This is reflected in the extremely small values in the likelihood function at 175 Hz in the left panel above. In the left panel in Figure 1.9 (code at end of chapter), we see that a normal distribution with a \\(\\mu\\) of 200 Hz is more likely to generate this data, and the probability distribution is clearly a much better fit. However a distribution with a mean of 200 Hz is still not very likely to have generated this data. Finally, in the right panel below we see the the maximum likelihood estimate of 225 Hz, the value representing the peak of the likelihood function (in the left panel above). When we say that 225 Hz is the most likely mean for this data, we are saying that this data is most probably the outcome of a normal distribution centered at 225 Hz, relative to the alternatives. Figure 1.9: (Left) The probability of the points given an assumed mean of 200 Hz. (right) The probability of the points given an assumed mean of 225 Hz. 1.3.1 Making inferences using likelihoods Previously, I mentioned using the normal distribution to make inferences. When variables are normally distributed we can use the theoretical normal distribution and functions such as pnorm to answer questions about values we expect, and dont expect, to see. We can take this same approach to make inferences about parameters when their likelihood functions follow a normal distribution. For example, we can use the results of the calculations below: mean (f0) ## sample mean ## [1] 220.401 sd (f0) ## sample standard deviation ## [1] 23.22069 length (f0) ## sample size ## [1] 576 sd (f0) / sqrt ( length (f0) ) ## the standard deviation of the likelihood function ## [1] 0.9675289 To draw the expected likelihood function for \\(\\mu\\) given our data and our model. You may be thinking, what model? It may seem too simple to be a model, but by assuming that our data can be understood as coming from a normal distribution with some given \\(\\mu\\) and \\(\\sigma\\), we have already created a simple model for our data. Ill return to this below. We can take our model and our parameter estimates and draw the likelihood function for \\(\\mu\\). We can then use the qnorm function to calculate quantiles for our likelihood, presented below. I added vertical lines at the 2.5% and 97.5% quantiles of our distribution. These vertical lines enclose 95% of the likelihood density, and so represent the range of values representing the 95% most likely values of \\(\\mu\\). I chose an interval enclosing 95% of the likelihood because this is used by convention. This is a commonly-used interval but otherwise has no special significance. par (mfrow =c(1,1), mar = c(4,4,1,1)) curve (dnorm (x, mean(f0), sd(f0)/sqrt(length(f0))), xlim = c(216,225), ylab = &#39;Density&#39;, xlab = &#39;f0&#39;, col = lavender, lwd = 4) quantiles = qnorm (c(0.025, 0.975), mean (f0), sd (f0) / sqrt (length (f0) ) ) quantiles ## [1] 218.5047 222.2974 abline (v = quantiles, lwd=2,col=deepgreen) Figure 1.10: Likelihood of population mean given our data. Horizontal lines indicate intervals enclosing 95% of the distribution. The likelihood tells you about the most believable/credible parameter values, given your model and data. Given the information presented in the figure above, we may conclude that the most likely parameter values fall between 218 and 222 Hz. This means that it is reasonable that the true value might be 221 Hz, as this value is very likely given our sample. Basically, maybe our sample mean is wrong and arose by accident, and 221 Hz is the true \\(\\mu\\). This outcome is compatible with our data. However, a value of 216 Hz is extremely unlikely to fit our data. It is just too far from our sample mean relative to the amount of variation in our sample. This is like if you measured the heights of 100 women in a small town (pop. 1500) and found the average was 54. You might accept that the actual population average is 55, but may find it difficult to accept that it was actually 60\". It would mean you happened to measure all of the shortest women in the town, an extremely unlikely event. So, since we think that 216 Hz is not a plausible mean f0 given our sample, this also means that it is very unlikely that the real \\(\\mu\\) is 216 Hz. This is because a distribution centered at 216 would be extremely unlikely to generate a sample mean of 220 Hz. Using this aproach, we can rule out implausible values of \\(\\mu\\) based on the characteristics of our data. At this point we can offer conventional responses to the research questions posed at the start of the Chapter: Q1) What is the average f0 of the whole population likely to be? A1) The most likely value for the population mean is our sample mean, 220.4 Hz. Q2) Can we set bounds on likely mean f0 values based on the data we collected? A2) Yes, there is a 95% probability that the population mean is between 218.5 222.3 Hz, given our data and model structure. Traditional approaches to statistics (sometimes generally referred to as frequentist) estimate parameters by trying to find the most likely values for parameters (i.e., maximum likelihood estimation). They do this by referring to the theoretical likelihood functions such as what we plotted above. Although this works very well for simple data, it is difficult if not impossible for some of the more complicated datasets that often arise for even the simplest research questions in linguistics. 1.4 Bayesian models In this class we are going to learn about multilevel Bayesian models. These models have many advantages over traditional approaches. They provide researchers with more information, are more robust, and at worst, they are as good as traditional models. I may sound biased, but the main reason for all of these advantages is that traditional models were developed over 100 years ago. On the other hand, mathematical and technological advances have only made Bayesian multilevel models possible in the last 10+ years. It is only reasonable that the newer approaches should offer some advantages over methods developed before calculators existed. Here, I am going to address what is meant by two aspects of the term Bayesian multilevel models: Bayesian and models. 1.4.1 What are regression models? Before beginning this section I just want to say that its ok if a lot of this section doesnt makes sense right now. It will make more sense once you start to actually build models and it becomes less hypothetical and more practical. I will use the terms and concepts described here in later chapters, but I will re-explain it each time. If you think that a model in a later section is not explained in as much detail as you would like, look at this section again! I have been referring somewhat obliquely to models without really explaining what I mean by this. Its difficult to offer a precise definition because the term is so broad, but regression modeling can be thought of as trying to understand variation the mean parameter (\\(\\mu\\)) of a normal distributions. Actually, you can use many other probability distributions, but for now we will focus on models based on the normal distribution. Basically it goes like this: you have a variable you are interested in, \\(y\\), which is is a vector containing N observations. We can refer to any one of these observations like this \\(y_{[i]}\\) for the \\(i^{th}\\) observation. In our case this is a vector of 576 f0 values (f0[1:576]). Although its not necessary, I am going to put the index variables associated with trial number (\\(i\\)) in brackets like this \\(y_{[i]}\\). This is just to make it easier to identify, and to highlight the similarity to vectors (e.g., f0[i]). you assume that your data is well described by a normal probability distribution. This is a mathematical function (\\(\\mathcal{N}(\\mu,\\sigma)\\)) that described what is and is not probable based on two parameters. the mean of this distribution is either fixed, or varies in a logical manner. the variation in the mean of this distribution can be understood using some other variables. We can write this model more formally like this: \\[ y_{[i]} \\sim \\mathcal{N}(\\mu,\\sigma) \\tag{1.1} \\] This says that we expect that the tokens of the variable we are interested in is distributed according to (\\(\\sim\\)) a normal distribution with those parameters. Notice hat \\(y\\) gets a subscript while \\(\\mu\\) and \\(\\sigma\\) do not. This is because for right now, those parameters are fixed for all observations, while the value of \\(y\\) changes for each observation based on the \\(i\\) subscript. For example, below I set \\(i=2\\) and use this index variable to show the second element of the data vector, i.e. \\(f0_{[i=2]}=214\\). head (f0) ## [1] 225 214 192 233 223 223 i = 2 f0[2] ## [1] 214 Equation (1.1) just formalizes the fact that we think the shape of our data will be like that of a normal distribution with a mean equal to \\(\\mu\\) and a standard deviation equal to \\(\\sigma\\). When you see this, \\(\\mathcal{N}(\\mu,\\sigma)\\), just picture in your mind the shape of a normal distribution, like if you see this \\(y=x^2\\) you may imagine a parabola. \\(\\mathcal{N}(\\mu,\\sigma)\\) Really just represents that shape of the normal distribution, and the associated expectation about more and less probable outcomes. The above relationship can also be presented like this: \\[ y_{[i]} = \\mu + \\mathcal{N}(0,\\sigma) \\tag{1.2} \\] Notice that we got rid of the \\(\\sim\\) symbol, moved \\(\\mu\\) out of the distribution function (\\(\\mathcal{N}()\\)), and that the mean of the distribution function is now 0. This breaks up our variable into two components: A systematic component, \\(\\mu\\), that contributes the same value to all instances of a variable. A random component, \\(\\mathcal{N}(0,\\sigma)\\), that causes unpredictable variation around \\(\\mu\\). In terms of our data, I might express the distribution in either of the following ways: \\[ f0_{[i]} = \\mathcal{N}(220.4,23.2) \\tag{1.3} \\] \\[ f0_{[i]} = 220.4 + \\mathcal{N}(0,23.2) \\tag{1.4} \\] The distribution on the left below is the original data, centered at 220.4 Hz and with a standard deviation of 23.2 Hz. On the right, the mean has been subtracted from each value. The sample now represents random variation around the sample mean, variation that our model cant explain. From the perspective of our model, this is noise, or error. This doesnt mean that its unexplainable, it only means that weve structured our model in a way that doesnt let us explain it. par (mfrow =c(1,2), mar = c(4,4,1,1)) hist (f0, main=&quot;&quot;, freq=FALSE, col = yellow) hist (f0 - mean (f0), main=&quot;&quot;, freq=FALSE, col = coral) Figure 1.11: (left) Histogram of data. (right) Histogram of centered data, basically the error distribution. In regression models, we can decompose systematic variation in \\(\\mu\\) into component parts, based on some predictor variables, \\(\\mathrm{x}\\). These predictor variables co-vary (vary with) our \\(y\\) variable, and we think help explain the variation in \\(y\\). Below, I am saying that I think \\(\\mu\\) is actually equal to some combination of \\(\\mathrm{x}_{1}\\) \\(\\mathrm{x}_{2}\\) and \\(\\mathrm{x}_{3}\\). For example, I could think that f0 is affected by the speaker age (\\(\\mathrm{x}_{1}\\)) and gender of the speaker (\\(\\mathrm{x}_{2}\\)), and vowel category (\\(\\mathrm{x}_{3}\\)) of the production. \\[ \\mu = \\mathrm{x}_{1} + \\mathrm{x}_{2} + \\mathrm{x}_{3} \\tag{1.5} \\] The values of the predictor variables will vary from trial to trial, and are not fixed. Often the whole point of running an experiment is to predict differences in observations based on differing predictor values! So obviously, \\(\\mu\\) will need to vary from trial to trial. That means that the equation above should actually include \\(i\\) subscripts indicating that the equation refers to the value of the predictors and expected mean, for that trial rather than overall. \\[ \\mu_{[i]} = \\mathrm{x}_{1[i]} + \\mathrm{x}_{2[i]} + \\mathrm{x}_{3[i]} \\tag{1.5} \\] Actually, the mean is very unlikely to just be an equal combination of the predictors, so that a weighting of the predictors will be necessary. We can use the symbol \\(\\alpha\\) for these weights. For example, maybe \\(\\mathrm{x}_{1}\\) is twice as important as the other two predictors and so \\(\\alpha_1\\) is 2, while \\(\\alpha_2\\) and \\(\\alpha_1\\) are 1. \\[ \\mu_{[i]} = \\alpha_1*\\mathrm{x}_{1[i]} + \\alpha_2*\\mathrm{x}_{2[i]} + \\alpha_3*\\mathrm{x}_{3[i]} \\tag{1.6} \\] Note that the weight terms (\\(\\alpha\\)) do not get an \\(i\\) subscript. This is because they do not change from trial to trial. The values of the predictors change from trial to trial, but the way that these are combined does not, they are a stable property of the model. Decomposition of \\(\\mu\\) into sub-components makes our model something more like: \\[ y_{[i]} = \\mu_{[i]} + \\mathcal{N}(0,\\sigma) \\tag{1.7} \\] \\[ y_{[i]} = (\\alpha_1*\\mathrm{x}_{1[i]} + \\alpha_2*\\mathrm{x}_{2[i]} + \\alpha_3*\\mathrm{x}_{3[i]} ) + \\mathcal{N}(0,\\sigma) \\tag{1.8} \\] Often, \\(\\varepsilon\\) is used to represent the random component, as in: \\[ y_{[i]} = \\alpha_1*\\mathrm{x}_{1[i]} + \\alpha_2*\\mathrm{x}_{2[i]} + \\alpha_3*\\mathrm{x}_{3[i]}+ \\varepsilon_{[i]} \\tag{1.9} \\] Notice that the error term does get a, \\(i\\) subscript, as in \\(\\varepsilon_{[i]}\\). That is because the exact value of the error changes from trial to trial, even of the general characteristics of the error (i.e., \\(\\mathcal{N}(0,\\sigma)\\)) do not. When expressed in this manner, this is now a regression equation or a regression model. Fitting a regression model basically consists of trying to guess the most likely values of \\(\\alpha_1\\), \\(\\alpha_2\\), and \\(\\alpha_3\\) given our data. Notice that the above formulation means that regression models do not require that our data be normally distributed, but only that the random variation in our data (\\(\\varepsilon\\)) be normally distributed. For example, in the left panel below I plot the distribution of f0 from among the entire Hillenbrand et al. data, including boys, girls, men and women. The data is not normally distributed, however, we can still use a regression based on normally-distributed data to model this as long as we expect that: There is systematic variation in the \\(\\mu_{[i]}\\) of f0 across different groups, speakers, conditions, etc. The random variation around these predicted values of \\(\\mu_{[i]}\\) more or less follows a normal distribution. In the right panel I plot the individual densities for different speaker classes. We see that although the data is not normally distributed, the within-group variation is. This suggests a regression model is appropriate for this data. par (mfrow =c(1,2), mar = c(4,4,1,1)) hist (h95$f0, main=&quot;&quot;, freq=FALSE, xlim = c(80,320), col = yellow) plot (density (h95$f0[h95$group==&#39;b&#39;]),col=2,lwd=4, main=&#39;&#39;, xlim = c(80,320),ylim=c(0,0.025), xlab = &#39;f0&#39;) lines (density (h95$f0[h95$group==&#39;g&#39;]),col=3,lwd=3) lines (density (h95$f0[h95$group==&#39;m&#39;]),col=4,lwd=3) lines (density (h95$f0[h95$group==&#39;w&#39;]),col=5,lwd=3) Figure 1.12: (left) Distribution of f0 across all speakers. (right) Densities of distributions of f0 for different speaker classes: boys (red), girls (green), men (blue) and women (cyan). 1.4.2 Whats Bayesian about these models? The major difference between Bayesian and traditional models is that Bayesian models rely on posterior distributions rather than likelihood functions. I am going to define some terms: prior probability distribution: the distribution of possible/believable parameter values prior to the current experiment. This a priori expectation can come from world knowledge, previous experiments, or some combination of the two. the likelihood: this is the distribution of possible/credible parameter values given the current data and probability model, and nothing else. posterior probability distribution: the distribution of possible/believable parameter values you have after your current experiment. You get this by combining the prior distribution and the likelihood. Traditional models make inferences based on the likelihood functions of parameters. Bayesian models make inferences based on the posterior distributions of parameters. To do this, they have to have to actually combine information about likelihood with information about the prior probabilities of parameters. 1.5 Posterior distributions The combination of probability distributions is straightforward conceptually: you just multiply the values of the distributions at each x-axis location, and the result is the new curve. In the figure below (code at end of chapter), I combine several sets of probability distributions, showing the effects of variations in priors and likelihoods. In each plot, I scale the posterior density so that it is the same height as the likelihood. This is only to make the figures interpretable but does not affect any of the points I make below. In the top-left panel, I plot the likelihood function for \\(\\mu\\) given a sample of size 5 with a mean of 220 Hz. I show what happens when I combine this with a relatively weak but very different prior: the standard deviation is the same as our f0 data, however the mean is much higher (250 Hz). With only 5 data points the likelihood already dominates the posterior, though the prior distribution is exerting a pull. In the top-right panel, the posterior is almost identical to the likelihood. The likelihood represents a sample of size 100, which is actually a tiny sample in experimental linguistics work where you may have 200+ samples from each of 50+ subjects. As you might imagine, when the sample size is that large the prior exerts almost no influence on results. In the bottom-left panel we see a situation where the prior dominates the estimate. Consider a situation where we actually have really good reasons to think that the mean is 250 Hz. If we really know this, why would we accept and estimate of 220 Hz based on only 5 samples? In this case, the posterior distribution is basically saying: your estimate is great, but come back when you have more evidence and I might believe you. In the bottom-right panel we see a situation where the likelihood and the prior are equal. In this case the posterior represents compromise between new and prior knowledge. Figure 1.13: Demonstration of the effect of different types of priors and likelihoods on posterior distributions. The use of prior probabilities is often said to make Bayesian models subjective but its not really a big deal. First, every model involves arbitrary decisions which can substantially affect our results. Second, a researcher will always use common sense to interpret a model. For example, before collecting my sample I can say that I expect my female average f0 to be 200 Hz or so, but think its reasonable to expect anything from 100 to 300 Hz. Based on everything we know about human speech, even these bounds are too wide, and anything outside would suggest something is very wrong. So, even if I did not use a prior, I would use my expectations to screen my results, and be very wary of anything that did not meet my expectations. A Bayesian model simply requires that you build your expectation into your model. It formalizes it, makes it definable and replicable. Also, being objective does not quite make sense in many cases. Is it really being objective to ignore common sense and act as if a mean f0 of 250 is exactly as likely a priori as one of 20,000 Hz? Because not using a prior is equivalent to using a flat prior and acting like almost any value is equally likely a priori, when this is hardly the case. 1.5.1 Sampling from the posterior We want to understand the posterior distribution of parameters. How do we get this information? It is difficult to get this analytically, that is, using exact methods and solving a bunch of equations. Many traditional methods can actually be solved in this way, and that is a big part of their popularity. Understanding the characteristics of posterior probabilities is not possible analytically for many Bayesian models. As a result, these questions are answered numerically, basically by using a bunch of guesses. To understand the properties of posterior distributions, we use sampling software that knows how to investigate these distributions. The way these samplers work is you specify a set of data and some relationships you think are represented in your data (i.e., a model). The sampler then walks around the parameter space, which is the range of possible values a parameter (or set of parameters) can take. For example, for a single parameter the parameter space is a line (like the x axis in the plots above) along which the parameter varies. The sampler then does some variant of the following algorithm: Pick a random value for the parameter (i.e., \\(\\mu_{tmp}\\) = 221 Hz). Calculate the posterior probability for the current estimate of \\(\\mu_{tmp}\\). If the posterior estimate meets some criteria (e.g., it is better than the last one, it is not too low, etc.), then the value of \\(\\mu_{tmp}\\) is recorded, and becomes \\(\\mu_{estimate}\\). If not it is just discarded. Go back to step 1. As incredible as it may seem, under a very reasonable set of conditions if you do the above enough times, the distribution of \\(\\mu_{estimate}\\) that results from the above process will converge on the posterior distribution of \\(\\mu\\) given your data and model structure (including prior probabilities). Below I have made a small example of this process. I use the Metropolis-Hastings algorithm, which is an algorithm to sample from probability distributions. The small example below assumes the standard deviation of the population is known, and just tries to investigate the posterior distribution of \\(\\mu\\). It uses a very broad prior distribution (\\(\\mu = 0\\), \\(\\sigma = 5000\\)) so that it will have a very weak effect on the outcomes. # the function below takes a random sample, an initial mean estimate and a fixed # standard deviation. It then takes a certain amount of samples from the # posterior distribution of the parameter, assuming a broad prior centered at 0 sampler_example = function (sample, mu_estimate = 0, stdev = 1, nsamples = 1000){ # initial posterior calculation. This is the sum of the log likelihood and # the logarithm of the prior probability. prior = log (dnorm (mu_estimate[1],0, 500)) loglik = sum (dnorm (sample, mu_estimate[1],stdev,log=TRUE)) old_posterior = loglik + prior for (i in 2:nsamples){ accept = FALSE ## this loop will keep proposing new steps until one gets accepted. while (!accept){ ## (step 1 above) ## draw new proposal by randomly changing the previous mu_estimate mu_tmp = mu_estimate[i-1] + rnorm (1, 0, .3) ## (step 2 above) ## find prior probability for new mu_tmp proposal prior = log (dnorm (mu_tmp,0, 500)) ## find log likelihood for new mu_tmp proposal loglik = sum (dnorm (sample, mu_tmp,stdev,log=TRUE)) ## calculate the new posterior probability new_posterior = prior + loglik ## (step 3 above) ## if better accept always. If worse, accept sometimes if ( ( new_posterior - old_posterior ) &gt;= log ( runif (1,0,1) ) ){ mu_estimate[i] = mu_tmp ## if you accept, the new estimate becomes the current estimate old_posterior = new_posterior accept = TRUE } } } return (mu_estimate) } In the plots below (code at end of chapter), I show this algorithm at work. In the top row, a random sample with a mean of -50 is used. You can see that the sampler starts at 0 but quickly finds the sample mean (left column). In the middle, I show the distribution of the samples on the left, minus the burn-in phase (arbitrarily chosen by me). On the right, I compare our samples (blue) to the theoretical posterior distribution for the mean given the data and prior (red). I toss out the samples during the burn in phase, as there are used up in trying to find the correct location in the parameter space. In the bottom row, I use this algorithm on our f0 data! This is a Bayesian analysis since it combines information about parameter likelihood and prior probabilities. We can also see that even this simple approach yields a good correspondence to the theoretical posterior distribution of the parameter, and results in broadly the same conclusions we have arrived at by other means. Figure 1.14: Demonstration of parameter estimation using a random walk, yielding a good approximation of analytically-derived values. The results clearly coincide, but arent perfect. But this sampler isnt very sophisticated! The samplers we will be using in this class do provide an excellent match to the posterior distribution. As a result, we can inspect the distribution of collected \\(\\mu_{estimate}\\) to understand the posterior of our parameter. We can use these distributions in the same way that we used the theoretical likelihood functions above, by using them to make statements about likely parameter values and ranges of values. 1.6 Plot Code Figure 1.8 x = f0[1:10] ## tiny sub sample for example par (mfrow = c(1,2), mar = c(4,4,1,1)) plot (x,rep(0,10), ylim = c(0,.08), pch=16,col=4, xlim = c(150,300), ylab=&#39;Density&#39;, main = &#39;Likelihood of mean&#39;,xlab=&#39;f0&#39;,cex.main=.8) ## here the likelihood sd is divided by the sample size curve (dnorm (x, mean(x), 21.9 / sqrt (10)), from = c(150,300), add=TRUE, col = 2, lwd = 2) abline (v = c(175, 200, 225), lwd=2,lty=3) plot (x,rep(0,10), ylim = c(0,.022), pch=16,col=4,xlim = c(150,300),cex.main=.8, ylab=&#39;Density&#39;, main = &quot;mean = 175&quot;,xlab=&#39;f0&#39;) ## now it is centered at mean = 175 Hz curve (dnorm (x, 175, 21.9), from = c(150,300), add=TRUE, col = 2, lwd = 2) segments (x,rep(0,10),x,dnorm (x, 175, sd (x))) Figure 1.9 x = f0[1:10] ## tiny sub sample for exampls par (mfrow = c(1,2), mar = c(4,4,1,1)) plot (x,rep(0,10), ylim = c(0,.022), pch=16,col=4,xlim= c(150,300),cex.main=.8, ylab=&#39;Density&#39;, main = &quot;mean = 200&quot;,xlab=&#39;f0&#39;) ## distribution centered ar 200 curve (dnorm (x, 200, 21.9), from = c(150,300), add=TRUE, col = 2, lwd = 2) segments (x,rep(0,10),x,dnorm (x, 200, sd (x))) plot (x,rep(0,10), ylim = c(0,.022), pch=16,col=4,xlim =c(150,300),cex.main=.8, ylab=&#39;Density&#39;, main = &quot;mean = 225&quot;,xlab=&#39;f0&#39;) ## and now at 220 curve (dnorm (x, 225, 21.9), from = c(150,300), add=TRUE, col = 2, lwd = 2) segments (x,rep(0,10),x,dnorm (x, 225, sd (x))) Figure 1.13 x = seq (150, 301, .1) par (mfrow = c(4,1)) par(mar =c(4,4,.1,.1), oma = c(1,0,1,0)) ## likelihood is a bit stronger than the prior likelihood = dnorm (x, mean (f0), sd (f0) / sqrt ( 5 ) ) prior = dnorm (x, 250, sd (f0)) ; posterior = likelihood * prior plot (x, likelihood / max (likelihood), type = &#39;l&#39;, ylab=&#39;Density&#39;,lwd=2, xlim = c(175, 300), ylim = c(0,1.1)) lines (x, prior / max (prior),lwd=2,col=2) lines (x, posterior / max (posterior),lwd=2,col=4) legend (178, 1, legend = c(&#39;Prior&#39;,&#39;Likelihood&#39;,&#39;Posterior&#39;), col = c(2,1,4), lwd = 2, bty = &#39;n&#39;) ## likelihood is much stronger than the prior likelihood = dnorm (x, mean (f0), sd (f0) / sqrt ( 100 ) ) prior = dnorm (x, 250, sd (f0)) ; posterior = likelihood * prior plot (x, likelihood / max (likelihood), type = &#39;l&#39;, ylab=&#39;Density&#39;,lwd=2, xlim = c(175, 300), ylim = c(0,1.1)) lines (x, prior / max (prior),lwd=2,col=2) lines (x, posterior / max (posterior),lwd=2,col=4) ## prior overwhelms the likelihood likelihood = dnorm (x, mean (f0), sd (f0) / sqrt ( 5 ) ) prior = dnorm (x, 250, sd (f0)/10) ; posterior = likelihood * prior plot (x, likelihood / max (likelihood), type = &#39;l&#39;, ylab=&#39;Density&#39;,lwd=2, xlim = c(175, 300), ylim = c(0,1.1)) lines (x, prior / max (prior),lwd=2,col=2) lines (x, posterior / max (posterior),lwd=2,col=4) ## prior and likelihood have about equal influence likelihood = dnorm (x, mean (f0), sd (f0) / sqrt ( 100 ) ) prior = dnorm (x, 250, sd (f0)/10) ; posterior = likelihood * prior plot (x, likelihood / max (likelihood), type = &#39;l&#39;, ylab=&#39;Density&#39;,lwd=2, xlim = c(175, 300), ylim = c(0,1.1)) lines (x, prior / max (prior),lwd=2,col=2) lines (x, posterior / max (posterior),lwd=2,col=4) Figure 1.14 set.seed(1) ## collect samples from the likelihood of the mean for our f0 sample samples_1 = sampler_example (f0,200,sd(f0), 5000) ## do a second one to show its not a fluke data_2 = rnorm (100,-50,1) ## collect samples from the likelihood of the mean samples_2 = sampler_example (data_2,0,sd(data_2), 5000) # the left column shows the path the sampler took. the middle column shows the # distribution of these samples, minus the burn in phase. the right column shows # a comparison of theoretical and observed posterior distributions par (mfrow = c(2,3), mar = c(4,4,1,1)) plot (samples_2) hist (samples_2[-(1:250)], freq = FALSE, ylim = c(0,4.5), main=&quot;&quot;,col=skyblue) x = seq (-50.6, -49.75,.001) likelihood = dnorm (x, mean (data_2), sd(data_2) / sqrt (100) ) prior = dnorm (x, 0, 500 ) posterior = likelihood * prior posterior = posterior / max (posterior) plot (x, posterior, lwd = 4, col = coral, type = &#39;l&#39;) density_2 = density (samples_2[-(1:250)]) density_2$y = density_2$y / max (density_2$y) lines (density_2, lwd = 3, col = skyblue) plot (samples_1) hist (samples_1[-(1:250)], freq = FALSE, ylim = c(0,0.5), main=&quot;&quot;,col=skyblue) x = seq (215,225,.001) likelihood = dnorm (x, mean (f0), sd(f0) / sqrt ( length(f0) ) ) prior = dnorm (x, 0, 500 ) posterior = likelihood * prior posterior = posterior / max (posterior) plot (x, posterior, lwd = 4, col = coral, type = &#39;l&#39;) density_1 = density (samples_1[-(1:250)]) density_1$y = density_1$y / max (density_1$y) lines (density_1, lwd = 3, col = skyblue) "],["inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html", "Chapter 2 Inspecting a single group of observations using a Bayesian multilevel model 2.1 Data and research questions 2.2 Estimating a single mean with the brms package 2.3 Repeated measures data 2.4 Estimating a multilevel model with brms 2.5 Checking model convergence 2.6 Specifying prior probabilities 2.7 Answering our research questions 2.8 Plot Code", " Chapter 2 Inspecting a single group of observations using a Bayesian multilevel model In this chapter I am going to discuss how to use the brms package to estimate a population mean given a sample of data. For these models the data: can come from one speaker/subject or many speakers/subjects. each speaker/subject can contribute multiple data points. does not need to be balanced or complete across all subjects. The traditional designs equivalent to these models are: one-sample t-test, and repeated-measures one-way ANOVA with only two groups. However, these models wont be discussed in the chapter below. 2.1 Data and research questions We are going to keep analyzing the female f0 data from the Hillenbrand et al. (1995) dataset, discussed in chapter 1. url1 = &quot;https://raw.githubusercontent.com/santiagobarreda&quot; url2 = &quot;/stats-class/master/data/h95_vowel_data.csv&quot; h95 = read.csv (url(paste0 (url1, url2))) # select women only w = h95[h95$group == &#39;w&#39;,] # this is unique subject numbers across all groups w$speaker = factor (w$speaker) # select only the vector of interest f0 = w$f0 We are going to try to address the same questions we talked about last week: What is the average f0 of the whole population likely to be? Can we set bounds on likely mean f0 values based on the data we collected? However, this time we are going to do this with a Bayesian multilevel model. 2.2 Estimating a single mean with the brms package The brms Bayesian regression models package in R lets you fit Bayesian models using the STAN probabilistic programming language using R. The package is really amazing and makes Bayesian multilevel modeling easy and accessible for anyone. It also includes a lot of helper functions that make working with these models very convenient. brms should be installed in R so that the models described below will work. Make sure you have the latest version of R (and Rstudio) and the latest version of the `brms package installed. Sometimes using older versions can cause R to crash when fitting models. 2.2.1 The model Model structures are expressed in R using a very specific syntax. Think of writing a model formula as writing a language within R. The good thing about learning to write models is then you can use this knowledge to describe your models in your work, and to interpret other peoples models. The model formulas resemble regression equations to some extent, but there are some differences. Remember that regression models can be thought of in either of two two ways: \\[\\begin{equation} \\begin{split} \\\\ y_{[i]} = \\mu_{[i]} + \\varepsilon_{[i]} \\\\ \\\\ y_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma) \\\\ \\\\ \\end{split} \\tag{2.1} \\end{equation}\\] The top line says that your observed variable for any given trial \\(y_{[i]}\\) is the sum of some of some average expected value for that trial, (\\(\\mu_{[i]}\\)) and some specific random error for that trial (\\(\\mu_{[i]}\\)). The random error is expected to be normally distributed with a mean of 0 and some unknown standard deviation (\\(\\varepsilon_{[i]} \\sim \\mathcal{N}(0,\\sigma)\\)). The second line presents the \\(y\\) variable as being a normally-distributed variable with a trial-specific mean of \\(\\mu_i\\), and a fixed standard deviation \\(\\sigma_{error}\\) In general, in regression models we would really like to understand orderly variation in \\(\\mu_{[i]}\\) from trial to trial by breaking it up into predictors (\\(\\mathrm{x}_{1}, \\mathrm{x}_{2},...\\)) that are combined using some weights (\\(\\alpha_1, \\alpha_2,...\\)). \\[ \\mu_{[i]} = \\alpha_1*\\mathrm{x}_{1{[i]}} + \\alpha_2*\\mathrm{x}_{2i}+...+\\alpha_j*\\mathrm{x}_{j{[i]}} \\tag{2.2} \\] Fitting a regression model consists of trying to guess the values of the weighing factors (\\(\\alpha\\)), called the model coefficients. When we are only trying to estimate a single average, we dont have any predictors to explain variation in \\(\\mu_{[i]}\\). In fact, our model structure suggests we expect no variation in \\(\\mu_{[i]}\\) from trial to trial!. Mathematically, we cant just say we have no predictor since everything needs to be represented by a number. As a result, we use a single predictor \\(\\mathrm{x}\\) with a value of 1 so that our regression equation is: \\[ \\mu_{[i]} = \\alpha_1*1 \\tag{2.3} \\] Now our model is trying to guess the value of a single coefficient (\\(\\alpha_1\\)), and we expect this coefficient to be equal to \\(\\mu_{[i]}\\) since it is being multiplied by a predictor with a constant value of 1. This kind of model is called an Intercept only model. Regression models are really about representing differences, differences between groups and across conditions. When you are encoding differences, you need an overall reference point. For example, saying that something is 5 miles north is only interpretable given some reference point. The reference point used by your model is called your Intercept. Basically, our model consists only of a single reference point, and the \\(\\alpha_1\\) parameter reflects its value (as shown in Equation (2.3)). As a result, the \\(\\alpha_1\\) coefficient is called the intercept in our model. When a coefficient is just being multiplied by a fake predictor that just equals 1, we can omit it from the regression model (but its still secretly there). Based on the above, our f0 model can be thought of like this: \\[\\begin{equation} \\begin{split} \\\\ f0_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma) \\\\ \\\\ \\mu_{[i]} = Intercept \\\\ \\\\ \\end{split} \\tag{2.4} \\end{equation}\\] Put in plain English, each line in the model says the following: We expect that f0 for a given observation \\(i\\) is normally distributed according to some trial-specific expected value and some unknown (but fixed) standard deviation. The expected value for any given trial (\\(\\mu_{[i]}\\)) is equal to the intercept of the model for all trials. This means its fixed and we have the same expected value for all tokens! What the model also implicitly says that the error is drawn from a normal distribution with a mean of 0 and a standard deviation of \\(\\sigma\\). This distribution represents all deviations in f0 around the mean f0 for the sample (\\(\\mu_{[i]}\\)). In other words, the error for this model is expected to look like: \\[ \\varepsilon_{[i]} \\sim \\mathcal{N}(0,\\sigma) \\tag{2.5} \\] 2.2.2 The model formula Generally, model formulas in R have the form: y ~ predictor where all variables are represented by their names in your data. The variable we are interested in understanding (\\(y\\)) goes on the left hand side, and on our predictors go on the right hand side, separated by a \\(\\sim\\). Notice that the random term (\\(\\varepsilon\\)) is not included in the model formula. The formula above can be read as y is distributed according to some predictor, which really means we think there is systematic variation in our y variable that can be understood by considering its joint variation with our predictor variable(s). For intercept only models, the number 1 is included in the model formula to indicate that a single constant value is being estimated (as in (2.3)). As a result, our model formula will have the form f0 ~ 1. This model could be said out loud like we are trying to estimate the mean of f0 or we are predicting mean f0 given only an intercept. 2.2.3 Calling the brm function Below, I load the brms package, which contains the brm function. The brm function takes a model specification, data and some other information, and fits a model that estimates all the model parameters. Unless otherwise specified, brm assumes that the error component (\\(\\varepsilon\\)) of your model is normally distributed. The first argument in the function call is the model formula, and the second argument tells the function where to find the data. The other arguments tell the function to estimates a single set of samples (chains = 1) using a single processor on your CPU (cores = 1). These arguments will be discussed more later. # To ensure predictable results in examples, I will be using the same random # seed throughout, and resetting it before running any &#39;random&#39; process. set.seed (1) model = brms::brm (f0 ~ 1, data = w, chains = 1, cores = 1) ## Compiling Stan program... ## Start sampling ## ## SAMPLING FOR MODEL &#39;98dae0f1caaef07c210aac2156c73749&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.062 seconds (Warm-up) ## Chain 1: 0.042 seconds (Sampling) ## Chain 1: 0.104 seconds (Total) ## Chain 1: By default, brms takes 2000 samples, throwing out the first 1000 and returning the last 1000. The output above shows you that the sampler is working, and tells you about the progress as it works. This is the last time I will be actually fitting a model in the code chunks. I am going to be relying on pre-fit models that you can load after downloading from the book GitHub. Models can be found in the folder corresponding to each chapter. ## load pre fit model model = readRDS (&#39;2_model.RDS&#39;) 2.2.4 Interpreting the model print statement We can evaluate the model name to show the default brms model print statement: model ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ 1 ## Data: w (Number of observations: 576) ## Samples: 1 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 1000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 220.40 0.97 218.33 222.30 1.00 851 557 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 23.24 0.69 21.99 24.61 1.00 653 550 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Typing the model name into the console and hitting enter prints the information seen above. The first part just tells you technical details that we dont have to worry about for now (though some are obvious). Family: gaussian Links: mu = identity; sigma = identity Formula: f0 ~ 1 Data: w (Number of observations: 576) Samples: 1 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup samples = 1000 Next we see estimated effects for out predictors, in this case only an intercept. This is a population level effect because is is shared by all observations in our sample, and not specific to any one observation. Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 220.40 0.97 218.33 222.30 1.00 851 557 The information above provides the mean (Estimate) and standard deviation (Est. Error) of the posterior distribution of \\(\\mu\\) (Intercept). The values of l-95% CI and u-95% CI represent the upper and lower 95% credible intervals for the posterior distribution of this parameter. The x% credible interval for a parameter is the smallest interval that encloses x% of the distribution. This parameter has an x% chance (0.x probability) of falling inside the x% credible interval. So, this means that there is a 95% probability that \\(\\mu\\) is between 218 and 222 Hz given our data and model structure. Notice that the parameter estimate and intervals almost exactly match the estimate and intervals we obtain by referencing the theoretical likelihood function (discussed in Chapter 1): ## sample mean mean (f0) ## [1] 220.401 ## theoretical quantiles for likelihood of mean qnorm (c(0.025, 0.975), mean (f0), sd (f0) / sqrt (length (f0) ) ) ## [1] 218.5047 222.2974 Our model also provides us an estimate of the error standard deviation(\\(\\sigma\\)), under Family Specific Parameters: sigma. This estimate closely matches our sample standard deviation (\\(s_{x}\\)) estimate of 23.2. In addition, we also get a 95% credible interval for this parameter (2.5% = 21.99, 97.5% = 24.61). Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma 23.24 0.69 21.99 24.61 1.00 653 550 This last section is just boilerplate and contains some basic reminders. This text will look the same after all models. Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). 2.2.5 Seeing the samples In Chapter 1 I discussed that samplers (like brm, or STAN) take samples of the posterior distributions of parameters given the data and model structure. Its helpful to see that this is quite literally what is happening, and that the print statement above just summarizes the information contained in the posterior samples. Below I get the posterior samples from the model. We have 1000 samples, as indicated in the model output above. The first column represents the model intercept, the middle column is the error, and the third column is a statistic related to model fit. ## get posterior samples from model samples = brms::posterior_samples (model) str (samples) ## &#39;data.frame&#39;: 1000 obs. of 3 variables: ## $ b_Intercept: num 221 221 220 222 220 ... ## $ sigma : num 22.1 21.9 23.2 21.9 22.9 ... ## $ lp__ : num -2635 -2635 -2634 -2637 -2634 ... ## inspect values head (samples) ## b_Intercept sigma lp__ ## 1 220.7860 22.06961 -2634.851 ## 2 220.5312 21.93719 -2635.160 ## 3 219.6445 23.22390 -2633.588 ## 4 222.3286 21.93414 -2637.392 ## 5 219.5626 22.87259 -2633.785 ## 6 221.2339 23.08532 -2633.672 I can plot the individual samples for the mean parameter on the left below. On the right I plot a histogram of the same samples, superimposed with the theoretical distribution of the likelihood. Although this is not the posterior, with so many data points we expect our posterior to be dominated by the likelihood so they should be similar. par (mfrow = c(1,2), mar = c(4,4,1,1)) plot (samples[,1], xlab = &#39;Sample number&#39;,ylab = &#39;f0&#39;,col=teal,pch=16) hist (samples[,1], freq = FALSE, breaks = 20,main=&#39;&#39;,xlab=&#39;f0&#39;,col=maroon) curve (dnorm (x, mean (f0), sd (f0) / sqrt (length (f0) )), add = TRUE, lwd = 4, col = yellow) Recall that our model output provides information about expected values for the mean parameter: Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 220.40 0.97 218.33 222.30 1.00 851 557 These simply correspond to the quantiles of the posterior samples! quantile (samples[,1], c(.025, .5, .975)) ## 2.5% 50% 97.5% ## 218.3288 220.3997 222.3001 There is no special status for these quantiles. We can check the values of other ones: quantile (samples[,1], c(.25, .75)) ## 25% 75% ## 219.8043 221.0296 Or even use the posterior distribution to find the probability that the mean parameter is over/under any arbitrary value: mean (samples[,1] &lt; 221) ## [1] 0.74 For example, given the calculation above we can say that there is a 0.74 probability (a 74% chance) that the mean f0 for female speakers in this population is under 221 Hz, given our data and model structure. We come to this conclusion by finding that 74% of the posterior samples of the parameter of interest are below 221 Hz. 2.3 Repeated measures data The model we fit above is a reasonable starting point, but it has many weaknesses. For example, it does not consider the fact that our data was produced by a fixed number of speakers, sampled from a population. It does not consider the variation in f0 inherent between speakers, treating this as noise. Importantly, our data consists of 12 productions from each speaker in our sample, meaning we have repeated measures data. Treating repeated measures data as if it were not repeated measures data can cause problems for our inferences. This is because it can give us a warped perspective of how much variability there really is in the sample. For example, if I told you I had 1,000,000 samples of speech from male speakers from Los Angeles, you may be confident that I can estimate the average f0 male speakers from Los Angeles very accurately. But what if I told you that all these samples were from only three different people? You know instinctively that this makes my data less reliable. The reason repeated-measures data can cause problems is because the measurements are correlated: multiple measurements from the same person are obviously going to be related to each other. If you measure the height of a tall person today, they will still be tall tomorrow. Because of this general principle, although we have 12 productions from each of 48 female speakers, we do not actually have 576=48*12 totally independent observations in our data. This can be seen quite clearly below. The top panel shows the distribution of all our f0 measurements. The bottom panel shows speaker boxplots (one for each speakers data). If we were to push down on the bottom panel and collapse all our boxplots into a single distribution, we would end up with the boxplot in the top panel. These boxplots shows that each speaker has their own average f0, and that their productions tend to vary around their natural average. As a result, we might have closer to 46 observations (one average value per speaker) than 576. For example, the outliers around 150 Hz may seem like huge errors in the top plot. In the bottom plot we see that these productions all come from one speaker, and actually reflect her average f0. These are not errors but systematic between-speaker variation. par (mar = c(4,4,2,1)); layout (mat = c(1,2), heights = c(.3,.7)) boxplot (f0, main = &quot;Overall Boxplot&quot;, col=&quot;lavender&quot;, horizontal = TRUE, ylim = c(140,320)) boxplot (f0 ~ speaker, data = w, main = &quot;Speaker Boxplots&quot;, col=c(yellow,coral, deepgreen,teal), horizontal = TRUE, ylim = c(140,320)) abline (h = 220.4,lty=3,col=&#39;grey&#39;,lwd=2) 2.3.1 Multilevel models In linguistics, and many other similar fields, almost all of our data is repeated measures data. The methods most commonly-used by linguists (e.g., experiments, interviews, corpora,  etc.) yield many observations per person, and typically all involve data from multiple people/sources. As a result, the analysis of this data requires that models be able to account for within and between speaker variation in our data. Multilevel models address the correlated nature of repeated measures data by estimating multiple sources of variation simultaneously. Repeated-measures data leads to random variation in parameters that is indistinguishable from that of our data. To a large extent, whether something is a parameter or a data point depends somewhat on your perspective. For example, consider the figure below. The top left presents a histogram of all f0 measurements, while the top right presents a boxplot of the same. The bottom left presents the speaker boxplots (one per speaker), each of which resembles the overall boxplot in the top right. We can then zoom in on a single speakers productions (bottom right) and produce a histogram that suggests a normal distribution reminiscent in shape to the overall aggregated data (top left). If you are trying to estimate a speakers mean f0, then the individual productions might be data and the mean can be thought of as a parameter. If you were instead only interested in the population average, maybe now your subject mean is actually just a single data point, and the population mean is actually your parameter. par (mfrow = c(2,2), mar = c(4,4,3,1)) hist (w$f0, main = &quot;Histogram of all f0&quot;,xlim = c(140, 290), freq = FALSE, col=4) boxplot (w$f0, main = &quot;Boxplot of all f0&quot;,col=lavender) boxplot (f0 ~ speaker, data = w, main = &quot;Speaker Boxplots&quot;,col=deepgreen) abline (v = 16,lty=3) hist (w$f0[w$speaker == 107], main = &quot;Histogram of speaker 107&quot;, xlim = c(160, 260), freq = FALSE,col=yellow) A multilevel model is able to simultaneously model independent variation at multiple levels. For our f0 data, these are: The upper level: Between-speaker variation in mean f0. This can be thought of like variation in \\(\\mu_{speaker}\\). Speakers have an average f0 (\\(\\mu_{speaker}\\)) that they produce over time. However, speakers are chosen randomly from a larger population, and so any given speakers \\(\\mu_{speaker}\\) is unpredictable a priori. The lower level: Within-speaker variation, analogous to \\(\\varepsilon\\). When an individual speaker produces speech, their productions will vary around their average from token to token. Our model cannot explain this and so this is error As seen in the figure above, the variation at the lower and upper levels are analogous. Just like individual speakers will rarely have an average f0 exactly like the population average, individual speakers will rarely produce f0 values exactly at their speaker average. Importantly, variation at the two levels is independent and logically distinct: within-speaker variation can be small or large independently of whether between-speaker variation is large or small. Basically, each subjects productions form a little normal distribution around their average, and the mix of these little distributions results in the overall big distribution of data across all subjects. By using multilevel models, we can estimate the effects of multiple sources of variation at the same time. 2.4 Estimating a multilevel model with brms We are now going to fit the same model we fit above, but with a structure that reflects the repeated-measures nature of the data. 2.4.1 The model To specify a multilevel model, you need to write a slightly more complicated model formula. This explanation assumes that you have a dataframe or matrix where one column contains the variable you are interested and predicting (in this case f0), and another column contains a vector containing unique labels for each speaker or source of data (in this case a unique speaker label speaker). To indicate that your model contains an upper level where you have clusters of data coming from different individuals, you have to put another model inside your main model! Before, the model formula looked like this: f0 ~ 1 which meant predict f0 using only an intercept. Now the model formula will look like this: f0 ~ 1 + ( 1 | speaker) When you place a predictor in the formula in parenthesis and on the right-hand-side of a pipe, like this ( | predictor ), you tell brm that you expect data to be clustered according to each category represented in the grouping vector. In this case, we are telling brm that each unique speaker is a cluster of data. Whatever you put in the left-hand-side of the parentheses ( in here | predictor ) is the model for each subcluster! So what does this model formula mean: f0 ~ 1 + ( 1 | speaker)? It tells brm: predict f0 based on only an intercept, but also allow intercept values to vary separately for each speaker. Effectively, this model formula is telling brm to figure out all the information presented in the figures above. This regression model is now something like this: \\[\\begin{equation} \\begin{split} \\\\ f0_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma) \\\\ \\\\ \\mu_{[i]} = Intercept + \\alpha_{speaker_{[i]}} \\\\ \\\\ \\end{split} \\tag{2.6} \\end{equation}\\] In English, the model above says: we expect f0 to be normally distributed. The f0 value we expect for any given token is equal to some overall average (\\(Intercept\\)), and some value associated with each the individual speaker (\\(\\alpha_{speaker_{[i]}}\\)) who uttered the trial. We now have another term \\(\\alpha_{speaker}\\), in addition to the intercept. This coefficient is actually a set of coefficients since it has a different value for each speaker (its a vector). For each trial, the value of \\(\\alpha_{speaker}\\) that should be used will vary based on the value of the vector indicating the speaker for that trial (e.g., w[[\"speaker\"]][i] for some value of i). We will talk more coefficients like these later, we dont really need to worry about it for now. The value of \\(\\alpha_{speaker}\\) has a different value for each speaker because it will reflect variation in \\(\\mu_{speaker}\\), the average f0 value produced by each speaker. However, \\(\\mu_{speaker}\\) is a random variable since it reflects the random average f0 of each person drawn from the population. If \\(\\mu_{speaker}\\) behaves like a random variable, then the coefficients that reflect this value in our model (\\(\\alpha_{speaker}\\)) will behave in the same way. This means that actually our model has two random variables. The first one is the error term \\(\\varepsilon_{[i]} \\sim \\mathcal{N}(0,\\sigma_{error})\\), which has a mean of 0 and a standard deviation which we can refer to as \\(\\sigma_{error}\\). The second is the random terms that allows for speaker-specific adjustments to the intercept (\\(\\alpha_{speaker}\\)), that can also be thought of as random draw from a normal distribution. A careful consideration of the model in equation 2.4 suggests that the (\\(\\alpha_{speaker}\\)) coefficients cant actually be exactly equal to \\(\\mu_{speaker}\\), the average f0 for a speaker. If the overall mean (the intercept) is 220 Hz and a speakers average is 230, this would suggest a predicted average of 450 (\\(Intercept + \\mu_{speaker}\\)) for this speaker. Clearly that is not how the model should be working. Recall that regression models encode differences, rather than absolute values. Our model already represents the overall data average in the intercept parameter. Thus, the speaker-specific averages only need to contain information about differences to this reference value. As a result, the model parameters for mean f0 across all speakers will be centered at 0 (i.e., the average), and will tend to be normally distributed with a population-specific standard deviation. Since our model coefficients reflect speaker-specific deviations rather than the actual mean f0 of different speakers, people often use this symbol, \\(\\gamma\\), for them instead of \\(\\mu\\), where \\(\\gamma\\) reflects the difference between the speaker means and the mean for the population of speakers, \\(\\gamma_{speaker} = \\mu_{speaker} - \\mu_{population}\\). We can show the expected distribution of this variable below, where \\(\\sigma_{speakers}\\) is a population-specific standard deviation term. Note the similarity of this to the expected variation in our original data in Equation (2.4). \\[\\begin{equation} \\begin{split} \\\\ \\gamma_{speaker} \\sim \\mathcal{N}(0,\\sigma_{speakers}) \\\\ \\\\ \\gamma_{speaker} = \\alpha_{speaker} \\\\ \\\\ \\end{split} \\tag{2.7} \\end{equation}\\] Our overall model is now as shown below, made specific for the data we have, and using expected parameter names. \\[\\begin{equation} \\begin{split} \\\\ f0_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\\\ \\mu_{[i]} = Intercept + \\alpha_{speaker_{[i]}} \\\\ \\\\ \\alpha_{speaker} \\sim \\mathcal{N}(0,\\sigma_{speakers}) \\\\ \\\\ \\end{split} \\tag{2.8} \\end{equation}\\] Each line in the model says the following: observed f0 is expected to be normally distributed around a trial-specific mean, with some unknown but fixed standard deviation (\\(\\sigma_{error}\\)). the expected value for a given trial (\\(\\mu_{[i]}\\)) is equal to the model intercept, plus some speaker-specific deviation/difference from the intercept for the speaker that produced that trial (\\(\\alpha_{speaker_{[i]}}\\)). the speaker effects (\\(\\alpha_{speaker}\\)) are also drawn from a normal distribution with a mean of 0 and a standard deviation of \\(\\sigma_{speakers}\\). This distribution represents the random, but systematic, between-speaker variation in average productions that exists within any population of speakers. All of this information can be seen in the speaker boxplots below. The observed error around some unknown mean is what causes there to be a distribution of f0 values around each speakers mean. The model intercept (horizontal dotted line) represents the overall mean, and the variation in \\(\\alpha_{speaker}\\) is what causes the middle of each little box to differ from the mean. par (mfrow = c(1,1), mar = c(4,4,3,1)) boxplot (f0 ~ speaker, data = w, main = &quot;Speaker Boxplots&quot;,col=cols[3:6]) abline (h = 220.4,lty=3, lwd=3) There is a very important difference in how the initial and final models we fit view and partition the variation in our model. The initial model we fit viewed the variation in the model like this: \\[ \\sigma_{total} = \\sigma_{error} \\tag{2.9} \\] In other words, all variation was error. We dont know why values vary from the mean. Our multilevel model views the variation in our data like this: \\[ \\sigma_{total} = \\sigma_{speaker} + \\sigma_{error} \\tag{2.10} \\] It sees only some of the variation in data as error. In terms of the boxplot above, only the variation within a speakers box is error. The differences from box to box represent random (but systematic) between-speaker variation in f0. Basically, from the perspective of this multilevel model, the random variation in the data is not all noise/error. 2.4.2 Fitting the model We can fit a model with a formula that appropriately specifies the clustering we expect in our data. As a result, this model can estimate both between- and within-speaker variability. set.seed (1) multilevel_model = brm (f0 ~ (1|speaker), data = w, chains = 1, cores = 1) ## loand and inspect pre-fit model multilevel_model = readRDS (&quot;2_multilevel_model.RDS&quot;) multilevel_model ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ (1 | uspeaker) ## Data: w (Number of observations: 576) ## Samples: 1 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 1000 ## ## Group-Level Effects: ## ~uspeaker (Number of levels: 48) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 20.19 2.10 16.65 25.06 1.00 135 192 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 220.39 3.10 214.49 226.13 1.03 51 108 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 12.54 0.39 11.83 13.34 1.01 400 700 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This new model contains one new chunk its print statement: Group-Level Effects: ~speaker (Number of levels: 48) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Intercept) 20.19 2.10 16.65 25.06 1.00 135 192 This sections contains information about the standard deviation of between-speaker averages (\\(\\mu_{speaker}\\)) in the sample. We can see that the information provided by brms is quite similar to what we can estimate directly using our data. However, brms does this all for us, in addition to giving us a lot more information. ## find mean f0 for each speaker speaker_means = aggregate (f0 ~ speaker, data = w, FUN = mean) ## find the within speaker variance. This is the within-talker &#39;error&#39;. speaker_vars = aggregate (f0 ~ speaker, data = w, FUN = var) ## the mean of the speaker means corresponds to our overall mean estimate mean (speaker_means$f0) ## [1] 220.401 ## sd(Intercept) in the model reflects the amount of variation in talker ## intercepts. This is the between speaker variation in our model. See how it is ## similar to the sd of the actual speaker means. sd (speaker_means$f0) ## [1] 20.07397 ## sigma in the model reflects the amount of variation in talker intercepts. ## This is the between speaker variation in our model. sqrt (sd (speaker_vars$f0)) ## [1] 12.42719 The overall mean f0 in our data (220.4) corresponds quite well to our model estimate of 220.4. This reflects the central location of the overall distribution below (the horizontal line in the figure below). The standard deviation of the speaker means (Intercept = 20.1) is again very similar to our model estimate (sd(Intercept) = 20.1). This reflects the average distance from each speakers average, and the overall average. Finally, the average of the within speaker standard deviation in our data (12.4) corresponds closely to our models error estimate (sigma = 12.5). This reflects the average spread of each speakers data relative to their own mean, within their own little boxplot. par (mfrow = c(1,1), mar = c(4,4,2,1)) boxplot (f0 ~ speaker, data = w, main = &quot;Speaker Boxplots&quot;,col=c(yellow,coral, deepgreen,teal)) abline (h = 220.4, lwd=3,lty=3) 2.5 Checking model convergence Remember that our model parameter estimates consist of a set of samples from the posterior distribution of a parameter. If we dont take enough of these samples, our parameter estimates will be unreliable. For this reason, its important to look at the ESS values (the expected sample size), and the Rhat values provided by brm. ESS tells you about how many independent samples you have taken from the likelihood. Bulk ESS is how many samples the sampler took in the thick part of the density, and Tail ESS reflects how much time the sampler spent in the thin part, the tails. Rhat tells you about whether your chains have converged (more on this later). As noted above, values of Rhat near 1 are good, and values higher than around 1.1 are a bad sign. We havent really taken many samples here, so we cant be confident in our parameter estimates. Ideally we would like several hundred samples (at least) for mean estimates, and thousands to be confident in the 95% confidence intervals. To get more samples we can run the model longer, or we can use more chains. A chain is basically a separate set of samples for your parameter values. Just imagine you had estimated the model 4 times in a row and mixed your estimations. A model can be fit in parallel across several chains, and then the estimates can be merged across chains. When you do this across multiple cores, you can get N times as many samples when you use N cores. Since many computers these days have 4-8 (or more) cores, we can take advantage of parallel processing to fit models faster. Below, I refit the same model from above but run it on 4 chains, and on 4 cores at once. This doesnt take any longer but it does give us a higher ESS. Just make sure you leave a couple of cores free on your computer when you fit a model! set.seed (1) multilevel_multicore = brm (f0 ~ 1 + (1|speaker), data = w, chains = 4, cores = 4) ## load and inspect pre-fit model multilevel_multicore = readRDS (&#39;2_multilevel_multicore.RDS&#39;) multilevel_multicore ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ 1 + (1 | uspeaker) ## Data: w (Number of observations: 576) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~uspeaker (Number of levels: 48) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 20.22 2.19 16.54 25.19 1.02 345 615 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 220.47 2.91 214.67 226.14 1.02 228 543 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 12.54 0.39 11.81 13.34 1.00 3427 2943 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If we compare the ESS for this new model to the previous model, we see that using 4 chains has substantially increased our ESS, without taking up any more computing time. One final tweak that I will be using going forward is to thin samples. Notice that we have collected total post-warmup samples = 4000. This means our model has 4000 samples for every parameter in the model. However, we have only about 400 effective samples to show for it for some parameters of interest. This means that a lot of our samples are basically dead weight, taking up space and slowing down computations for no good reason. Sometimes consecutive samples can be too similar and so dont given you that much independent information. A way to fix this is to run longer chains and keep only every nth one. This lets your models be smaller while containing approximately the same information. To do this you have to set the iter, warmup and thin parameters. You will keep every sample after the warmup is done, up to the iter maximum. So if iter=3000 and warmup=1000 you will end up with 2000 samples. After this, you keep only one every thin samples. Basically, you will end up with \\((iter-warmup) / thin\\) samples per chain. Below, I ask for 11,000 sample per chain, 10,000 post warm-up. However, since I plan to keep only 1/10, I will have 1000 per core, so 4000 samples in total. However, despite having the same number of samples as the multilevel_multicore, the ESS for this model is much higher for important parameters such as the model intercept. set.seed (1) multilevel_thinned = brm (f0 ~ 1 + (1|speaker), data = w, chains = 4, cores = 4, warmup = 1000, iter = 11000, thin = 10) ## load and inspect pre-fit model multilevel_thinned = readRDS (&#39;2_multilevel_thinned.RDS&#39;) multilevel_thinned ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ 1 + (1 | uspeaker) ## Data: w (Number of observations: 576) ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~uspeaker (Number of levels: 48) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 20.06 2.23 16.24 24.98 1.00 2775 3392 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 220.40 2.95 214.57 226.19 1.00 1987 2625 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 12.54 0.38 11.81 13.31 1.00 3908 3891 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 2.6 Specifying prior probabilities In Chapter 1 we discussed that Bayesian models require that prior probabilities be specified for all parameters. You may have noticed that to this point I havent discussed priors at all. If you dont specify prior probabilities for your parameters, brm will use a flat prior for all parameters. When you do this, you are basically relying only on the likelihood for your analysis. You are also telling your model that, a priori, any value of average f0 is equally believable. Empirically, this is false. As a practical matter this can cause problems for samplers like brm (STAN actually). Basically, the sampler has a harder time figuring out the most likely values when you tell it to look anywhere from positive to negative infinity. Even a bit of guidance can help. brms makes it really easy to specify prior probabilities for specific parameters or whole groups of parameters. First we figure out the overall mean and the standard deviation of the data. mean(f0) ## [1] 220.401 sd(f0) ## [1] 23.22069 In the example below, I use this information to set reasonable bounds on the parameters in the model. I do this by class of parameter: Intercept: this is a unique class, only for intercepts. b: this is for all the non-intercept predictors. There are none in this model. sd: this is for all standard deviation parameters. In our example this is sd(Intercept) for speaker (\\(\\sigma_{speakers}\\)), and sigma (\\(\\sigma_{error}\\)). Both priors below use a t distribution, which is just like a normal distribution but it is more pointy, and has more density in the outer parts of the distribution. I use this because it has good properties, but you can use normal priors, or any other priors that you think work for your model. Rather than focusing on the mathematical properties of priors, the most important thing is that their shape reflect the distribution of credible parameter values a priori (before you conducted your experiment). The format for the priors looks like this student_t(nu, mean, sd), where nu is a parameter that determines how pointy the distribution is, and mean and sd are the same mean and standard deviation parameters from the normal distribution. The nu parameter ranges from 1 to infinity, and large numbers result in a more normal-like distribution. So, for the overall intercept (\\(\\mu_{overall}\\)) I am using a prior with the same mean as the data mean, and a standard deviation that is twice as large as the data standard deviation. For both the model standard deviation terms (\\(\\sigma_{error}, \\sigma_{speaker}\\)) I am using a t distribution centered at 0 (explained below), with a standard deviation twice as large as the data standard deviation. set.seed (1) multilevel_priors = brm (f0 ~ 1 + (1|speaker), data = w, chains = 4, cores = 4, warmup = 1000, iter = 11000, thin = 10, prior = c(set_prior(&quot;student_t(3, 220.4, 46.4)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 46.4)&quot;, class = &quot;sd&quot;))) ## load and inspect pr-fit model multilevel_priors = readRDS (&#39;2_multilevel_priors.RDS&#39;) multilevel_priors ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ 1 + (1 | uspeaker) ## Data: w (Number of observations: 576) ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~uspeaker (Number of levels: 48) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 20.19 2.24 16.29 25.06 1.00 3114 3663 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 220.42 2.97 214.44 226.38 1.00 1796 2841 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 12.54 0.39 11.81 13.34 1.00 3968 3972 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). In the left panel below (plot code at end of chapter) I compare the t distribution we used (blue) to the equivalent normal distribution (red). It is clear that the t distribution can tolerate more extreme values because of its fatter tails. As we will discuss later, using t distributions can make our models more robust to outliers. This is really important in linguistics where subjects/speakers sometimes do weird stuff! In the middle panel we compare this prior to the data, and see that the prior distribution is much broader (more vague) than the data distribution. The right panel compares the prior for the standard deviation parameters to the absolute value of the centered f0 data. This presentation shows how far each observation is from the mean f0 (at 220 Hz). Again, the prior distribution we have assigned for these parameters is much larger than the variation in the data. As a result, neither of these priors is going to have much of an effect on our parameter estimates. If we compare the output of this model to multilevel_thinned, we see that specifying a prior has has no noticeable effect on our results. This is because the prior matters less and less when you have a lot of data, and because we have set wide priors that are appropriate (but vague) given our data. Although the priors may not matter much for models as simple as these, they can be very important when working with more complex data, and are a necessary component of Bayesian modeling. 2.7 Answering our research questions Lets return again to the research questions I posed initially: What is the average f0 of the whole population likely to be? Can we set bounds on likely mean f0 values based on the data we collected? And we can compare the answers provided to this question by our initial and final models. model ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ 1 ## Data: w (Number of observations: 576) ## Samples: 1 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 1000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 220.40 0.97 218.33 222.30 1.00 851 557 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 23.24 0.69 21.99 24.61 1.00 653 550 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). multilevel_priors ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ 1 + (1 | uspeaker) ## Data: w (Number of observations: 576) ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~uspeaker (Number of levels: 48) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 20.19 2.24 16.29 25.06 1.00 3114 3663 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 220.42 2.97 214.44 226.38 1.00 1796 2841 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 12.54 0.39 11.81 13.34 1.00 3968 3972 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our initial model (model) and our final model (multilevel_priors) agree on what the average f0 is. However, they disagree on a credible interval for that parameter. Our initial model did not specify information about repeated measures. This causes our model to think that it has more independent observations than it does, and so it returns an overly-precise estimate. Another difference is that the final model has a much smaller sigma parameter (12.5 vs 23.2). This indicates that the error (\\(\\varepsilon\\)) is much smaller in the final model than in the initial model. Keep in mind that error is just what your model cant explain. Our final model explains much more and so there is less error. The reduced error is a direct result of the fact that the final model splits random variation up into between-speaker and within-speaker components, estimating the between-speaker variation (\\(sigma_{speaker}\\)) to be about 20 Hz. Usually, when I report parameters I provide the mean and standard deviations of the posterior distribution, in addition to the bounds of the 95% credible interval of the parameter. Based on the result of our final model, I think a thorough description of the general properties of our data might go something like: \"Based on our model the average f0 produced by adult females in Michigan is likely to be 220 Hz (s.d. = 2.97, 95% CI = 214.4, 226.4). However, consistent between-speaker variation averages about 20 Hz (s.d. = 2.24, 95% CI = 16.29, 25.06), meaning that we can expect the average f0 produced by individuals to deviate substantially from 220 Hz. Finally, the standard deviation of production error was about 12.5 Hz (s.d. = 0.39, 95% CI = 11.81, 13.34) indicating that the amount of random between speaker variation in production is about half the magnitude of the stable, between-speaker differences in f0. Im again including the speaker boxplots below because I think this image basically presents the same information as the paragraph above, but in visual form. In general, any data relationship or result can be presented in a figure, and the relationships presented in a figure can also be expressed as a mathematical model. When you are thinking about the relationships in your data, or that you expect in your data, its a good idea to think: what kind of picture could illustrate this relationship? Conversely, if you see a figure of your results that you feel really expresses something interesting about your data you should think, how can these relationships be represented in a model? par (mfrow = c(1,1), mar = c(4,4,2,1)) boxplot (f0 ~ speaker, data = w, main = &quot;Speaker Boxplots&quot;,col=c(yellow,coral, deepgreen,teal)) abline (h = 220.4, lwd=3,lty=3) 2.8 Plot Code Plot for comparison of prior distributions with f0 data # get t density x1 = seq (-4,4,.1) y1 = dt (x1, 3); y1 = y1 / max (y1) / 50 x2 = 220+(x1*46.4) y2 = dnorm (x2, 220, 46.4); y2 = y2 / max (y2) / 50 par (mfrow = c(1,3), mar = c(4,4,1,1)) ## plot t plot (x2, y1, type = &#39;l&#39;, lwd = 2, ylab = &#39;Density&#39;, xlab = &#39;f0&#39;, col = 4) ## compare to equivalent normal lines (x2, y2, lwd=2,col=2) ## plot t plot (x2, y1, type = &#39;l&#39;, lwd = 2, ylab = &#39;Density&#39;, xlab = &#39;f0&#39;, col = 4) ## compare to equivalent normal hist (f0, add = TRUE, freq = FALSE) ## plot prior for standard deviations x3 = x2 - mean (x2) y3 = dt (x1, 3); y3 = y3 / max (y3) / 20 plot (x3[x3&gt;0], y3[x3&gt;0], type = &#39;l&#39;, lwd = 2, ylab = &#39;Density&#39;, xlab = &#39;f0&#39;, col = 4) hist (abs (f0 - mean (f0)), add = TRUE, freq = FALSE) "],["comparing-two-groups-of-observations.html", "Chapter 3 Comparing two groups of observations 3.1 Data and research questions 3.2 Estimating the difference two means with brms 3.3 Contrasts 3.4 Refitting the model with sum coding 3.5 Random Effects 3.6 But what does it all mean?", " Chapter 3 Comparing two groups of observations In the previous chapter, I focused on investigating values from a single group, which is the simplest data you can deal with. In this chapter we are going to compare data from two groups to see how similar/different they really are. The models we will discuss are similar to two-sample t-tests (and some ANOVA designs) in the types of research questions they can help investigate. 3.1 Data and research questions We are going to be using the same Hillenbrand et al. f0 data we used in the other chapters, but this time we are going to compare the f0 measurements for adult females to those of girls between 10-12 years of age. Below I load in the Hillenbrand et al. data, and include a new variable that indicates whether the talker is an adult or a child. I also split the data up by gender. The variable speaker is a speaker number that is unique across all speaker groups, and group indicates speaker group from among b (boys), g (girls), m (men), and w (women). url1 = &quot;https://raw.githubusercontent.com/santiagobarreda&quot; url2 = &quot;/stats-class/master/data/h95_vowel_data.csv&quot; h95 = read.csv (url(paste0 (url1, url2))) ## make variable that indicates if the talker is an adult h95$adult = &quot;&quot; h95$adult[h95$group %in% c(&#39;w&#39;,&#39;m&#39;)] = &quot;adult&quot; h95$adult[h95$group %in% c(&#39;g&#39;,&#39;b&#39;)] = &quot;child&quot; ## split up data by into male and female groups, only some columns males = h95[h95$group %in% c(&#39;m&#39;,&#39;b&#39;),c(&#39;f0&#39;, &#39;adult&#39;, &#39;group&#39;, &#39;speaker&#39;)] females = h95[h95$group %in% c(&#39;w&#39;,&#39;g&#39;),c(&#39;f0&#39;, &#39;adult&#39;, &#39;group&#39;, &#39;speaker&#39;)] f0 = females[[&#39;f0&#39;]] # re-factor to remove excluded subjects males$speaker = factor (males$speaker) # re-factor to remove excluded subjects females$speaker = factor (females$speaker) One of the simplest questions a researcher can ask (from a modeling perspective) is: Are two groups of observations different or are they the same? For example in phonetics, researchers ask questions like, have these vowels merged in this dialect (are these two things different)? Does visual information speed up speech perception or not (are two sets of reaction times the same)? A good way to isolate a single difference is to create groups that differ primarily according to the characteristic you are interested in testing, but are otherwise the same on average. For the reaction-time example above, I might present the same listeners with very similar words in the same conditions, but half with and half without visual information. The difference in reaction times between the groups should reflect the advantage provided by visual information. The question would then be, are reaction times with visual information the same thing as reaction times without visual information? Are these two sets of observations samples from the same population? Or is the set of observations reaction times for speech recognized with visual information a different set of things than reaction times for speech recognized without visual information? In our data we have male and female talkers of the same dialect, who differ primarily in terms of age. Basically, these groups differ mostly in terms of adultness, and so we can use the characteristics of these groups to investigate the effect of adultness on average f0. Below I present boxplots of the effect of adultness on f0, first overall and then divided by speaker gender. par (mfrow = c(1,3)) boxplot (f0 ~ adult, data = h95, main = &quot;Overall&quot;, ylim = c(90,330), col = c(teal, yellow)) boxplot (f0 ~ adult, data = females, main = &quot;Females&quot;, ylim = c(90,330), col = c(deepgreen,skyblue)) boxplot (f0 ~ adult, data = males, main = &quot;Males&quot;, ylim = c(90,330), col = c(maroon,lightpink)) Clearly there is a difference in f0 between children and adults, but the difference also seems to be gender-dependent: there is more of a difference between men and boys than between women and girls. We are going to focus on the female data for now, and I leave the male data so that the reader can modify the analysis presented here to analyze that data. Below, the figure on the left highlights both between- and within-speaker variation in f0 by women (cyan) and girls (red). On the right, we see the overall distributions of f0 for women (cyan) and girls (red). Obviously the distributions seem a bit different, but they are also not that different. We can also clearly see that there is substantial overlap between individual girls and women (in the figure on the left). As a result, a cautious analysis of this data would recognize this overlap even if somehow differentess were found. We are going to fit a model that can help us quantify all of the variability shown in the figures below. colors = c(coral,teal)[ apply (table(females$speaker, females$adult),1,which.max) ] par (mfrow = c(1,2)); layout (mat = t(c(1,2)), widths = c(.7,.3)) boxplot (f0 ~ speaker, data=females, col = colors) ## I rotate the density figures so that you can see how these correspond to ## the boxplots on the left. tmp = density (females$f0[females$adult==&quot;child&quot;]) plot (tmp$y, tmp$x, lwd = 3, col = teal, ylab = &quot;f0&quot;,xlab=&quot;Density&quot;, ylim = c(140,320), xlim = c(0,0.025), type = &#39;l&#39;) tmp = density (females$f0[females$adult==&quot;adult&quot;]) lines (tmp$y, tmp$x, lwd = 3, col = coral) 3.2 Estimating the difference two means with brms In chapter 2 the model we fit had the simplest possible formula, just an intercept. Here, we need to extend this to include an actual predictor: a vector indicating adultness. Remember that formulas look like this variable ~ predictor. So, if we want to predict f0 based on whether the talker is an adult or not, our model is going to have a formula that looks basically like this f0 ~ adult + (1|speaker). You can think of this meaning something like We expect the distribution to vary based on whether the talker is an adult or not, in addition to speaker-specific adjustments to the intercept. Note that the model calculates an intercept (mean value) for each speaker, but does not include an effect for adult for each speaker (i.e., the model does not include a term like (adult|speaker)). This is because we are estimating a mean for each speaker, but not the effect for adultness for each speaker. Each speaker is only either an adult or a child, and so we cannot estimate the effect for adultness for each speaker. Doing things that dont make sense from the models perspective will cause it to crash or return strange values. Im not going to go into so much detail about the structure of the regression model right now because an explanation involves some of the less intuitive concepts relating to regression. We are going to to fit the model first and discuss its structure, and then get to the details of the model later. 3.2.1 Fitting the model First we find the mean and the standard deviation of our data to use in the priors. As before, I intend to use the data mean for the intercept, and use a standard deviation twice as wide as the data standard deviation. ## establish data statistics for priors mean (f0) ## [1] 225.4913 sd (f0) ## [1] 23.98959 We load the brms package and fit the model. Notice that I now include a prior for class = \"b\", which is the class for all predictors that are not the intercept. Our model now includes a non-intercept term, and so I need to include this prior: library (brms) set.seed (1) model = brms::brm (f0 ~ adult + (1|speaker), data = females, chains = 4, cores = 4, warmup = 1000, iter = 11000, thin = 10, prior = c(set_prior(&quot;student_t(3, 225.5, 48)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 48)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 48)&quot;, class = &quot;sd&quot;))) ## load and inspect pre-fit model model = readRDS (&#39;3_model.RDS&#39;) model ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ adult + (1 | uspeaker) ## Data: females (Number of observations: 804) ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~uspeaker (Number of levels: 67) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 19.10 1.77 16.00 22.88 1.00 2781 3089 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 220.47 2.80 215.10 226.16 1.00 1827 2813 ## adultchild 17.63 5.22 7.34 27.88 1.00 1987 2068 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 12.93 0.34 12.28 13.59 1.00 3537 3773 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The output is mostly familiar, but there is a new predictor in the section on Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 220.47 2.80 215.10 226.16 1.00 1827 2813 adultchild 17.63 5.22 7.34 27.88 1.00 1987 2068 In addition to the Intercept term, we now get estimates for a term called adultchild. Admittedly, this is a strange name, but its how R handles predictors that are words (called factors in R). In general, r names predictors like these factornameFactorlevel. For example, a factor called colors with levels red, green and blue might have a level represented like colorsred. So, the adultchild name tells us is that this is the estimate for the child level of the adult factor. A couple of questions arise. First, the Intercept term in the model above seems to correspond to the mean f0 for adult females. We can confirm this: ## calculate means of f0 based on values of adult vector tapply (females$f0, females$adult, mean) ## adult child ## 220.4010 238.3509 However, the estimate for children is 17.6 Hz, which is odd. This is obviously not the mean f0 for the girls in our sample. Why not? To understand model coefficients we are going to have to talk about contrasts 3.3 Contrasts Contrasts are the numerical implementation of factors in your model. Factors are variables like adult vs. child that are not inherently numerical. You may initially think that we can separately estimate the womens average, the girls average, and the overall mean. However, our models cant actually do this. The general problem is this: if you have two groups you cant independently calculate: group 1 mean. group 2 mean. the overall mean. Why not? Because once you know 2 of those things you know the 3rd. For example, if the group 1 mean is 5 and the overall mean is 6, obviously the group 2 mean must be 7. Why does this matter? Because when things are entirely predictable based on each other, they are not actually separate things, even though they may seem that way to us. When things are entirely predictable in this way we say they are linearly dependent, and regression models dont like this. Heres three perspectives on why this is a problem: Imagine you were trying to predict a persons weight from their height. You want to include height in centimeters and height in meters in your model, and you want to independently estimate effects for both predictors. Since height in centimeters = height in meters / 100, that is obviously not going to be possible. The effect of one must be 100 times the effect of the other! Even though it may be less transparent, this is the same reason why we cant estimate all the group means and the overall mean. With two groups, or any two points in a space, you can estimate one distance, not two. If each group could really be a different distance from the mean, you would need to estimate two distances. Instead, we are really only in a position to estimate one difference, that between our two group averages. When we had one group we obviously couldnt get the overall mean independently from the sample mean. All we had was the sample mean, and that was our best estimate of the population mean too. Adding 1 more group allows us to calculate 1 more mean. Why would it let us calculate 2 more means? That would mean adding a second group (with 1 mean) somehow contributed twice as much information as the first group did. Instead, adding a second mean changes our best guess for the population mean: it is now between the two groups. However, this information is in no way independent from the two group means in our design. 3.3.1 Treatment coding The coding scheme you use determines how your model represents the differences it encodes. In the model above we used treatment coding (the default in R). In treatment coding, a reference level is chosen to be the intercept, and all group effects reflect the difference between the group mean and the mean for the reference level. By default, R chooses the alphabetically-lowest level to be the reference level. That is why the Intercept in our model is equal to the mean of the adult group, the average for adult females. The effect for child (adultchild) represents the difference between the child mean and the adult mean. This means that our credible intervals also represent the difference in the means and not the means themselves. So, we expect the difference between girls and women in this sample to be about 17 Hz, and we think there is a 95% chance that the difference between the means is between 7.3 and 27.9 Hz in magnitude. We can see how the effects estimates in our model resemble the means, or differences between means, in our sample. # calculate group means tapply (females$f0, females$adult, mean) ## adult child ## 220.4010 238.3509 # find the difference between them diff (tapply (females$f0, females$adult, mean)) ## child ## 17.94984 To interpret treatment coded coefficients remember: The reference category mean is the Intercept in the model. The value of the coefficients of any other group mean will be equal to group mean - reference group mean. To recover the mean estimate for any other group, we add group mean + reference group mean. 3.3.2 Sum coding There are multiple options for coding schemes, and the best one for you depends on what you want to get out of the model. The only other alternative to treatment coding that we will talk about is what is called sum coding. It is a very useful coding scheme for many of the designs most commonly-used by linguists, and it is the coding shceme we will be using going forward. In sum coding, there is no reference level. Instead, the model Intercept represents the overall mean of all your groups, and group differences are represented with respect to the mean of your data. Just like for treatment coding, you cant estimate all of your group means. For treatment coding, R selects the alphabetically last level of your factor, and does not estimate it. Under sum coding, the missing factor level will always be equal to the negative sum of the other factors. This means you add up the values of the levels that are present, and flip the sign. The outcome is the value of your missing level. As discussed earlier, with only two groups if you know the overall mean and the distance of one group to the mean, you also know the distance of the other group to the mean. This can be seen quite clearly below where the difference of each group to the overall mean is exactly -8.97. So, if our model tells us that the mean is 229.4 and the adult group is -8.97 Hz below this, then the child group must be the negative sum of the other coefficients. In this case there is only one so we just flip the sign on it (i.e., - (-8.97)). # calculate group means means = tapply (females$f0, females$adult, mean) mean (means) ## [1] 229.376 # find the distances to the overall mean means - mean (means) ## adult child ## -8.974918 8.974918 To interpret treatment coded coefficients remember: The overall mean of all your groups is the Intercept in the model. The value of the coefficients of any other group mean will be equal to group mean - overall mean. To recover the mean estimate for any other group, we add group mean + overall mean. 3.3.3 Comparison of sum and treatment coding The image below shows a comparison of the way the two coding schemes treat our data. In each case they estimate 1 intercept and one effect, letting you recreate 1 other mean (i.e., they each omit one parameter). In treatment coding this is the overall mean, which in the 2-group case will always be Intercept + Effect/2. In the case of sum coding this is the effect for the second group, which will always be the same magnitude but have the opposite sign as the effect for the first group. 3.4 Refitting the model with sum coding We are going to re-fit the model we fit above using sum coding, and see how the coefficients change. 3.4.1 Fitting the model To fit a model with sum coding, we change the global contrast options in R. These options will be in effect until we restart R or change the contrasts to something else. ## to change to sum coding options (contrasts = c(&#39;contr.sum&#39;,&#39;contr.sum&#39;)) ## to change back to treatment coding #options (contrasts = c(&#39;contr.treatment&#39;,&#39;contr.treatment&#39;)) We can fit the model with sum coding using the exact same code since the options have changed. Please keep in mind you will need to set this every time you start R, as it will reset to treatment coding each time it restarts. set.seed (1) model_sum_coding = brms::brm (f0 ~ adult + (1|speaker), data = females, chains = 4, cores = 4, warmup = 1000, iter = 11000, thin = 10, prior = c(set_prior(&quot;student_t(3, 225.5, 48)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 48)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 48)&quot;, class = &quot;sd&quot;))) ## load and inspect pre-fit model model_sum_coding = readRDS (&#39;3_model_sum_coding.RDS&#39;) model_sum_coding ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ adult + (1 | uspeaker) ## Data: females (Number of observations: 804) ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~uspeaker (Number of levels: 67) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 19.13 1.80 16.01 22.96 1.00 2892 3144 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 229.33 2.71 224.07 234.81 1.00 2169 2758 ## adult1 -8.89 2.65 -14.19 -3.55 1.00 2085 2825 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 12.95 0.34 12.30 13.62 1.00 3922 3710 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). A comparison of these results to that of our first model shows that the main difference is that, as expected, the Intercept now reflects the overall mean and the single parameter reflects the distance of the adult mean to the overall mean. Note that the parameter is now called adult1. This is just how brm handles factors. Predictors representing factors will be named factornameN, where factorname is the predictor name and N is the level number. Levels are ordered alphabetically starting at one, and the alphabetically-last level will not be estimated. sort ( unique (females$adult)) ## [1] &quot;adult&quot; &quot;child&quot; So, adult1 in our model corresponds to the adult level of our predictor, and adult2 would be child, but it is not separately estimated by our model (since adult2 = -adult12). 3.4.2 The model Regression models try to break up values into their components. This is why effects are expressed in terms of differences to some reference value. For example, if we say that that a persons average f0 is usually 220 Hz and under some condition their average f0 is also 220 Hz, then we could say that this condition has no effect on their f0. To say that this condition has no effect on mean f0 is to say that it causes no difference in mean f0. On the other hand something that does cause a difference in mean f0 does have an effect on mean f0. We can express the effect of something in terms of the difference it causes, for example, saying that under so and so conditions a person will tend to raise their f0 by 20 Hz, relative to some reference value (the intercept). More generally, we can think of any variable as the sum of a bunch of independent effects. This is just a way to think about variables, to break up observed values into their component parts. It should not be confused with the reality of these values and the processes that underlie them (whatever that is!). So far we have covered the fact that after picking a value to use as a reference point (the model intercept), our models: represent group means as deviations from the intercept. represent the speaker-specific deviations from the intercept (\\(\\gamma_{speaker}\\)) as being centered at 0, with a standard deviation of \\(\\sigma_{speaker}\\). represent the random error (\\(\\varepsilon\\)) as having a mean of 0 and a standard deviation of \\(\\sigma_{error}\\). In each case, these reflect deviations from some reference point. As a result, when the parameters associated with different effects equal 0, this means that no effect is present. when group coefficients are 0 the group lies exactly at the intercept. In sum coding this is the overall mean. when a speaker-effect is 0 this speaker is exactly average with respect to their group. when an error is 0 this production is exactly as expected for a given speaker. Based on thinking of our predictors as representing deviations from some reference value, We can break up any observed value into its component parts. For example, suppose that the the overall mean is 229 Hz, the adult female mean is 220 Hz, and that a particuar speaker has a mean f0 of 240 Hz. If we observe a value of 256 Hz for this speaker, that suggests the following decomposition: 256 = 229 (Intercept) - 9 (adult female mean) + 20 (speaker effect) + 16 (error) This reflects the following considerations: the average f0 across the groups is 229 Hz. the average for adult females is 9 Hz below the overall mean. this speakers average f0 is 20 Hz above the average for adult females. this particular production is 16 Hz higher than expected for this particular speaker. Another observation from this talker might be: 237 = 229 (Intercept) - 9 (adult female mean) + 20 (speaker effect) - 3 (error) In this case, the error is -3 since the production is now 3 Hz below the speaker average. Regressions models basically carry out these decompositions for us, and present information regarding these decompositions in the model summaries. The full model specification, including prior probabilities is below. I used the same ordering format for the t-distributions that brm uses (nu, mean, sd). \\[\\begin{equation} \\begin{split} \\textrm{Likelihood:} \\\\ y_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\mu_{[i]} = Intercept + adult1 + \\alpha_{speaker_{[i]}} \\\\\\\\ \\textrm{Priors:} \\\\ \\alpha_{speaker} \\sim \\mathcal{N}(0,\\sigma_{speaker}) \\\\ \\\\ Intercept \\sim t(3, 225.5, 48) \\\\ adult1 \\sim t(3, 0, 48) \\\\ \\sigma_{error} \\sim t(3, 0, 48) \\\\ \\sigma_{speaker} \\sim t(3, 0, 48) \\\\ \\end{split} \\end{equation}\\] The top chunk is labeled likelihood because this chunk specifies the relationships between our parameters and our data. As a result, the relationships specified in this section determine the likelihood of the model parameters. For example, in the first line we say that our data is normally distributed around some mean parameter. In turn, this specifies the shape of the likelihood function for that parameter given the data (as discussed in Figure 1.8). What I mean by this is that the shape of the likelihood functions for different model coefficients will be based on the relationships laid out in the section of the model description labeled likelihood. 3.4.3 Interpreting the two-group model The brms package has several functions that make getting information from these models simple. The fixef function gets you means and 95% credible intervals for all your population level parameters (sometimes called fixed effects). brms::fixef (model_sum_coding) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 229.327715 2.711261 224.07166 234.810010 ## adult1 -8.888301 2.651641 -14.18979 -3.550734 To recover the group means for women and girls, we need to combine the Intercept and the group parameters. Were not actually allowed to just add the values you see above (for good, but technical reasons). What you actually need to do is add the individual samples for parameters, and then inspect the output. You can see the individual samples by calling the fixef function and setting summary to FALSE. Below I plot the individual samples and histograms for the Intercept (the overall mean), the adult1 (the effect for adults) parameter, and the negative of the adult parameter (the effect for girls). I also present the combination of Intercept+adult1, which yields an estimate of the adult female mean. samples = brms::fixef (model_sum_coding, summary = FALSE) head(samples, 10) ## parameters ## iterations Intercept adult1 ## [1,] 226.1704 -8.121943 ## [2,] 234.4365 -10.380060 ## [3,] 228.6168 -7.901476 ## [4,] 226.2287 -8.130842 ## [5,] 225.0607 -10.228533 ## [6,] 227.6880 -6.691685 ## [7,] 227.8307 -8.078614 ## [8,] 230.0513 -10.569158 ## [9,] 228.3428 -8.612124 ## [10,] 228.9045 -5.617644 par (mfrow = c(2,4), mar = c(4,4,3,1)) hist (samples[,&#39;Intercept&#39;],freq=FALSE, col = skyblue,main=&#39;Intercept&#39;) hist (samples[,&#39;adult1&#39;], freq=FALSE, col = deeppurple,main=&#39;adult1&#39;) hist (-samples[,&#39;adult1&#39;], freq=FALSE, col = teal,main=&#39;-adult1&#39;) hist (samples[,&#39;Intercept&#39;]+samples[,&#39;adult1&#39;], freq=FALSE, col = yellow,main=&#39;Intercept+adult1&#39;) plot (samples[,&#39;Intercept&#39;], col = skyblue,pch=16) plot (samples[,&#39;adult1&#39;], col = deeppurple, pch=16) plot (-samples[,&#39;adult1&#39;], col = teal, pch=16) plot (samples[,&#39;Intercept&#39;]+samples[,&#39;adult1&#39;], col = yellow,pch=16) We can then summarize the sum of the parameters using the posterior_summary function, resulting in a mean, standard deviation, and credible interval for the new parameter: ## calculate child mean adult_mean = samples[,&#39;Intercept&#39;] + samples[,&#39;adult1&#39;] ## report mean and spread of samples brms::posterior_summary (adult_mean) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 220.4394 2.871496 214.8694 226.2171 Luckily, there is a function in brms called hypothesis that helps us add terms very easily, without having to do any of the above steps. You can ask the hypothesis function to add terms in your model (spelled just as they are in the print statement), and to compare the result to some number. If you compare the result to 0, it just tells you about the result of the terms you added. brms::hypothesis(model_sum_coding, &quot;Intercept + adult1 = 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio ## 1 (Intercept+adult1) = 0 220.44 2.87 214.87 226.22 NA ## Post.Prob Star ## 1 NA * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. The output above tells us what our estimate is for Intercept + adult1, which we know to be the expected mean f0 for adult females. Whereas the credible interval for the original effect reflected uncertainty in difference between means, the credible interval provided is now for the actual girls mean, not for the difference between the means. We can use the hypothesis function to confirm similar results for the model fit using treatment coding. In that model, the reference category was the adult mean. So, if we call the hypothesis function on the intercept of the treatment-coding model, we can see that it will present similar values to those seen above. brms::hypothesis(model, &quot;Intercept = 0&quot;) ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob ## 1 (Intercept) = 0 220.47 2.8 215.1 226.16 NA NA ## Star ## 1 * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. We can check several parameter combinations simultaneously. Below I recreate all our mean estimates of interest, first for the sum coding model, and then for the treatment coding model. Notice that these models contain the same information, just represented in different ways. brms::hypothesis(model_sum_coding, c(&quot;Intercept = 0&quot;, # overall mean &quot;Intercept + adult1 = 0&quot;, # adult mean &quot;Intercept - adult1 = 0&quot;)) # child mean ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio ## 1 (Intercept) = 0 229.33 2.71 224.07 234.81 NA ## 2 (Intercept+adult1) = 0 220.44 2.87 214.87 226.22 NA ## 3 (Intercept-adult1) = 0 238.22 4.53 229.34 247.13 NA ## Post.Prob Star ## 1 NA * ## 2 NA * ## 3 NA * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. brms::hypothesis(model, c(&quot;Intercept + adultchild/2 = 0&quot;, # overall mean &quot;Intercept = 0&quot;, # adult mean &quot;Intercept + adultchild = 0&quot;)) # child mean ## Hypothesis Tests for class b: ## Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio ## 1 (Intercept+adultc... = 0 229.28 2.69 224.19 234.74 NA ## 2 (Intercept) = 0 220.47 2.80 215.10 226.16 NA ## 3 (Intercept+adultc... = 0 238.10 4.51 229.41 247.26 NA ## Post.Prob Star ## 1 NA * ## 2 NA * ## 3 NA * ## --- ## &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. ## &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; ## for two-sided hypotheses, the value tested against lies outside the 95%-CI. ## Posterior probabilities of point hypotheses assume equal prior probabilities. If I were writing this is in a paper, at this point I could present this information in a paragraph. Based on the sum coded model, I would say something like: The overall mean f0 across all speakers was 229 Hz (sd = 2.7, 95% CI = 224, 234). Adult female mean f0 was 220 Hz (sd = 2.9, 95% CI = [215, 226]), while the mean f0 for girls was 228 Hz (sd = 4.28, 95% CI = [230, 246]). The difference between the group means was 18 Hz on average (sd = 5.3, 95% CI = [7, 28]), suggesting a small but noisy difference between groups, on average. 3.5 Random Effects Below I present the speaker boxplot, with different colors for each group. So far we have discussed the speaker-dependent deviations from averages, but we havent actually done anything with them. One of the nice things about multilevel models is that we can actually estimate these parameters, and use them to answer our research questions. Multilevel models are sometimes also called random effects or mixed effects models. What is mixed about them? Researchers often talk about whether effects are fixed or random. The general idea is that fixed effects are specifically chosen from a small set of possibilities, and we are not necessarily interested in the other levels. For example, in this experiment we include children 10-12 and adults. Speaker age-groups dont really come from an infinite set of independent age-groups, not ones that will meaningfully affect our research questions anyways. Instead, categories like adult and child are chosen from a finite set of meaningful categories, and we are specifically interested in the categories weve chosen. In this case, I really do care about mean f0 for adults and not some other category I didnt sample. In contrast, random effects are not chosen arbitrarily but at random. The speakers in our sample do form part of a practically infinite sample. I am actually not interested in the speakers I have but in what this says about the categories I did not include. In fact, most researchers would gladly trade perfect knowledge about any one speaker for even a small amount knowledge about a large set of speakers from the population. Despite this primarily philosophical definition, in practice the terms fixed and random effects have several inconsistent and sometimes contradictory definitions. Luckily, when thinking about these concepts in terms of multilevel models we can be very specific: random effects are those for which we estimate a standard deviation term. So, we treat differences in speaker average f0 (\\(\\gamma_{speaker}\\)) as a variable, and estimate \\(\\sigma_{speaker}\\) for the distribution of variable. 3.5.1 Random effects, priors and pooling Look at the list of priors in our latest model definition. If you look at our latest brm model fit, you will see where most of these are specifically defined in the function call. However, you will notice that the standard deviation of the prior (\\(\\sigma_{speakers}\\)) for the speaker-specific intercept deviations (\\(\\alpha_{speaker}\\)) is actually not specifically defined in the model. \\[\\begin{equation} \\begin{split} \\textrm{Priors:} \\\\ \\alpha_{speaker} \\sim \\mathcal{N}(0,\\sigma_{speaker}) \\\\ \\\\ Intercept \\sim t(3, 225.5, 48) \\\\ adult1 \\sim t(3, 225.5, 48) \\\\ \\sigma_{error} \\sim t(3, 0, 48) \\\\ \\sigma_{speaker} \\sim t(3, 0, 48) \\\\ \\end{split} \\end{equation}\\] It turns out that the \\(\\sigma_{speaker}\\) parameter is estimated from the data! For this reason, the prior you set on \\(\\sigma_{speaker}\\) is sometimes called a hyperprior, because it is the prior for your prior! By definition, the speaker effects are centered around 0 (the average). The only question is, how widely are they distributed? Well, what better way to answer this question than using the distribution evident in the data itself? This means that the amount of variation you expect between speakers is based on the amount of between-speaker variation you observe. The idea is basically: is it believable that this one person be this far away from the average? Well, it depends on what everyone else looks like! By estimating the prior for some parameters from the data itself, multilevel models can help protect against problems that naturally arise when researchers compare many things. This is because in a Bayesian analysis, the prior influences the estimates of your individual parameters. As a result, the variation in the other parameters in your sample can influence any given parameter. This process is sometimes referred to as partial pooling, since it refers to the partial pooling of information across subjects. Using partial pooling means the subject estimates are neither completely independent nor totally merged. They actually end up influencing each other in a logical manner. Broadly speaking, individual observations that deviate from typical values of the population are maintained when there is good enough evidence for them. When there is wak evidence for them relative to the other observations in the sample, estimates may be brought closer to the group averages. As a result, partial pooling results in what is sometimes called shrinkage, because extreme values get shrunk towards the overall mean. In the context of a Bayesian multilevel model random effects are those you estimate with partial pooling, where the prior is estimated from the data (e.g., \\(\\sigma_{speaker}\\)). In contrast, fixed effects are those predictors for which you set arbitrary priors before fitting the model. Although the terms terms fixed and random effects are useful (and I continue to use them to describe my models), it is important to keep in mind that the philosophical distinction between fixed and random variables is not relevant for the models we are discussing here. The real distinction is: do I want to fit every level totally independently as if there were all unrelated? Or do I want to use partial pooling in my estimates, thereby using all of the information present in my sample to protect against spurious findings? In general, when you have many levels of a factor, it may be a good idea to include these as random effects. There is not much downside to it, you get more information from the model (e.g., information about \\(\\sigma_{predictor}\\)), and you can always fit multiple models to see what, if any differences, result form the different approaches. Some useful things to consider are also: Do you believe that treating a predictor as a random effect offers a modeling advantage? Does it better reflect the process/relationships you are trying to understand? Does it provide you with information you find useful? Is it realistic to fit this kind of model given the amount of data you have, and the nature of the relationships in the data? Right now the last point is not something we have talked about very much, but it is something we will need to worry about more as our models become more complicated. 3.5.2 Inspecting the random effects The brms package has several functions to help understand our random effects. There is a function called ranef which returns random effects estimates in the same way that fixef provides fixed effects estimates. The leftmost column of the output below represents the estimated effects associated with each speaker average value. ## I am telling it to give me the &#39;speaker&#39; Intercepts, but only the first ## 10 rows. This is just so it doesn&#39;t take up the whole page. brms::ranef (model_sum_coding)$speaker[1:10,,&quot;Intercept&quot;] ## NULL Notice that the speaker averages vary around 0, and some are even negative. That is because the speaker effects (and all random effects) are coded using sum-coding. So, what the speaker-specific mean terms tell us is: what is the average value for this speaker, relative to the overall average, and the group specific adjustment? Keep in mind, since our model encodes both the overall average and the group means, the speaker-effects encode differences to the group means and not to the overall means. For example, we see that the mean for the second speaker above is -19.8 Hz. This means that they have a lower f0 than expected, and their mean f0 is 19.8 Hz lower than their group mean f0. In contrast, the first speaker has a speaker mean effect that is nearly zero (1.1). That tells you that this speakers mean f0 was nearly the same as that of the average for their group We can use a simple plotting function I wrote to look at the distribution of speaker effects terms. The function takes in the summaries made by brms and plots a point for each parameter mean, and lines indicating the credible intervals calculated by brm (usually 95% credible intervals). par (mfrow = c(1,1), mar = c(4,4,1,1)) brmplot( brms::ranef (model_sum_coding)[[&quot;uspeaker&quot;]][,,&quot;Intercept&quot;], col = colors) abline (h=0) We can compare the estimates of by-speaker intercepts to the distribution of actual data arranged by subject. There is clearly a close correspondence! ## plot comparison of estimates of speaker means to actual data par (mfrow = c(1,2), mar = c(4,4,1,1)) brmplot( brms::ranef (model_sum_coding)[[&quot;uspeaker&quot;]][,,&quot;Intercept&quot;], col = colors) boxplot (f0 ~ speaker, data=females, col = colors, ylim = c(150,310)) 3.6 But what does it all mean? Ok, so are the f0s produced by women and girls different? Consider the distribution of productions between and within speakers and groups, as shown in the figure below (lines represent the group means). An unbiased look at our results (and figures) so far suggests that: the magnitude of between speaker variation is larger than the difference between girls and women (19 Hz vs 18/17 Hz). This means that random people drawn from the two groups largely overlap. the magnitude of within-speaker variation (12 Hz) is almost as large as the group difference and the between-speaker difference! This means that two random productions from two different people might be the same, even when the speakers average f0s are quite different. And yet: the mean f0 is reliably different between women and girls, and there are well-known anatomical reasons for this that are expected a priori (i.e., adult females are larger than younger women, larger speakers often produce lower f0s). the between-speaker differences are random from person to person, but systematic for a given person. even the within-speaker variation may be systematic given a more-complicated model. So are they different yes or no? Statistics aside, a fair assessment of our data suggests that neither binary conclusion is fully supported: they are distinguishable but overlap substantially. If you want to use this model to highlight the differences between girls and women, I think it is valid. I also think it would be valid to use this data to talk about between and within-speaker variation, highlighting the overlap that exists between speakers. Both are true! To a large extent, the meaning is as much in our heads as it is in the model, and we are free to interpret the results as we see fit, as long as reviewers (and readers in general) will believe us. The model is simply a reflection of the relationships in our data, and the interpretation is up to us. Keep in mind that the as long as reviewers (and readers in general) will believe us component is crucial. The results of the model will need to be interpreted in the larger context of the work it is presented in, and in terms of scientific and general knowledge that readers have. The results of any model will need to make sense given this, and a statistical result on its own will not be enough to make people (including us) believe outlandish, or even weakly supported claims. The model is not reality and should not be confused with reality. This is a very important point! A statistical finding does not prove that something is true. This kind of thinking has caused many problems for researchers in the social sciences recently. In general, we can imagine that 10 people might approach any given research question in 10 different ways, a concept known as researcher degrees of freedom. This would cause slight differences in their results, resulting in a sort of distribution for any given result. How can a fixed underlying reality result in a distribution or results? When they are all slightly wrong! For example, we know for a fact that f0 varies, weakly but systematically, across vowel categories, a concept known as intrinsic f0. A model that included vowel category as a within-speaker predictor would reduce the apparent error in our model (making \\(sigma_{error}\\) smaller), and might affect the precision of our other estimates. Would this new model invalidate our current model? Answering yes to this question is generally problematic because there is always a better model out there, and so every model is automatically invalid. The solution is to think of your model not as a mathematical implementation of reality but instead as a mathematical implementation of your research questions. Your model should include the information and structure that you think are necessary to represent and investigate your questions. Using a different model can result in different results given the same data, but asking a different question can also lead to different results given then same data! One of my favorite phrases to use is given our data and model structure. This phrase is helpful because it highlights the fact that your results are contingent on: the data you collected. Given other data you may have come to other conclusions. the model you chose. Given another model you may have come to other conclusions. "],["comparing-many-groups.html", "Chapter 4 Comparing many groups 4.1 Data and research questions 4.2 Comparing four (or any number of) groups 4.3 Investigating many groups using predictors: Analysis of Variance 4.4 Interactions and interaction plots 4.5 Investigating interactions with a model", " Chapter 4 Comparing many groups Last chapter we talked about comparing two groups. Although its very simple, its also fundamental and used often: more complicated problems are often broken down into sets of several two-group questions. However, real experiments dont usually begin as two-group questions. In the last chapter we found a reliable, but noisy, difference between women and girls in average f0. In this chapter, we are going to consider the productions of f0 from all four groups in the data. These research questions would traditionally be addressed with a repeated-measures ANOVA. Alternatively, the speaker means could have been used as single data points for each speaker and and ANOVA might have been used. I am only pointing this out in case it is useful, we will not be talking about traditional ANOVA here! 4.1 Data and research questions We are still going to work with the Hillenbrand et al. data. This time we are going to add a variable indicating adultness, and one indicating gender. However, this time we are going to work with all four groups at the same time: b (boys), g (girls), m (men), and w (women). library (brms) options (contrasts = c(&quot;contr.sum&quot;,&quot;cont.sum&quot;)) url1 = &quot;https://raw.githubusercontent.com/santiagobarreda&quot; url2 = &quot;/stats-class/master/data/h95_vowel_data.csv&quot; h95 = read.csv (url(paste0 (url1, url2)))[,c(&#39;f0&#39;,&#39;speaker&#39;,&#39;group&#39;)] ## make variable that indicates if the talker is an adult h95$adult = &quot;&quot; h95$adult[h95$group %in% c(&#39;w&#39;,&#39;m&#39;)] = &quot;adult&quot; h95$adult[h95$group %in% c(&#39;g&#39;,&#39;b&#39;)] = &quot;child&quot; ## make variable indicating speaker gender h95$gender = &quot;female&quot; h95$gender[h95$group %in% c(&#39;b&#39;,&#39;m&#39;)] = &quot;male&quot; ## make speaker number a factor h95$speaker = factor (h95$speaker) ## see data head (h95) ## f0 speaker group adult gender ## 1 174 47 m adult male ## 2 102 48 m adult male ## 3 99 49 m adult male ## 4 124 50 m adult male ## 5 115 51 m adult male ## 6 96 52 m adult male Our potential research questions are substantially more complicated than in the two-group case. First, there are four groups now, meaning we could potentially make 6 2-group comparisons. Second, the groups also differ along multiple dimensions, making it more difficult to make two-group comparisons that ask one single question. For example, the man and girl groups differ according to adultness and gender. How could we know what part of their f0 difference we should attribute to adultness and what part we should attribute to maleness? par (mfrow = c(1,3)) boxplot (f0 ~ group, data=h95, main = &quot;Overall&quot;, ylim = c(90,330),col=cols[3:6]) boxplot (f0 ~ adult, data=h95, main=&quot;Adultness&quot;, ylim =c(90,330), col=cols[7:8]) boxplot (f0 ~ gender, data=h95, main=&quot;Gender&quot;, ylim = c(90,330), col=cols[1:2]) We can consider our data in several ways: as four independent groups, or as two 2-groups comparisons (adult vs child, female vs male). We are going to focus on the 4-way comparison first. colors = cols[c(3,5,4,6)][ apply (table(h95$speaker, h95$group),1,which.max) ] ## speaker boxplots par (mar = c(4,4,1,1)); layout (mat = t(c(1,2)), widths = c(.7,.3)) boxplot (f0 ~ speaker, data=h95, col = colors, ylim = c(90,330)) ## The density figures are rotated tmp = density (h95$f0[h95$group==&quot;b&quot;]) plot (tmp$y, tmp$x, lwd = 3, col = cols[3], ylab = &quot;f0&quot;,xlab=&quot;Density&quot;, ylim = c(90,330), xlim = c(0,0.025), type = &#39;l&#39;) tmp = density (h95$f0[h95$group==&quot;g&quot;]); lines (tmp$y, tmp$x, lwd=3, col=cols[4]) tmp = density (h95$f0[h95$group==&quot;m&quot;]); lines (tmp$y, tmp$x, lwd=3, col=cols[5]) tmp = density (h95$f0[h95$group==&quot;w&quot;]); lines (tmp$y, tmp$x, lwd=3, col=cols[6]) R treats verbal predictors as factors and assumes that each different label is a different group. Each group of a factor is called a level. Actually, weve been using factors all along because our speaker predictor is a factor and the individual participants are levels! As far as these our are concerned, participant/speaker/subject has no special status as a predictor and it is just a factor with many levels. A factor is actually a data type in R. Its basically the same as a vector of words (or numbers!), but it has some additional properties that are useful. For example, consider our group predictor, which tells us which group each speaker falls into. Initially it is a character vector. We see that the first few tokens are produced by men (m), and that there is no numerical value associated with these letter labels. The unique function returns all unique labels in the vector, in the order that they appear. head (h95$group) ## see the first 6 observations ## [1] &quot;m&quot; &quot;m&quot; &quot;m&quot; &quot;m&quot; &quot;m&quot; &quot;m&quot; class (h95$group) ## class starts as a character vector ## [1] &quot;character&quot; head (as.numeric (h95$group)) ## no numerical values ## Warning in head(as.numeric(h95$group)): NAs introduced by coercion ## [1] NA NA NA NA NA NA unique (h95$group) ## we can see the number of unique groups ## [1] &quot;m&quot; &quot;w&quot; &quot;b&quot; &quot;g&quot; We can turn the character vector group into a factor vector group_f. The benefit of this is that these nominal labels now have an inherent ordering, and associated numerical values. Actually, R functions such as brm turn your character predictors into factors in the process of fitting the model. Doing this yourself gives you control over how they will be handled. h95$group_f = factor(h95$group) ## we can turn it into a factor in R levels(h95$group_f) ## now it has official levels ## [1] &quot;b&quot; &quot;g&quot; &quot;m&quot; &quot;w&quot; head (as.numeric (h95$group_f)) ## now it has nuerical values ## [1] 3 3 3 3 3 3 For example, by default factor levels are ordered alphabetically. This means that if we are using sum coding we will not estimate the g parameter (the last group) and if we are using treatment coding, the intercept will be equal to m (the first group). You can control this behavior by re-ordering the factor levels as below: h95$group_f2 = factor (h95$group_f, levels = c(&#39;w&#39;,&#39;m&#39;,&#39;g&#39;,&#39;b&#39;)) levels (h95$group_f2) ## [1] &quot;w&quot; &quot;m&quot; &quot;g&quot; &quot;b&quot; head (as.numeric (h95$group_f2)) ## now it has official levels ## [1] 2 2 2 2 2 2 4.2 Comparing four (or any number of) groups We are first going to treat the four groups as four groups with no internal structure. It makes no sense to do this for this particular data, but in many cases we would have several groups with no logical internal divisions. 4.2.1 The model Unlike in the two-group example, this is not just a single characteristic that can be represented with a single variable. Actually, the model needs (approximately) one variable per group, but you dont need to worry about this. Our factor group has four levels: b,g,m, and w. For models where the predictor is a factor with more than two levels, we can represent the predictor in a vector like this, \\(group_{i}\\), where \\(i\\) is a counter variable that goes from 1 to the number of groups. Since we have four groups, our group predictor can be thought of as a vector like [b, g, m, w] where the letters represent the values of the four groups predictors. When considered this way, we see why it is useful to represent factor levels as number. Our updated model is now as seen below. \\[\\begin{equation} \\begin{split} \\textrm{Likelihood:} \\\\ y_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\mu_{[i]} = Intercept + group_{group_{[i]}} + \\alpha_{speaker_{[i]}} \\\\\\\\ \\textrm{Priors:} \\\\ \\alpha_{speaker} \\sim \\mathcal{N}(0,\\sigma_{speaker}) \\\\ \\\\ Intercept \\sim t(3, 220, 100) \\\\ group_{[i]} \\sim t(3, 0, 100) \\\\ \\sigma_{error} \\sim t(3, 0, 100) \\\\ \\sigma_{speaker} \\sim t(3, 0, 100) \\\\ \\end{split} \\tag{4.1} \\end{equation}\\] Notice that for each trial number \\(i\\) the group predictor is indexed by a variable called group. This is a bit confusing, but I am just trying to be consistent with how R does things. As noted above, a factor predictor like group is really just a bunch of numbers that represent group effects in a vector. So R really treats group as a sequence of numbers representing group numbers. But it also calls the predictor by that name! So, though it may look strange \\(group_{group_{[i]}}\\) just says that you have a predictor in your model called group and it has a few possible values (four in this case). Also, you have a variable in your data with the same name that tells you which value of \\(group\\) to use for each observation! So, for example, above we saw that the first value of the group vector is 3. This means this speaker is a member of the m group. This means that in our model above, the equation determining \\(\\mu_{[i]}\\) will include the value \\(group_{[3]}\\) in it because \\(group_{group_{[i]}=3}\\). We can check out the mean and standard deviations for the data to set prior probabilities for the model parameters. mean (h95$f0) ## [1] 197.027 sd (h95$f0) ## [1] 51.76277 We are going to use sum coding, which means that the intercept will be the mean of all the groups, and group effects will be represented as differences from this mean. Remember that the missing group effect will be equal to the negative sum of the coefficients that are present. By default, R drops the last level from your factor, which in our case will be the w level. We fit the model below: options (contrasts = c(&quot;contr.sum&quot;,&quot;cont.sum&quot;)) set.seed (1) model_four_groups = brm (f0 ~ group + (1|speaker), data = h95, chains = 4, cores = 4, warmup = 1000, iter = 11000, thin = 10, prior = c(set_prior(&quot;student_t(3, 200, 100)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 100)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 100)&quot;, class = &quot;sd&quot;))) # saveRDS (model_four_groups, &quot;4_model_four_groups.RDS&quot;) ## load and inspect pre-fit model model_four_groups = readRDS (&quot;4_model_four_groups.RDS&quot;) model_four_groups ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ group + (1 | speaker) ## Data: h95 (Number of observations: 1668) ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~speaker (Number of levels: 139) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 20.95 1.34 18.51 23.82 1.00 2080 2564 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 206.49 1.93 202.67 210.21 1.00 1474 2270 ## group1 29.53 3.52 22.78 36.65 1.00 1253 2236 ## group2 31.88 3.98 24.35 39.68 1.00 1565 2392 ## group3 -75.18 2.90 -80.89 -69.62 1.00 1246 2383 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 11.86 0.22 11.46 12.31 1.00 3882 3814 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We can see that the intercept is the average of the group means, and our coefficients are equal to the centered group means: ## grop means means = tapply (h95$f0, h95$group, mean) ## overall mean mean (means) ## [1] 206.5111 ## group means means ## b g m w ## 236.0741 238.3509 131.2185 220.4010 ## centered means means - mean (means) ## b g m w ## 29.56295 31.83975 -75.29261 13.88991 ## parameters = centered means hypothesis (model_four_groups, c(&quot;group1 = 0&quot;, &quot;group2 = 0&quot;, &quot;group3 = 0&quot;, &quot;-(group1+group2+group3) = 0&quot;))[[1]][,1:5] ## Hypothesis Estimate Est.Error CI.Lower CI.Upper ## 1 (group1) = 0 29.53202 3.523919 22.783088 36.64507 ## 2 (group2) = 0 31.87791 3.976443 24.351030 39.68355 ## 3 (group3) = 0 -75.17590 2.903665 -80.888641 -69.61975 ## 4 (-(group1+group2+group3)) = 0 13.76598 2.934586 7.923066 19.37857 In these Bayesian models, we can actually compare any groups we want in this model, using comparisons of the posterior samples (as shown in chapter 3). For example, the difference between girls and boys can be found by asking if one minus the other equals 0 (which would be true if these were identical): hypothesis (model_four_groups, &quot;group1 - group2 = 0&quot;)[[1]][,1:5] ## Hypothesis Estimate Est.Error CI.Lower CI.Upper ## 1 (group1-group2) = 0 -2.345888 6.437511 -15.03817 10.28229 4.3 Investigating many groups using predictors: Analysis of Variance 4.3.1 The model In the previous section, we acted like we just had four different groups with no internal structure. Of course, we know that our groups differ systematically from each other in meaningful ways. For example, we might have chosen to fit two separate models that looked like this: brm (f0 ~ gender + (1|speaker) brm (f0 ~ adult + (1|speaker) For several reasons (some of which we will see very soon), it is preferable to fit a single model with both predictors at once, rather than fitting two separate models to each research question. Our R model formula wil now look like this, reflecting the influence of both predictors simultaneously: f0 ~ adult + gender + (1|speaker) This can be read like f0 is distributed according to effects for speaker adultness and gender, with random intercepts for each speaker. You may have noticed that our model no longer includes the group predictor. This is because the group label is perfectly predictable on the basis of adult and gender (a g must have values of female and child, and so on). Basically, we have decomposed the groups into two components to help us understand the effect of each. This is simply and extension of what we have been doing from the start. For example our model was previously: \\(\\mu_{[i]} = Intercept + (group_{group_{[i]}}) + \\alpha_{speaker_{[i]}}\\) However, since group can be exactly represented by combinations of gender and adult, our model sort of always contained this more-complicated model inside of it: \\(\\mu_{[i]} = Intercept + (adult_{adult_{[i]}} + gender_{gender_{[i]}}) + \\alpha_{speaker_{[i]}}\\) This is what I often refer to in my work as an ANOVA-like decomposition. ANOVA, the ANalysis Of VAriance, is a technique, or a general approach, to understanding data by focusing on the sources of variance contained in it. We have actually been chipping away at the error variance little by little by making more complicated models. Recall that our very first approach to understanding f0 looked like this: \\[ \\sigma_{total} = \\sigma_{error} \\] In other words, all variation was error. After this we added between-speaker variation to the model, and removed that from the error. \\[ \\sigma_{total} = \\sigma_{speaker} + \\sigma_{error} \\] Now, our model can individually estimate the variation in observed f0 due adultness, gender, to between-speaker variation and to production error. \\[ \\sigma_{total} = \\sigma_{adult} + \\sigma_{gender}+\\sigma_{speaker} + \\sigma_{error} \\] Our complete model is now: \\[\\begin{equation} \\begin{split} \\textrm{Likelihood:} \\\\ y_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\mu_{[i]} = Intercept + adult_{adult_{[i]}}+gender_{gender_{[i]}} + \\alpha_{speaker_{[i]}} \\\\\\\\ \\textrm{Priors:} \\\\ \\alpha_{speaker} \\sim \\mathcal{N}(0,\\sigma_{speaker}) \\\\ \\\\ Intercept \\sim t(3, 200, 100) \\\\ adult_{[i]} \\sim t(3, 0, 100) \\\\ gender_{[i]} \\sim t(3, 0, 100) \\\\ \\sigma_{error} \\sim t(3, 0, 100) \\\\ \\sigma_{speaker} \\sim t(3, 0, 100) \\\\ \\end{split} \\tag{4.2} \\end{equation}\\] 4.3.2 Fitting the model and interpreting the results Below I fit the model using the data statistics outlined above. set.seed (1) model_both = brm (f0 ~ adult + gender + (1|speaker), data = h95, chains = 4, cores = 4, warmup = 1000, iter = 11000, thin = 10, prior = c(set_prior(&quot;student_t(3, 200, 100)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 100)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 100)&quot;, class = &quot;sd&quot;))) saveRDS (model_both, &quot;4_model_both.RDS&quot;) ## load and inspect pre-fit model model_both = readRDS (&quot;4_model_both.RDS&quot;) model_both ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ adult + gender + (1 | speaker) ## Data: h95 (Number of observations: 1668) ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~speaker (Number of levels: 139) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 29.30 1.83 26.01 33.29 1.00 1754 2626 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 209.27 2.64 204.11 214.56 1.00 1105 1675 ## adult1 -32.84 2.73 -38.21 -27.33 1.00 924 1482 ## gender1 30.61 2.46 25.73 35.43 1.01 1181 2186 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 11.86 0.21 11.46 12.30 1.00 4241 3814 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We can see that the model output is largely familiar, except now we have two non-Intercept Population-Level effects: adult1 and gender1, representing the categories adult and female respectively. Remember that since we used sum coding, the effects for the groups that are not represented (child, male) are just the opposite of the groups that are represented. Below we can see that the Intercept is reasonably close to the mean of the group means and the adult effect is about half the difference between the mean of the adult and child groups. However, the model seems to overestimate the average differences between male and female groups. ## grop means means = tapply (h95$f0, h95$group, mean) ## overall mean in data mean (means) ## [1] 206.5111 ## group means means ## b g m w ## 236.0741 238.3509 131.2185 220.4010 ## average adultness difference in data ((means[&#39;b&#39;]+means[&#39;g&#39;])/2 - (means[&#39;m&#39;]+means[&#39;w&#39;])/2) / 2 ## b ## 30.70135 ## average gender difference in data ((means[&#39;g&#39;]+means[&#39;w&#39;])/2 - (means[&#39;m&#39;]+means[&#39;b&#39;])/2) / 2 ## g ## 22.86483 We can recover the group means by adding up the individual coefficients. This can be tedious and requires you to be careful and methodical, but this isnt actually difficult. Remember that each of the four groups is uniquely identified by a combination of gender and adultness. To recover the group means we need to add the right combination of coefficients to the intercept. For example, the second hypothesis we are testing below says Intercept - adult1 - gender1 = 0. This could be read like \"take the overall mean, add the effect for child/adult2 (-adult1=adult2), and add the effect for male/gender2 (-gender1=gender2). If you start at the overall mean f0 and add the effects for a male and a child, you are estimating the mean f0 for a boy. tapply (h95$f0, h95$group, mean) ## b g m w ## 236.0741 238.3509 131.2185 220.4010 ## intercept, boys, girls, men, women ## adult1 = &quot;adult&quot;, and gender1=&quot;child&quot; because its alphabetical. ## +adult1 = -adult2 since it will be the opposite value means_pred = hypothesis (model_both, c(&quot;Intercept = 0&quot;, &quot;Intercept - adult1 - gender1 = 0&quot;, &quot;Intercept - adult1 + gender1 = 0&quot;, &quot;Intercept + adult1 - gender1 = 0&quot;, &quot;Intercept + adult1 + gender1 = 0&quot;))[[1]][,1:5] means_pred ## Hypothesis Estimate Est.Error CI.Lower CI.Upper ## 1 (Intercept) = 0 209.2714 2.637054 204.1100 214.5586 ## 2 (Intercept-adult1-gender1) = 0 211.5077 4.849196 201.8424 220.8455 ## 3 (Intercept-adult1+gender1) = 0 272.7214 5.127175 262.5223 282.6433 ## 4 (Intercept+adult1-gender1) = 0 145.8213 4.037644 137.9749 153.7633 ## 5 (Intercept+adult1+gender1) = 0 207.0351 3.970953 199.2521 214.6761 The predictions above are actually not a good match for our group means, suggesting that maybe our model is not capturing something important about our data. 4.3.3 Investigating model fit So far we have been working with very simple models and not worrying much about how well they fit, meaning how well they represent our data. Our reconstruction of the group means above suggests our current model may have some issues. We can investigate model fit with what is called a posterior predictive check. The posterior predictions made by your model are the most probable values predicted by your model by each data point. Effectively, these are the \\(\\mu_{[i]}\\) predicted by your model for each trial. For example, if our current model looks like this: \\(\\mu_{[i]} = Intercept + adult_{adult_{[i]}} + gender_{gender_{[i]}} + \\alpha_{speaker_{[i]}}\\) Then the predicted value (\\(\\mu\\)) for each trial is the sum of the Intercept and appropriate coefficients for the trial. The brms package has a useful predict function which can help you make these predictions easily. y_pred = predict (model_both) y_pred_no_re = predict (model_both, re_formula = NA) head (y_pred) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 171.49420 12.22930 147.62523 195.5108 ## [2,] 107.18890 11.92348 83.56357 130.6007 ## [3,] 97.09596 12.25352 73.42253 120.9922 ## [4,] 133.31890 12.23698 110.05712 157.9008 ## [5,] 116.72685 12.44052 91.78161 140.5365 ## [6,] 99.68485 12.21819 76.22729 123.1292 You will note that the output of prediction, y_pred, has four columns, indicating a predicted value but also information about variation. That is because, actually, our model produces a different prediction for each set of samples. This means that if we have 4000 samples we actually have 4000 slightly different models and 4000 slightly different predictions! This means in addition to the most probable estimates, we can get information about expected variation around these estimates Above in the second line, I make predictions without random effects (re_formula = NA). This corresponds to a model like this without speaker adjustments: \\(\\mu_{[i]} = Intercept + adult_{adult_{[i]}} + gender_{gender_{[i]}}\\) These predictions will help us understand how well our model can represent average f0 without the speaker-dependent adjustments, based only on group averages. Below I plot both types of predictions. We can see that the model with speaker adjustments does a very good job of predicting f0, with most credible intervals for most predictions overlapping with the diagonal. On the other hand the predictions without speaker adjustments show systematic biases in prediction. It seems that the f0 of many high f0 speaker lie to the left of the diagonal, meaning the predictions are higher than they should be. par (mfrow = c(1,2), mar = c(4,4,1,1)) brmplot (xs = h95$f0, y_pred, xlim = range(h95$f0)); axis(side=1); mtext(side=1,text=&quot;f0&quot;,line=2.5);mtext(side=2,text=&quot;f0_pred&quot;,line=2.5) abline (0,1, col = 2, lwd=2) brmplot (xs = h95$f0, y_pred_no_re, xlim = range(h95$f0)); axis (side=1); mtext(side=1,text=&quot;f0&quot;,line=2.5);mtext(side=2,text=&quot;f0_pred&quot;,line=2.5) abline (0,1, col = 2, lwd=2) The above plots are good for basic analysis, but it is hard to get a good picture of how well our groups are being modeled. For that we need an interaction plot. 4.4 Interactions and interaction plots Interactions and understanding their graphical representations is so important that it gets its own subsection. In many if not most cases, researchers will need to at least consider the effects of interactions in their models. We can think of a single effect representing a difference between groups as a slope. For example, in the left panel below I have plotted the means for males and females at x-axis locations 0 and 1. The difference in the group means is 55 Hz (females 225 Hz, males 170 Hz). As a result, the line formed by joining these groups has a slope of -55 (i.e., it drops 55 Hz from 0 to 1). We can use any arbitrary x axis locations to calculate this slope that we want. However, there are obvious practical reasons why we choose to calculate these slopes over the arbitrary distance of 1. In the middle panel we see the effect for adultness, which shows a positive slope for the difference from adult to child: the f0 increases by about 60 Hz. The plots highlighting the effects for adultness and gender are main effects plots. You may have heard things like the analysis showed a significant main effect for so n so. Main effects are the average effects for one predictor averaged across everything else. Saying were averaging across everything else basically means we are ignoring it. A person looking only at the left plot would not realize our data also investigates the effect of adultness. Another way to think of main effects are that they are marginal effects. They are the overall average difference. Someone might ask you, whats the average difference between males and females in your sample? and you can respond 55 Hz. However, sometimes you cant just answer, sometimes the answer is actually well, it depends. This is one of those cases. On the right we see a two-way interaction plot. Interaction plots show you what are called the simple effects of your predictors. The simple effects are the conditional probabilities the effects of yor factor, conditional on the level of another factor. For example, the left plot shows the overall (marginal) effect for gender. The right plot also shows the effect for gender. However, it uses a blue line to show the effect for adults and a green line to show this effect for children. As a result, we can consider the effects of gender conditioned on adultness. par (mfrow = c(1,3), mar = c(4,4,3,2)) plot (0:1,tapply (h95$f0,h95$gender,mean), col = 3, ylim = c(130,280),xaxt=&#39;n&#39;, lwd=3,type=&#39;b&#39;,pch=16,cex=1.5,main=&quot;Gender&quot;,xlim=c(-.2,1.2),ylab=&#39;f0&#39;,xlab=&#39;&#39;) axis (at=0:1, labels = c(&#39;female&#39;,&#39;male&#39;), side=1) plot (0:1,tapply (h95$f0,h95$adult,mean), col = 3, ylim = c(130,280),xaxt=&#39;n&#39;, lwd=3,type=&#39;b&#39;,pch=16,cex=1.5, main = &quot;Adult&quot;,xlim=c(-.2,1.2),ylab=&#39;f0&#39;,xlab=&#39;&#39;) axis (at=0:1, labels = c(&#39;adult&#39;,&#39;child&#39;),side=1) interaction.plot (h95$gender, h95$adult, h95$f0,col=3:4, lwd = 3, type=&#39;b&#39;,pch=c(16,17),cex=1.5, ylim = c(130,280),main=&quot;Both&quot;,ylab=&#39;f0&#39;,xlab=&#39;&#39;) If adultness in no way affected the gender effect, the right panel should look identical to the left panel. If adultness affected f0 in the same way across genders, we would see parallel lines in the right plot. This is because if you are adding a single, consisent value to each gender based on adultness, the line would just slide up and down the f0 axis but would not change in slope. What we see in the right panel above is that the lines are not parallel at all. When we see lines that are not parallel, that means there may be an interaction in our data. In the absence of an interaction, we could just answer the question whats the average difference between males and females in your sample? with a number like 55 Hz. In the presence of an interaction we need to consider the conditional effects of each predictor, at the levels of the other predictor. I think the above may sound complicated, but its just what we would all do to try to make sense of the plot on the right. There is a large negative slope for gender for adults. This tells us that gender has a large effect on f0 for adults. However, the slope for gender is basically zero for children. So, we might say there is a large f0 difference for adult males and females in our sample, but basically no gender based difference for children. Alternatively, we might look at the changing effect for gender across adultness levels. If we did this, we might look say \"there is a small effect for adultness in the average f0 produced by females, but a very large effect for adultness for males. In summary, if the answer to what is the effect of X on Y is well, it depends on the values of another one of my predictors, you have an interaction in your data. When you have substantial interactions present in your data and you do not include these in your model, this can cause a problem for your model fit. 4.4.1 Interactions in our f0 data We can take the different posterior predictions we made above and make interaction plots out of them. We can see that when the model includes random effects, prediction is quite good. This is not surprising since the speaker-specific intercept adjustments allow for eah speakers mean f0 to be modeled effectively. However, we see that the predictions made by our model are substantially and systematically wrong in the absence of the speaker random effect. Its clear that the problem with our predictions in the right panel is that the lines are parallel. As weve just discussed, in the absence of interactions, interaction plots contain only parallel lines. Well, since our model (model_both) does not include interaction terms it cannot represent interactions, and so is only capable of producing effects that result in parallel lines! par (mfrow = c(1,3), mar = c(4,4,3,2)) interaction.plot (h95$gender, h95$adult, h95$f0, col = 3:4, ylim = c(130,280), lwd = 3, type = &#39;b&#39;, pch = c(16,17), cex = 1.5, main = &quot;Data&quot;) interaction.plot (h95$gender, h95$adult, y_pred[,1], col = 3:4,lwd=3, type=&#39;b&#39;, pch = c(16,17), cex = 1.5, ylim = c(130,280),main=&quot;Pred. with RE&quot;) interaction.plot (h95$gender, h95$adult, y_pred_no_re[,1],col=3:4, lwd = 3, type = &#39;b&#39;, pch = c(16,17), cex = 1.5, ylim = c(130,280),main=&quot;Pred. with RE&quot;) In order to properly model the group differences, and the real effects of gender and adultness on average f0, we need to build a model that can represent the interactions in our data. 4.5 Investigating interactions with a model This model requires only a slight tweak to our previous model. There are two ways to include interactions in R model formulas, as shown below: f0 ~ adult + gender + adult:gender + (1|speaker) f0 ~ adult * gender + (1|speaker) The first way includes an explicit interaction term, adult:gender. The syntax for these is X:Z for an interaction between effects X and Z, W:X:Z for a three-way interaction, and so on. The second way uses * between our two predictors. This tells R to include those predictors, and the interactions between them This can be much faster then specifying all interactions, but you lose control over which ones you include. For example the first formula implies the second, but cannot represent the third (since it omits one interaction): y ~ Z * X * W y ~ Z + X + W + Z:X + Z:W + X:W + Z:X:W y ~ Z + X + W + Z:X + X:W + Z:X:W Our full model specification now includes an interaction term that can help variation that cannot be explained by the independent effects of adultness and gender: \\[\\begin{equation} \\begin{split} \\textrm{Likelihood:} \\\\ y_{[i]} \\sim \\mathcal{N}(\\mu_{[i]},\\sigma_{error}) \\\\ \\mu_{[i]} = Intercept + adult_{\\textrm{adult}_{[i]}} + gender_{\\textrm{gender}_{[i]}}+ adult:gender + \\alpha_{speaker_{[i]}} \\\\\\\\ \\textrm{Priors:} \\\\ \\alpha_{speaker} \\sim \\mathcal{N}(0,\\sigma_{speaker}) \\\\ \\\\ Intercept \\sim t(3, 200, 100) \\\\ adult_{[i]} \\sim t(3, 0, 100) \\\\ gender_{[i]} \\sim t(3, 0, 100) \\\\ adult:gender \\sim t(3, 0, 100) \\\\ \\sigma_{error} \\sim t(3, 0, 100) \\\\ \\sigma_{speaker} \\sim t(3, 0, 100) \\\\ \\end{split} \\tag{4.3} \\end{equation}\\] 4.5.1 Fitting the model and interpreting the results Below I fit the model which now includes an interaction term investigating the changing effect of gender and adultness on average f0. set.seed (1) model_interaction = brm (f0 ~ adult + gender + adult:gender + (1|speaker), data = h95, chains = 4, cores = 4, warmup = 1000, iter = 11000, thin = 10, prior = c(set_prior(&quot;student_t(3, 200, 100)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 100)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 100)&quot;, class = &quot;sd&quot;))) saveRDS (model_interaction, &quot;4_model_interaction.RDS&quot;) ## load and inspect pre-fit model model_interaction = readRDS (&quot;4_model_interaction.RDS&quot;) model_interaction ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: f0 ~ adult + gender + adult:gender + (1 | speaker) ## Data: h95 (Number of observations: 1668) ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~speaker (Number of levels: 139) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 20.90 1.32 18.60 23.69 1.00 2712 3406 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 206.54 1.93 202.69 210.22 1.00 2341 3337 ## adult1 -30.63 1.94 -34.36 -26.79 1.00 2409 3133 ## gender1 22.80 1.94 18.99 26.57 1.00 2363 3190 ## adult1:gender1 21.71 1.95 17.96 25.57 1.00 2291 2991 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 11.87 0.21 11.46 12.27 1.00 3857 3818 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Remember that this line set_prior(\"student_t(3, 0, 100)\", class = \"b\") sets the prior for all non-intercept Population-Level predictors. This allows you to efficiently set priors for the adult, gender, and adult:gender predictors in your model in a single line, and becomes more and more useful as our models grow more complex. A look at the model output above indicates that we have a large interaction term. In fact, our interaction is as large as the main effect for gender! In cases with large interactions we have to be careful about interpreting the main effects. In other words, when the answer to a question is it depends, you should be wary of making blanket statements. We need to talk about why there is only a single interaction term. The reason for this is related to the same reason we cant get estimates for all our group effect (linear dependence). The number of terms you can estimate is generally one fewer than the number of levels. For interaction terms, the number of parameters is equal to (number of levels of factor A - 1)x(number of levels of factor B - 1). Since each of our factors have two levels, we can only estimate one parameter, (2-1)x(2-1). The interaction term is just another element of your prediction equation (i.e., \\(\\mu + x_1+x_2...\\)) intended to help predict variation that cant be predicted by the independent effects of the other predictors in the model. Recovering the predicted group means based on the coefficient values is again tedious but straightforward. The only difference compared to our previous approach is that we must now either add or subtract the value of the interaction term, adult1:gender1 from each group. We can easily determine which to do for this model because the sign on the interaction term is the product of the signs on the relevant main effects terms. For example, the second hypothesis we are testing below says Intercept - adult1 - gender1 + adult1:gender1 = 0. This could be read like take the overall mean, add the effect for child (or adult2, since -adult1=adult2), and add the effect for male (or gender2 since -gender1=gender2), and add the effect for when the speaker is male and a child (-adult1*-gender1 = +adult1:gender1). If you look at the hypotheses below, you will see that the sign on the interaction terms solely depends on the signs of the corresponding main effects terms. We can see that the inclusion of an interaction term allows our model to capture group averages more accurately than the model without intercepts. tapply (h95$f0, h95$group, mean) ## b g m w ## 236.0741 238.3509 131.2185 220.4010 ## intercept, boys, girls, men, women means_pred_interaction = hypothesis (model_interaction, c(&quot;Intercept = 0&quot;, &quot;Intercept - adult1 - gender1 + adult1:gender1 = 0&quot;, &quot;Intercept - adult1 + gender1 - adult1:gender1 = 0&quot;, &quot;Intercept + adult1 - gender1 - adult1:gender1 = 0&quot;, &quot;Intercept + adult1 + gender1 + adult1:gender1 = 0&quot;))[[1]][,1:5] means_pred ## Hypothesis Estimate Est.Error CI.Lower CI.Upper ## 1 (Intercept) = 0 209.2714 2.637054 204.1100 214.5586 ## 2 (Intercept-adult1-gender1) = 0 211.5077 4.849196 201.8424 220.8455 ## 3 (Intercept-adult1+gender1) = 0 272.7214 5.127175 262.5223 282.6433 ## 4 (Intercept+adult1-gender1) = 0 145.8213 4.037644 137.9749 153.7633 ## 5 (Intercept+adult1+gender1) = 0 207.0351 3.970953 199.2521 214.6761 means_pred_interaction ## Hypothesis Estimate Est.Error CI.Lower ## 1 (Intercept) = 0 206.5369 1.931096 202.6886 ## 2 (Intercept-adult1-gender1+adult1:gender1) = 0 236.0827 4.065229 228.1277 ## 3 (Intercept-adult1+gender1-adult1:gender1) = 0 238.2590 4.924929 228.2973 ## 4 (Intercept+adult1-gender1-adult1:gender1) = 0 131.3907 3.152187 125.2071 ## 5 (Intercept+adult1+gender1+adult1:gender1) = 0 220.4150 3.108604 214.1694 ## CI.Upper ## 1 210.2240 ## 2 244.0703 ## 3 247.7889 ## 4 137.5445 ## 5 226.4643 Below I print the estimates of the fixed effects in the model so we can focus on those. If you fit a model like this and are having trouble interpreting it, I would really encourage you to write down an interpretation using pen and paper, by focusing on the decomposition of values provided by the regression model. fixef (model_interaction) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 206.53685 1.931096 202.68860 210.22397 ## adult1 -30.63400 1.943657 -34.35968 -26.78826 ## gender1 22.80015 1.943186 18.99251 26.56927 ## adult1:gender1 21.71202 1.952504 17.96248 25.56939 For example, the average f0 is 206. The gender difference is 44 Hz (22 * 2) between groups, and there is a gender-based 23 Hz deviation from the mean between groups. However, the adult1:gender1 interaction is 22 Hz. This means that when the speaker was adult (adult1), the gender difference was nearly doubled. We know this because the effect for gender1 is 22.8, and the effect for gender1 given adult1 (adult1:gender1) is a further 21.7 Hz higher than that. So, the total effect for f0 across adults is about 22.8 + 21.7 = 44.5, suggesting a difference in f0 between the groups of 89 Hz (44.5 * 2). In contrast, the fact that adult1:gender1 = 21.7 suggests that adult2:gender1 = -21.7. So, we can say that although the effect for gender is 22.8 overall, given that the speaker is a child (adult2), this effect is 21.7 Hz lower than the main effect estimate. So, for children we expect an f0 effect of 22.8 - 21.7 = 1.1, suggesting a group difference of 2.2 Hz. In other words, the effect of gender is basically zero given that the speakers are children. We can consider the effects the other way. The marginal effect for adult1 is -30, meaning that when speakers are adults, their f0 is 30 Hz lower than the overall average. However, the interaction is 21.7 Hz. This means that if the speaker is female (gender1), then the expected effect of adultness is reduced (-30 + 21.7 = -8.3) so that the expected group difference is only 17 Hz. On the other hands, when the speaker is a male (gender2) then the interaction term should be flipped in sign (adult1:gender2 = 21.7). This means that the effect of adultness, conditional on the speakre being male is nearly doubled (-30 + -21.7 = -51.7). When considered in this way all these coefficients are just telling us what we already knew from looking at the interaction plot: f0 varies substantially based on gender for adults but not for children. 4.5.2 Assessing model fit We can asses model fit for the model including interaction terms. y_pred_int = predict (model_interaction) y_pred_no_re_int = predict (model_interaction, re_formula = NA) head (y_pred_int) ## Estimate Est.Error Q2.5 Q97.5 ## [1,] 170.98864 12.38839 146.16052 194.8080 ## [2,] 107.35424 12.34343 83.29906 132.1596 ## [3,] 97.04254 12.37383 72.62049 121.5229 ## [4,] 133.38287 12.41807 109.24117 157.3635 ## [5,] 116.94452 12.26171 92.70863 140.4540 ## [6,] 99.99772 12.47452 75.65962 123.7280 We can again make interaction plots using our data and our posterior predictions. Below I compare our data, the predictions of our original model, and the predictions of our model that includes interactions. Whereas the model with no interaction forced parallelism on the effects, our new model is able to capture the conditional nature of gender and adultness on our data. par (mfrow = c(1,3), mar = c(4,4,3,2)) interaction.plot (h95$gender, h95$adult, h95$f0, col = 3:4, ylim = c(130,280), lwd = 3, type = &#39;b&#39;, pch = c(16,17), cex = 1.5, main = &quot;Data&quot;) interaction.plot (h95$gender, h95$adult, y_pred_no_re[,1], col = 3:4,lwd=3, type=&#39;b&#39;, pch = c(16,17), cex = 1.5, ylim = c(130,280),main=&quot;No Int., no RE&quot;) interaction.plot (h95$gender, h95$adult, y_pred_no_re_int[,1],col=3:4, lwd = 3, type = &#39;b&#39;, pch = c(16,17), cex = 1.5, ylim = c(130,280),main=&quot;With Int., no RE&quot;) For the first time, we have a model that really does a reasonably-good job of representing the information in our data. The model can capture the gender-dependent nature of age-based f0 differences, and separately estimates between group variation and between group variation. 4.5.3 Investigating the interactions the easy way There are built-in functions in brms that help you investigate you effects in the presence of interactions. The conditional_effects function calculates the values of your effects conditional on the mean (for continuous variables). This means it adds the value of the Intercept to all its effects estimates. This function is extremely handy but also limited. You cant get the posterior samples from this function, meaning you cant use this output to compare the values of rows. Also, conditioning on the mean introduces noise to our effects estimates and makes their credible intervals larger. In some cases variability in the estimate of the mean can make effects seem small relative to the error in the estimates themselves. tapply (h95$f0, h95$group, mean) ## b g m w ## 236.0741 238.3509 131.2185 220.4010 ## intercept, boys, girls, men, women model_effects = conditional_effects (model_interaction) model_effects[[1]] ## adult f0 gender speaker cond__ effect1__ estimate__ se__ lower__ ## 1 adult 197.027 female NA 1 adult 220.4292 3.102745 214.1694 ## 2 child 197.027 female NA 1 child 238.2584 4.904636 228.2973 ## upper__ ## 1 226.4643 ## 2 247.7889 model_effects[[2]] ## gender f0 adult speaker cond__ effect1__ estimate__ se__ lower__ ## 1 female 197.027 adult NA 1 female 220.4292 3.102745 214.1694 ## 2 male 197.027 adult NA 1 male 131.4076 3.239214 125.2071 ## upper__ ## 1 226.4643 ## 2 137.5445 model_effects[[3]] ## adult gender f0 speaker cond__ effect1__ effect2__ estimate__ se__ ## 1 adult female 197.027 NA 1 adult female 220.4292 3.102745 ## 2 adult male 197.027 NA 1 adult male 131.4076 3.239214 ## 3 child female 197.027 NA 1 child female 238.2584 4.904636 ## 4 child male 197.027 NA 1 child male 236.0642 4.096959 ## lower__ upper__ ## 1 214.1694 226.4643 ## 2 125.2071 137.5445 ## 3 228.2973 247.7889 ## 4 228.1277 244.0703 We can see that these values match those we recreated above using thehypothesis function and the individual coefficients: tapply (h95$f0, h95$group, mean) ## b g m w ## 236.0741 238.3509 131.2185 220.4010 ## intercept, boys, girls, men, women hypothesis (model_interaction, c(&quot;Intercept = 0&quot;, &quot;Intercept - adult1 - gender1 + adult1:gender1 = 0&quot;, &quot;Intercept - adult1 + gender1 - adult1:gender1 = 0&quot;, &quot;Intercept + adult1 - gender1 - adult1:gender1 = 0&quot;, &quot;Intercept + adult1 + gender1 + adult1:gender1 = 0&quot;))[[1]][,1:5] ## Hypothesis Estimate Est.Error CI.Lower ## 1 (Intercept) = 0 206.5369 1.931096 202.6886 ## 2 (Intercept-adult1-gender1+adult1:gender1) = 0 236.0827 4.065229 228.1277 ## 3 (Intercept-adult1+gender1-adult1:gender1) = 0 238.2590 4.924929 228.2973 ## 4 (Intercept+adult1-gender1-adult1:gender1) = 0 131.3907 3.152187 125.2071 ## 5 (Intercept+adult1+gender1+adult1:gender1) = 0 220.4150 3.108604 214.1694 ## CI.Upper ## 1 210.2240 ## 2 244.0703 ## 3 247.7889 ## 4 137.5445 ## 5 226.4643 4.5.4 Making plots There are many ways to make nice graphics using brms models. Many use packages like ggplot2 and bayesplot. In general these figures are nice but too fancy for many purposes. I also dont really know how to use ggplot2. As a result, I am going to focus on making simple line plots (like those common in journal article) using base R graphics. The standard information provided in the output of brm model summaries can be used to make plots easily. I wrote a small function called brmplot that will help draw effects plots easily using these summaries. The standard format of these summaries is that they consist of four columns, corresponding to the mean estimate, the standard deviation, and the lower and upper credible intervals. The brmplot function takes in a matrix like this and makes plots showing the means and credible intervals of different effects. Below I get a summary of the fixed effects. I make plots of these effects in two orientations. In each one I omit the Intercept estimate as the magnitude of this is so different that it cannot easily be included on the plot. fixef_interaction = fixef (model_interaction) fixef_interaction ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 206.53685 1.931096 202.68860 210.22397 ## adult1 -30.63400 1.943657 -34.35968 -26.78826 ## gender1 22.80015 1.943186 18.99251 26.56927 ## adult1:gender1 21.71202 1.952504 17.96248 25.56939 par (mfrow =c(1,2), mar = c(8,3,1,1)) brmplot (fixef_interaction[-1,]) ; abline (h = 0,lty=3) par (mar = c(3,8,1,1)) brmplot (fixef_interaction[-1,], horizontal = FALSE) ; abline (v=0,lty=3) In the left panel below, I use the effects calculated using the marginal_effects function to draw figures using the brmplot function. First I isolate the columns that represent the information I need. Then I plot the effects using repeated calls to brmplot. I can add a second plot over the first by setting add = TRUE. On the right, I do the same thing using the output of the hypothesis function. This will be a plot of the group effects centered at 0, since I have not added the overall Intercept to the means. Note that to get the table of interest out of the hypothesis object we have to first select the element named hypothesis from the group_effects object. In general, you may have to dig around in the structure of the objects created by brms to get the information you want. model_effects[[3]][,8:11] ## estimate__ se__ lower__ upper__ ## 1 220.4292 3.102745 214.1694 226.4643 ## 2 131.4076 3.239214 125.2071 137.5445 ## 3 238.2584 4.904636 228.2973 247.7889 ## 4 236.0642 4.096959 228.1277 244.0703 par (mfrow = c(1,2), mar = c(4,4,1,1)) brmplot (model_effects[[3]][1:2,8:11], type = &#39;b&#39;, ylim = c(120,250), labels=c(&quot;Female&quot;,&quot;Male&quot;), col = 3, xlim = c(.8,2.2)) brmplot (model_effects[[3]][3:4,8:11], type = &#39;b&#39;, add = TRUE, col = 4, labels=&quot;&quot;, pch=17) group_effects = hypothesis (model_interaction, c(&quot;Intercept = 0&quot;, &quot;Intercept - adult1 - gender1 + adult1:gender1 = 0&quot;, &quot;Intercept - adult1 + gender1 - adult1:gender1 = 0&quot;, &quot;Intercept + adult1 - gender1 - adult1:gender1 = 0&quot;, &quot;Intercept + adult1 + gender1 + adult1:gender1 = 0&quot;)) group_effects[[&quot;hypothesis&quot;]][,2:5] ## Estimate Est.Error CI.Lower CI.Upper ## 1 206.5369 1.931096 202.6886 210.2240 ## 2 236.0827 4.065229 228.1277 244.0703 ## 3 238.2590 4.924929 228.2973 247.7889 ## 4 131.3907 3.152187 125.2071 137.5445 ## 5 220.4150 3.108604 214.1694 226.4643 brmplot (group_effects[[&quot;hypothesis&quot;]][5:4,2:5], type = &#39;b&#39;, ylim = c(120,250), labels=c(&quot;Female&quot;,&quot;Male&quot;), col = 3, xlim = c(.8,2.2)) brmplot (group_effects[[&quot;hypothesis&quot;]][3:2,2:5], type = &#39;b&#39;, add = TRUE, col = 4, labels=&quot;&quot;, pch=17) "]]
