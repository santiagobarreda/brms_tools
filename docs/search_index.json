[["modeling-variances.html", "Chapter 10 Modeling Variances 10.1 Data 10.2 Modeling variation in the error variance 10.3 Modeling group variation in our random effects 10.4 Plot Code", " Chapter 10 Modeling Variances In this chapter were going to consider models with more complicated variance structures thn anything we have considered so far. If you think about our models, we have covered increasingly complicated ways of estimating the value of \\(\\mu\\) from trial to trial. In this chapter we are going to talk about allowing the values of quantities like \\(\\sigma_{error}\\) and \\(\\sigma_{speaker}\\) vary in our model. 10.1 Data Were going to continue working with the same data and variables described at the top of the previous chapter. We load the data below: library (brms) # data setup ################################################# # source data url1 = &quot;https://raw.githubusercontent.com/santiagobarreda&quot; url2 = &quot;/stats-class/master/data/h95_experiment_data.csv&quot; h95 = read.csv (url(paste0 (url1, url2))) # set up colors for plotting devtools::source_url (paste0 (url1, &quot;/stats-class/master/data/colors.R&quot;)) # source functions devtools::source_url (paste0 (url1, &quot;/stats-class/master/data/functions.R&quot;)) 10.2 Modeling variation in the error variance In traditional models we assume that our data has what is called a homoscedastic variance. This means that every single data point in our data is drawn from a normal distribution with a fixed variance as below: \\[ \\begin{equation} \\begin{split} y_{i} \\sim \\mathrm{Normal}(\\mu_{i}, \\sigma) \\end{split} \\tag{10.1} \\end{equation} \\] So, even though the mean is allowed to vary from trial to trial, the variance is not. This is an extremely important and basic assumption made by all traditional statistical models (including lmer). Why is this assumption made? Basically because it had to be made in order to obtain solutions to the regression equation using traditional methods. However, Bayesian models are not constrained in this way: the variance is just another parameter. Below, we see data that is drawn from a normal distribution with a varying mean and a varying standard deviation. This distribution slides around the number line generating number as it goes, but also getting narrower and broader while it does so. \\[ \\begin{equation} \\begin{split} y \\sim \\mathrm{Normal}(\\mu_{i}, \\sigma_{error[i]}) \\end{split} \\tag{10.2} \\end{equation} \\] These sorts of models are very easy to fit in brms, and they can potentially provide better fits to our data and provide us with useful information. 10.2.1 Fitting and interpreting our model Our model is now going to feature two formulas, one for \\(\\mu\\) and another for \\(\\sigma_{error[i]}\\) sigma parameter. brms makes it incredibly easy to do this by letting you just write both formulas and stick them together inside the brmsformula function. Below I use the model formula from the model we fit at the end of the last chapter and add a second line. The second line says we are also going to let the parameter sigma (i.e., \\(\\sigma_{speaker}\\)) vary in a speaker-dependent manner. Notice that this is just what we would have done to estimate a model with only and intercept, and random by-subject intercepts. model_formula = brmsformula ( # data formula pheight ~ (g0_s + gbar_s) * padult * pgender + vowel + ((g0_s + gbar_s) * padult * pgender + vowel | subj) + (1 | speaker) , # data formula sigma ~ (1|subj) ) We could have made this even more complicated, for example, by including differences in expected errors according to perceived adultness. Such a model would look like: sigma ~ padult + (1|subj) Were not going to fit that model, but I just want to point out that your modeling of variance components can also include multiple predictors and random effects, just like the modeling of mean values. Below I prepare the data: # standardize log f0 h95$g0_s = (h95$g0-mean(h95$g0)) / sd(h95$g0) # standardize log geometric mean formant frequency h95$gbar_s = (h95$gbar-mean(h95$gbar)) / sd(h95$gbar) # create gender and adultness variables h95$pgender = c(&#39;m&#39;,&#39;w&#39;)[h95$pgroup %in% c(&#39;g&#39;,&#39;w&#39;)+1] h95$padult = c(&#39;c&#39;,&#39;a&#39;)[h95$pgroup %in% c(&#39;m&#39;,&#39;w&#39;)+1] And fit our model. Notice that instead of supplying the model formula, I supply the object called model_formula that I created above. library (brms) options (contrasts = c(&quot;contr.sum&quot;,&quot;cont.sum&quot;)) set.seed (1) height_perception_het = brms::brm (model_formula, data=h95, chains=4, cores=4, warmup=1000, iter = 6000, thin = 4, control = list(adapt_delta = 0.95), family = &#39;student&#39;, prior = c(set_prior(&quot;student_t(60, 0, 12)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 6)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 6)&quot;, class = &quot;sd&quot;), set_prior(&quot;lkj_corr_cholesky (2)&quot;, class = &quot;cor&quot;))) # save model # saveRDS (height_perception_het, &#39;9_height_perception_het.RDS&#39;) The print statement is way too long to show here, but heres the last chunk. Notice that sigma is no longer in our Family Specific Parameters section. Instead, we have sigma_Intercept in the Population-Level Effects section. Also notice that the value is now a lot smaller than it was in our last model around 2.2). Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Intercept 61.17 0.65 59.83 62.46 1.00 3347 4014 sigma_Intercept 0.79 0.09 0.61 0.96 1.00 3846 3606 g0_s -1.47 0.36 -2.19 -0.77 1.00 4276 4230 gbar_s -2.88 0.34 -3.57 -2.19 1.00 4486 4186 padult1 3.67 0.58 2.48 4.82 1.00 4131 4361 pgender1 -0.91 0.35 -1.61 -0.22 1.00 4620 4715 vowel1 -0.23 0.07 -0.36 -0.09 1.00 4905 4885 g0_s:padult1 0.02 0.23 -0.45 0.47 1.00 4902 4791 gbar_s:padult1 0.85 0.25 0.38 1.36 1.00 4720 4634 g0_s:pgender1 -0.31 0.18 -0.67 0.04 1.00 4851 4698 gbar_s:pgender1 0.22 0.20 -0.17 0.62 1.00 4312 4705 padult1:pgender1 0.31 0.33 -0.33 0.98 1.00 4787 4663 g0_s:padult1:pgender1 -0.10 0.19 -0.49 0.28 1.00 4682 4429 gbar_s:padult1:pgender1 -0.51 0.19 -0.88 -0.15 1.00 4593 4792 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS nu 7.83 1.30 5.83 10.87 1.00 4947 4764 Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). Our sigma_Intercept parameter represents the average sgima parameter for all subjects. This is the same information that sigma used to give us in our previous models. The reason the value is so low is because brms models variance components by log-transforming the. So, to get the actual mean and credible interval we need to exponentiate the values. # log-transformed standard deviation fixef(height_perception_het, pars=&quot;sigma_Intercept&quot;) ## Estimate Est.Error Q2.5 Q97.5 ## sigma_Intercept 0.7860117 0.09058804 0.6092945 0.9633645 # standard deviation exp ( fixef(height_perception_het, pars=&quot;sigma_Intercept&quot;) ) ## Estimate Est.Error Q2.5 Q97.5 ## sigma_Intercept 2.194626 1.094818 1.839133 2.620498 We can use the hypothesis function to get both the sigma random effects, and the subject-specific sigmas. To do this, you need to set the group parameter to the random effect cluster (in this case subj). Then the scope parameter can be set to ranef to get just the random effects, or to coeff to get the sum of the coefficient across the fixed and random effect. fixef (height_perception_het) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 61.16697079 0.65376724 59.8254096 62.45618022 ## sigma_Intercept 0.78601175 0.09058804 0.6092945 0.96336447 ## g0_s -1.46906050 0.35912869 -2.1902060 -0.77295635 ## gbar_s -2.88328078 0.34241384 -3.5651700 -2.19007218 ## padult1 3.67082384 0.58168587 2.4841210 4.82026659 ## pgender1 -0.91030822 0.35253976 -1.6069869 -0.21804147 ## vowel1 -0.22621242 0.06724439 -0.3581328 -0.09474008 ## g0_s:padult1 0.01543639 0.23083817 -0.4474999 0.47144895 ## gbar_s:padult1 0.85072185 0.24890700 0.3769474 1.36054163 ## g0_s:pgender1 -0.31054478 0.17856677 -0.6708868 0.03852319 ## gbar_s:pgender1 0.22084086 0.19892341 -0.1662271 0.62406929 ## padult1:pgender1 0.30667174 0.32816711 -0.3265976 0.97984060 ## g0_s:padult1:pgender1 -0.09586683 0.19411646 -0.4872256 0.27522748 ## gbar_s:padult1:pgender1 -0.51079963 0.18651495 -0.8759576 -0.14628872 # the sum of the sigma random effect and the sigma intercept # sigma_Intercept + sigma_Intercept:subj subj_sigma_ranefs = hypothesis (height_perception_het, &quot;sigma_Intercept=0&quot;, scope = &#39;ranef&#39;, group = &quot;subj&quot;) # the sum of the sigma random effect and the sigma intercept # sigma_Intercept + sigma_Intercept:subj subj_sigmas = hypothesis (height_perception_het, &quot;sigma_Intercept=0&quot;, scope = &#39;coef&#39;, group = &quot;subj&quot;) We can plot these below using brmplot. When we just consider the random effects like this, they look a lot like the random slopes we considered in chapter 6. Thats basically because as far as our model is concerned, they are not that different: they are just subject-specific parameter estimates. Figure 10.1: (left) The subject-specific random sigma deviations (in lof units). (middle) Subject specific subject-specific sigmas (random effects + sigma Intercept) on a log scale. (right) The same information as in the middle panel but values have been exponentiated. Looking at our model above its clear that there is heteroscedastic variation in our model, meaning that there is substantial between-subject variation in our \\(\\sigma_{error}\\) parameter. But what does this mean? The error standard deviation is the distribution of noise around our lines and planes. So, differing \\(\\sigma_{error}\\) parameters reflect different amount of predictability for different subjects. You can get an idea of what this means below. Each plane is the overall plane for a different subject in the experiment based on that subjects random effects, and the points of the same color are that subjects individual responses. The residuals are actually calculated with respect to different planes based on the other predictors (so, different planes for adult and child responses as shown in the previous chapter). However, you can more or less see that the green points are closer to the green plane than the orange points are to the orange plane. Figure 10.2: A comparison of data from different subjects to the overall planes suggested by their subject-specific intercept and slopes terms. 10.3 Modeling group variation in our random effects 10.3.1 Describing and fitting the model Our model formula is going to be as below. Note that padult and pgender interact with our continuous predictors, but vowel does not. This is because I think its reasonable that perceived gender or adultness affects the use of acoustic cues. However, I think vowel category will probably not have such a complicated effect and so it is included only as an intercept. pheight ~ (g0_s + gbar_s) * padult * pgender + vowel + ((g0_s + gbar_s) * padult * pgender + vowel | subj) + (1 | speaker) Which in plain English mains: Were predicting perceived height using two continuous predictors (g0_s and gbar_s). The slopes of these planes are allowed to vary according to padult, pgender, and the interaction of the two. Our model includes intercept shifts for these plains according to padult, pgender, the interaction of the two, and vowel. All of the aforementioned effects are included as random by-subject effects, and the model includes random by-speaker intercepts as well. Below, I set up the necessary variables in our data: # standardize log f0 h95$g0_s = (h95$g0-mean(h95$g0)) / sd(h95$g0) # standardize log geometric mean formant frequency h95$gbar_s = (h95$gbar-mean(h95$gbar)) / sd(h95$gbar) # create gender and adultness variables h95$pgender = c(&#39;m&#39;,&#39;w&#39;)[h95$pgroup %in% c(&#39;g&#39;,&#39;w&#39;)+1] h95$padult = c(&#39;c&#39;,&#39;a&#39;)[h95$pgroup %in% c(&#39;m&#39;,&#39;w&#39;)+1] And fit our model. library (brms) options (contrasts = c(&quot;contr.sum&quot;,&quot;cont.sum&quot;)) set.seed (1) adultness_group_sds = brms::brm (padult ~ (g0_s + gbar_s)*pgender+vowel + ((g0_s + gbar_s)*pgender+vowel|subj) + (1|gr(speaker, by = group)), data=h95, chains=4, cores=4, warmup=1000, iter = 6000, thin = 4, control = list(adapt_delta = 0.95), family = &quot;bernoulli&quot;, prior = c(set_prior(&quot;student_t(60, 0, 12)&quot;, class = &quot;Intercept&quot;), set_prior(&quot;student_t(3, 0, 6)&quot;, class = &quot;b&quot;), set_prior(&quot;student_t(3, 0, 6)&quot;, class = &quot;sd&quot;), set_prior(&quot;lkj_corr_cholesky (2)&quot;, class = &quot;cor&quot;))) # save model saveRDS (adultness_group_sds, &#39;10_adultness_group_sds.RDS&#39;) 10.3.2 Interpreting the model #baov = banova(normal_g0_gbar) #par (mar = c(13,4,1,1)) #banovaplot (baov[-2,], las = 2) 10.4 Plot Code "]]
