<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Logistic regression | A Quick Introduction to Multilevel Bayesian Models for Linguistic Researchers</title>
  <meta name="description" content="Bayesian Models for Linguists" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Logistic regression | A Quick Introduction to Multilevel Bayesian Models for Linguistic Researchers" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://santiagobarreda.com" />
  
  <meta property="og:description" content="Bayesian Models for Linguists" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Logistic regression | A Quick Introduction to Multilevel Bayesian Models for Linguistic Researchers" />
  
  <meta name="twitter:description" content="Bayesian Models for Linguists" />
  

<meta name="author" content="Santiago Bareda" />


<meta name="date" content="2021-03-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="random-slopes.html"/>
<link rel="next" href="bayesian-anova-and-interpreting-complicated-models.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.3/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Models for Linguists</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html"><i class="fa fa-check"></i><b>1</b> Inspecting a single group of observations</a>
<ul>
<li class="chapter" data-level="1.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#data-and-research-questions"><i class="fa fa-check"></i><b>1.1</b> Data and research questions</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#inspecting-the-central-location-and-spread-of-values"><i class="fa fa-check"></i><b>1.1.1</b> Inspecting the central location and spread of values</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#probability-distributions"><i class="fa fa-check"></i><b>1.2</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#the-normal-distribution"><i class="fa fa-check"></i><b>1.2.1</b> The normal distribution</a></li>
<li class="chapter" data-level="1.2.2" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#referring-to-the-normal-distribution-to-make-inferences"><i class="fa fa-check"></i><b>1.2.2</b> Referring to the normal distribution to make inferences</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#probabilities-of-events-and-likelihoods-of-parameters"><i class="fa fa-check"></i><b>1.3</b> Probabilities of events and likelihoods of parameters</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#making-inferences-using-likelihoods"><i class="fa fa-check"></i><b>1.3.1</b> Making inferences using likelihoods</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#bayesian-models"><i class="fa fa-check"></i><b>1.4</b> Bayesian models</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#what-are-regression-models"><i class="fa fa-check"></i><b>1.4.1</b> What are regression models?</a></li>
<li class="chapter" data-level="1.4.2" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#whats-bayesian-about-these-models"><i class="fa fa-check"></i><b>1.4.2</b> What’s ‘Bayesian’ about these models?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#posterior-distributions"><i class="fa fa-check"></i><b>1.5</b> Posterior distributions</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#sampling-from-the-posterior"><i class="fa fa-check"></i><b>1.5.1</b> Sampling from the posterior</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="inspecting-a-single-group-of-observations.html"><a href="inspecting-a-single-group-of-observations.html#plot-code"><i class="fa fa-check"></i><b>1.6</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><i class="fa fa-check"></i><b>2</b> Inspecting a ‘single group’ of observations using a Bayesian multilevel model</a>
<ul>
<li class="chapter" data-level="2.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#data-and-research-questions-1"><i class="fa fa-check"></i><b>2.1</b> Data and research questions</a></li>
<li class="chapter" data-level="2.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#estimating-a-single-mean-with-the-brms-package"><i class="fa fa-check"></i><b>2.2</b> Estimating a single mean with the <code>brms</code> package</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#description-of-the-model"><i class="fa fa-check"></i><b>2.2.1</b> Description of the model</a></li>
<li class="chapter" data-level="2.2.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#the-model-formula"><i class="fa fa-check"></i><b>2.2.2</b> The model formula</a></li>
<li class="chapter" data-level="2.2.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#fitting-the-model-calling-the-brm-function"><i class="fa fa-check"></i><b>2.2.3</b> Fitting the model: Calling the <code>brm</code> function</a></li>
<li class="chapter" data-level="2.2.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#interpreting-the-model-the-print-statement"><i class="fa fa-check"></i><b>2.2.4</b> Interpreting the model: the print statement</a></li>
<li class="chapter" data-level="2.2.5" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#seeing-the-samples"><i class="fa fa-check"></i><b>2.2.5</b> Seeing the samples</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#repeated-measures-data"><i class="fa fa-check"></i><b>2.3</b> Repeated measures data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#multilevel-models"><i class="fa fa-check"></i><b>2.3.1</b> Multilevel models</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#estimating-a-multilevel-model-with-brms"><i class="fa fa-check"></i><b>2.4</b> Estimating a multilevel model with <code>brms</code></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#description-of-the-model-1"><i class="fa fa-check"></i><b>2.4.1</b> Description of the model</a></li>
<li class="chapter" data-level="2.4.2" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#fitting-the-model"><i class="fa fa-check"></i><b>2.4.2</b> Fitting the model</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#checking-model-convergence"><i class="fa fa-check"></i><b>2.5</b> Checking model convergence</a></li>
<li class="chapter" data-level="2.6" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#specifying-prior-probabilities"><i class="fa fa-check"></i><b>2.6</b> Specifying prior probabilities</a></li>
<li class="chapter" data-level="2.7" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#answering-our-research-questions"><i class="fa fa-check"></i><b>2.7</b> Answering our research questions</a></li>
<li class="chapter" data-level="2.8" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#simulating-data-using-our-model-parameters"><i class="fa fa-check"></i><b>2.8</b> Simulating data using our model parameters</a></li>
<li class="chapter" data-level="2.9" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#lmer-corner"><i class="fa fa-check"></i><b>2.9</b> Lmer corner</a></li>
<li class="chapter" data-level="2.10" data-path="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html"><a href="inspecting-a-single-group-of-observations-using-a-bayesian-multilevel-model.html#plot-code-1"><i class="fa fa-check"></i><b>2.10</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html"><i class="fa fa-check"></i><b>3</b> Comparing two groups of observations</a>
<ul>
<li class="chapter" data-level="3.1" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#data-and-research-questions-2"><i class="fa fa-check"></i><b>3.1</b> Data and research questions</a></li>
<li class="chapter" data-level="3.2" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#estimating-the-difference-between-two-means-with-brms"><i class="fa fa-check"></i><b>3.2</b> Estimating the difference between two means with ‘brms’</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#fitting-the-model-1"><i class="fa fa-check"></i><b>3.2.1</b> Fitting the model</a></li>
<li class="chapter" data-level="3.2.2" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#interpreting-the-model"><i class="fa fa-check"></i><b>3.2.2</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#contrasts"><i class="fa fa-check"></i><b>3.3</b> Contrasts</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#treatment-coding"><i class="fa fa-check"></i><b>3.3.1</b> Treatment coding</a></li>
<li class="chapter" data-level="3.3.2" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#sum-coding"><i class="fa fa-check"></i><b>3.3.2</b> Sum coding</a></li>
<li class="chapter" data-level="3.3.3" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#comparison-of-sum-and-treatment-coding"><i class="fa fa-check"></i><b>3.3.3</b> Comparison of sum and treatment coding</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#refitting-the-model-with-sum-coding"><i class="fa fa-check"></i><b>3.4</b> Refitting the model with sum coding</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#fitting-the-model-2"><i class="fa fa-check"></i><b>3.4.1</b> Fitting the model</a></li>
<li class="chapter" data-level="3.4.2" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#description-of-the-model-2"><i class="fa fa-check"></i><b>3.4.2</b> Description of the model</a></li>
<li class="chapter" data-level="3.4.3" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#interpreting-the-model-1"><i class="fa fa-check"></i><b>3.4.3</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#random-effects"><i class="fa fa-check"></i><b>3.5</b> ‘Random’ Effects</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#random-effects-priors-and-pooling"><i class="fa fa-check"></i><b>3.5.1</b> Random effects, priors and pooling</a></li>
<li class="chapter" data-level="3.5.2" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#inspecting-the-random-effects"><i class="fa fa-check"></i><b>3.5.2</b> Inspecting the random effects</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#but-what-does-it-all-mean"><i class="fa fa-check"></i><b>3.6</b> But what does it all mean?</a></li>
<li class="chapter" data-level="3.7" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#simulating-the-two-group-model"><i class="fa fa-check"></i><b>3.7</b> Simulating the two-group model</a></li>
<li class="chapter" data-level="3.8" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#lmer-corner-1"><i class="fa fa-check"></i><b>3.8</b> Lmer corner</a></li>
<li class="chapter" data-level="3.9" data-path="comparing-two-groups-of-observations.html"><a href="comparing-two-groups-of-observations.html#plot-code-2"><i class="fa fa-check"></i><b>3.9</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html"><i class="fa fa-check"></i><b>4</b> Comparing many groups</a>
<ul>
<li class="chapter" data-level="4.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#data-and-research-questions-3"><i class="fa fa-check"></i><b>4.1</b> Data and research questions</a></li>
<li class="chapter" data-level="4.2" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#comparing-four-or-any-number-of-groups"><i class="fa fa-check"></i><b>4.2</b> Comparing four (or any number of) groups</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#the-model"><i class="fa fa-check"></i><b>4.2.1</b> The model</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#investigating-many-groups-using-predictors-analysis-of-variance"><i class="fa fa-check"></i><b>4.3</b> Investigating many groups using predictors: Analysis of Variance</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#description-of-the-model-3"><i class="fa fa-check"></i><b>4.3.1</b> Description of the model</a></li>
<li class="chapter" data-level="4.3.2" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#fitting-the-model-and-interpreting-the-results"><i class="fa fa-check"></i><b>4.3.2</b> Fitting the model and interpreting the results</a></li>
<li class="chapter" data-level="4.3.3" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#investigating-model-fit"><i class="fa fa-check"></i><b>4.3.3</b> Investigating model fit</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#interactions-and-interaction-plots"><i class="fa fa-check"></i><b>4.4</b> Interactions and interaction plots</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#interactions-in-our-f0-data"><i class="fa fa-check"></i><b>4.4.1</b> Interactions in our f0 data</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#investigating-interactions-with-a-model"><i class="fa fa-check"></i><b>4.5</b> Investigating interactions with a model</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#fitting-the-model-and-interpreting-the-results-1"><i class="fa fa-check"></i><b>4.5.1</b> Fitting the model and interpreting the results</a></li>
<li class="chapter" data-level="4.5.2" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#assessing-model-fit"><i class="fa fa-check"></i><b>4.5.2</b> Assessing model fit</a></li>
<li class="chapter" data-level="4.5.3" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#investigating-the-interactions-the-easy-way"><i class="fa fa-check"></i><b>4.5.3</b> Investigating the interactions the easy way</a></li>
<li class="chapter" data-level="4.5.4" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#making-plots"><i class="fa fa-check"></i><b>4.5.4</b> Making plots</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#lmer-corner-2"><i class="fa fa-check"></i><b>4.6</b> Lmer corner</a></li>
<li class="chapter" data-level="4.7" data-path="comparing-many-groups.html"><a href="comparing-many-groups.html#plot-code-3"><i class="fa fa-check"></i><b>4.7</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html"><i class="fa fa-check"></i><b>5</b> Including continuous predictors in our model</a>
<ul>
<li class="chapter" data-level="5.1" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#data-and-research-questions-4"><i class="fa fa-check"></i><b>5.1</b> Data and research questions</a></li>
<li class="chapter" data-level="5.2" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#continuous-predictors-modeling-variation-along-lines"><i class="fa fa-check"></i><b>5.2</b> Continuous predictors: modeling variation along lines</a></li>
<li class="chapter" data-level="5.3" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#models-with-a-single-slope-and-intercept"><i class="fa fa-check"></i><b>5.3</b> Models with a single slope and intercept</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#description-of-the-model-4"><i class="fa fa-check"></i><b>5.3.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.3.2" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#fitting-the-model-3"><i class="fa fa-check"></i><b>5.3.2</b> Fitting the model</a></li>
<li class="chapter" data-level="5.3.3" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#interpreting-the-model-2"><i class="fa fa-check"></i><b>5.3.3</b> Interpreting the model</a></li>
<li class="chapter" data-level="5.3.4" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#centering-predictors"><i class="fa fa-check"></i><b>5.3.4</b> Centering predictors</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#interactions-in-our-line-parameters"><i class="fa fa-check"></i><b>5.4</b> Interactions in our line parameters</a></li>
<li class="chapter" data-level="5.5" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#models-with-group-dependent-intercepts-but-shared-slopes"><i class="fa fa-check"></i><b>5.5</b> Models with group-dependent intercepts, but shared slopes</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#description-of-the-model-5"><i class="fa fa-check"></i><b>5.5.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.5.2" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#fitting-the-model-4"><i class="fa fa-check"></i><b>5.5.2</b> Fitting the model</a></li>
<li class="chapter" data-level="5.5.3" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#the-effect-of-including-a-slope"><i class="fa fa-check"></i><b>5.5.3</b> The effect of including a slope</a></li>
<li class="chapter" data-level="5.5.4" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#interpreting-group-effects-in-the-presence-of-a-continuous-predictor"><i class="fa fa-check"></i><b>5.5.4</b> Interpreting group effects in the presence of a continuous predictor</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#models-with-group-dependent-slopes-and-intercepts"><i class="fa fa-check"></i><b>5.6</b> Models with group-dependent slopes and intercepts</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#description-of-the-model-6"><i class="fa fa-check"></i><b>5.6.1</b> Description of the model</a></li>
<li class="chapter" data-level="5.6.2" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#fitting-the-model-5"><i class="fa fa-check"></i><b>5.6.2</b> Fitting the model</a></li>
<li class="chapter" data-level="5.6.3" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#interpreting-the-model-3"><i class="fa fa-check"></i><b>5.6.3</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="including-continuous-predictors-in-our-model.html"><a href="including-continuous-predictors-in-our-model.html#plot-code-4"><i class="fa fa-check"></i><b>5.7</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="random-slopes.html"><a href="random-slopes.html"><i class="fa fa-check"></i><b>6</b> Random slopes</a>
<ul>
<li class="chapter" data-level="6.1" data-path="random-slopes.html"><a href="random-slopes.html#data-and-research-questions-5"><i class="fa fa-check"></i><b>6.1</b> Data and research questions</a></li>
<li class="chapter" data-level="6.2" data-path="random-slopes.html"><a href="random-slopes.html#repeated-measures-and-speaker-dependent-parameter-values"><i class="fa fa-check"></i><b>6.2</b> Repeated measures and speaker-dependent parameter values</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="random-slopes.html"><a href="random-slopes.html#description-of-the-model-7"><i class="fa fa-check"></i><b>6.2.1</b> Description of the model</a></li>
<li class="chapter" data-level="6.2.2" data-path="random-slopes.html"><a href="random-slopes.html#fitting-the-model-6"><i class="fa fa-check"></i><b>6.2.2</b> Fitting the model</a></li>
<li class="chapter" data-level="6.2.3" data-path="random-slopes.html"><a href="random-slopes.html#interpreting-the-model-4"><i class="fa fa-check"></i><b>6.2.3</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="random-slopes.html"><a href="random-slopes.html#random-effects-and-the-multivariate-normal-distribution"><i class="fa fa-check"></i><b>6.3</b> Random effects and the multivariate normal distribution</a></li>
<li class="chapter" data-level="6.4" data-path="random-slopes.html"><a href="random-slopes.html#random-slopes-1"><i class="fa fa-check"></i><b>6.4</b> Random slopes</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="random-slopes.html"><a href="random-slopes.html#description-of-the-model-8"><i class="fa fa-check"></i><b>6.4.1</b> Description of the model</a></li>
<li class="chapter" data-level="6.4.2" data-path="random-slopes.html"><a href="random-slopes.html#fitting-the-model-7"><i class="fa fa-check"></i><b>6.4.2</b> Fitting the model</a></li>
<li class="chapter" data-level="6.4.3" data-path="random-slopes.html"><a href="random-slopes.html#interpreting-the-model-5"><i class="fa fa-check"></i><b>6.4.3</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="random-slopes.html"><a href="random-slopes.html#more-predictors-and-more-random-slopes"><i class="fa fa-check"></i><b>6.5</b> More predictors and more random slopes</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="random-slopes.html"><a href="random-slopes.html#adding-another-random-slope"><i class="fa fa-check"></i><b>6.5.1</b> Adding another random slope</a></li>
<li class="chapter" data-level="6.5.2" data-path="random-slopes.html"><a href="random-slopes.html#adding-random-factors"><i class="fa fa-check"></i><b>6.5.2</b> Adding random factors</a></li>
<li class="chapter" data-level="6.5.3" data-path="random-slopes.html"><a href="random-slopes.html#the-independence-of-continuous-predictors"><i class="fa fa-check"></i><b>6.5.3</b> The independence of continuous predictors</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="random-slopes.html"><a href="random-slopes.html#answering-our-research-questions-1"><i class="fa fa-check"></i><b>6.6</b> Answering our research questions</a></li>
<li class="chapter" data-level="6.7" data-path="random-slopes.html"><a href="random-slopes.html#lmer-corner-3"><i class="fa fa-check"></i><b>6.7</b> Lmer corner</a></li>
<li class="chapter" data-level="6.8" data-path="random-slopes.html"><a href="random-slopes.html#plot-code-5"><i class="fa fa-check"></i><b>6.8</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>7</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="logistic-regression.html"><a href="logistic-regression.html#data-and-research-questions-6"><i class="fa fa-check"></i><b>7.1</b> Data and research questions</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#dichotomous-variables-and-data"><i class="fa fa-check"></i><b>7.1.1</b> Dichotomous variables and data</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="logistic-regression.html"><a href="logistic-regression.html#generalizing-our-linear-models"><i class="fa fa-check"></i><b>7.2</b> Generalizing our linear models</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#link-functions"><i class="fa fa-check"></i><b>7.2.1</b> Link functions</a></li>
<li class="chapter" data-level="7.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#logits"><i class="fa fa-check"></i><b>7.2.2</b> Logits</a></li>
<li class="chapter" data-level="7.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#the-logistic-link-function"><i class="fa fa-check"></i><b>7.2.3</b> The logistic link function</a></li>
<li class="chapter" data-level="7.2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#building-intuitions-about-logits"><i class="fa fa-check"></i><b>7.2.4</b> Building intuitions about logits</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression-with-one-predictor"><i class="fa fa-check"></i><b>7.3</b> Logistic regression with one predictor</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#description-of-the-model-9"><i class="fa fa-check"></i><b>7.3.1</b> Description of the model</a></li>
<li class="chapter" data-level="7.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#fitting-the-model-8"><i class="fa fa-check"></i><b>7.3.2</b> Fitting the model</a></li>
<li class="chapter" data-level="7.3.3" data-path="logistic-regression.html"><a href="logistic-regression.html#interpreting-the-model-6"><i class="fa fa-check"></i><b>7.3.3</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="logistic-regression.html"><a href="logistic-regression.html#answering-our-research-question"><i class="fa fa-check"></i><b>7.4</b> Answering our research question</a></li>
<li class="chapter" data-level="7.5" data-path="logistic-regression.html"><a href="logistic-regression.html#plot-code-6"><i class="fa fa-check"></i><b>7.5</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html"><i class="fa fa-check"></i><b>8</b> Bayesian ANOVA and interpreting complicated models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#data-and-research-questions-7"><i class="fa fa-check"></i><b>8.1</b> Data and research questions</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#the-experiment"><i class="fa fa-check"></i><b>8.1.1</b> The experiment</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#deciding-on-a-model"><i class="fa fa-check"></i><b>8.2</b> Deciding on a model</a></li>
<li class="chapter" data-level="8.3" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#fitting-the-models"><i class="fa fa-check"></i><b>8.3</b> Fitting the models</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#the-maximal-models"><i class="fa fa-check"></i><b>8.3.1</b> The ‘maximal’ models</a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#fitting-the-final-models-in-lmer-and-brms"><i class="fa fa-check"></i><b>8.3.2</b> Fitting the ‘final’ models in lmer and brms</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#comparing-the-maximal-and-final-models"><i class="fa fa-check"></i><b>8.4</b> Comparing the maximal and final models</a></li>
<li class="chapter" data-level="8.5" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#bayesian-analysis-of-variance"><i class="fa fa-check"></i><b>8.5</b> Bayesian Analysis of Variance</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#getting-the-superpopulation-standard-deviations-from-our-models"><i class="fa fa-check"></i><b>8.5.1</b> Getting the superpopulation standard deviations from our models</a></li>
<li class="chapter" data-level="8.5.2" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#getting-the-finite-population-standard-deviations-from-our-models"><i class="fa fa-check"></i><b>8.5.2</b> Getting the finite-population standard deviations from our models</a></li>
<li class="chapter" data-level="8.5.3" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#using-the-banova-function"><i class="fa fa-check"></i><b>8.5.3</b> Using the <code>banova</code> function</a></li>
<li class="chapter" data-level="8.5.4" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#applying-the-banova-function-to-our-gender-perception-model"><i class="fa fa-check"></i><b>8.5.4</b> Applying the <code>banova</code> function to our gender perception model</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#applying-a-bayesian-anova-to-a-more-complicated-model"><i class="fa fa-check"></i><b>8.6</b> Applying a Bayesian ANOVA to a more-complicated model</a>
<ul>
<li class="chapter" data-level="8.6.1" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#description-of-the-model-10"><i class="fa fa-check"></i><b>8.6.1</b> Description of the model</a></li>
<li class="chapter" data-level="8.6.2" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#fitting-the-model-9"><i class="fa fa-check"></i><b>8.6.2</b> Fitting the model</a></li>
<li class="chapter" data-level="8.6.3" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#interpreting-the-model-7"><i class="fa fa-check"></i><b>8.6.3</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="bayesian-anova-and-interpreting-complicated-models.html"><a href="bayesian-anova-and-interpreting-complicated-models.html#plot-code-7"><i class="fa fa-check"></i><b>8.7</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multivariate-models-predictive-accuracy-and-handling-outliers.html"><a href="multivariate-models-predictive-accuracy-and-handling-outliers.html"><i class="fa fa-check"></i><b>9</b> Multivariate models, predictive accuracy, and handling outliers</a>
<ul>
<li class="chapter" data-level="9.1" data-path="multivariate-models-predictive-accuracy-and-handling-outliers.html"><a href="multivariate-models-predictive-accuracy-and-handling-outliers.html#data-and-research-questions-8"><i class="fa fa-check"></i><b>9.1</b> Data and research questions</a></li>
<li class="chapter" data-level="9.2" data-path="multivariate-models-predictive-accuracy-and-handling-outliers.html"><a href="multivariate-models-predictive-accuracy-and-handling-outliers.html#using-multiple-continuous-predictors"><i class="fa fa-check"></i><b>9.2</b> Using multiple continuous predictors</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="multivariate-models-predictive-accuracy-and-handling-outliers.html"><a href="multivariate-models-predictive-accuracy-and-handling-outliers.html#describing-and-fitting-the-model"><i class="fa fa-check"></i><b>9.2.1</b> Describing and fitting the model</a></li>
<li class="chapter" data-level="9.2.2" data-path="multivariate-models-predictive-accuracy-and-handling-outliers.html"><a href="multivariate-models-predictive-accuracy-and-handling-outliers.html#interpreting-the-model-8"><i class="fa fa-check"></i><b>9.2.2</b> Interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="multivariate-models-predictive-accuracy-and-handling-outliers.html"><a href="multivariate-models-predictive-accuracy-and-handling-outliers.html#considering-predictive-accuracy"><i class="fa fa-check"></i><b>9.3</b> Considering predictive accuracy</a></li>
<li class="chapter" data-level="9.4" data-path="multivariate-models-predictive-accuracy-and-handling-outliers.html"><a href="multivariate-models-predictive-accuracy-and-handling-outliers.html#handling-outliers-t-distributed-errors"><i class="fa fa-check"></i><b>9.4</b> Handling outliers: t-distributed errors</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="multivariate-models-predictive-accuracy-and-handling-outliers.html"><a href="multivariate-models-predictive-accuracy-and-handling-outliers.html#fitting-and-interpreting-the-model"><i class="fa fa-check"></i><b>9.4.1</b> Fitting and interpreting the model</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="multivariate-models-predictive-accuracy-and-handling-outliers.html"><a href="multivariate-models-predictive-accuracy-and-handling-outliers.html#plot-code-8"><i class="fa fa-check"></i><b>9.5</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="modeling-standard-deviations.html"><a href="modeling-standard-deviations.html"><i class="fa fa-check"></i><b>10</b> Modeling Standard Deviations</a>
<ul>
<li class="chapter" data-level="10.1" data-path="modeling-standard-deviations.html"><a href="modeling-standard-deviations.html#data"><i class="fa fa-check"></i><b>10.1</b> Data</a></li>
<li class="chapter" data-level="10.2" data-path="modeling-standard-deviations.html"><a href="modeling-standard-deviations.html#modeling-variation-in-the-error"><i class="fa fa-check"></i><b>10.2</b> Modeling variation in the error</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="modeling-standard-deviations.html"><a href="modeling-standard-deviations.html#fitting-and-interpreting-our-model"><i class="fa fa-check"></i><b>10.2.1</b> Fitting and interpreting our model</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="modeling-standard-deviations.html"><a href="modeling-standard-deviations.html#modeling-group-variation-in-our-random-effects"><i class="fa fa-check"></i><b>10.3</b> Modeling group variation in our random effects</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="modeling-standard-deviations.html"><a href="modeling-standard-deviations.html#fitting-and-interpreting-our-model-1"><i class="fa fa-check"></i><b>10.3.1</b> Fitting and interpreting our model</a></li>
<li class="chapter" data-level="10.3.2" data-path="modeling-standard-deviations.html"><a href="modeling-standard-deviations.html#two-dimensional-response-surfaces"><i class="fa fa-check"></i><b>10.3.2</b> Two-dimensional response surfaces</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="modeling-standard-deviations.html"><a href="modeling-standard-deviations.html#plot-code-9"><i class="fa fa-check"></i><b>10.4</b> Plot Code</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a></li>
<li class="divider"></li>
<li><a href="http://www.santiagobarreda.com" target="blank">Written by Santiago Barreda</a></li>
<li><a href="A-Quick-Introduction-to-Multilevel-Bayesian-Models-for-Linguistic-Researchers.pdf" target="blank">Download possibly outdated and badly-formatted PDF</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Quick Introduction to Multilevel Bayesian Models for Linguistic Researchers</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Logistic regression</h1>
<p>In this chapter we’re going to talk about the prediction of categorical variables. These are variables that can take on a (usually small) number of discrete values. We’re only going to talk about dichotomous (i.e. binary) outcomes for now, though the same ideas can be extended to model ordinal (ordered categories such as in 1st, 2nd, 3rd), and multinomial data (unordered categories such as English, French and Spanish).</p>
<div id="data-and-research-questions-6" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Data and research questions</h2>
<p>We’re going to use the data from our perceptual experiment discussed in Chapter 6. Listeners were presented with monosyllabic words, presented at random. For each trial, listeners reported the height of the speaker (in feet and inches) and guessed whether the speaker was a boy, girl, man or woman.</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="logistic-regression.html#cb210-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span> (brms)</span>
<span id="cb210-2"><a href="logistic-regression.html#cb210-2" aria-hidden="true" tabindex="-1"></a><span class="fu">options</span> (<span class="at">contrasts =</span> <span class="fu">c</span>(<span class="st">&quot;contr.sum&quot;</span>,<span class="st">&quot;cont.sum&quot;</span>))</span>
<span id="cb210-3"><a href="logistic-regression.html#cb210-3" aria-hidden="true" tabindex="-1"></a><span class="co"># source data</span></span>
<span id="cb210-4"><a href="logistic-regression.html#cb210-4" aria-hidden="true" tabindex="-1"></a>url1 <span class="ot">=</span> <span class="st">&quot;https://raw.githubusercontent.com/santiagobarreda&quot;</span></span>
<span id="cb210-5"><a href="logistic-regression.html#cb210-5" aria-hidden="true" tabindex="-1"></a>url2 <span class="ot">=</span> <span class="st">&quot;/stats-class/master/data/h95_experiment_data.csv&quot;</span></span>
<span id="cb210-6"><a href="logistic-regression.html#cb210-6" aria-hidden="true" tabindex="-1"></a>h95 <span class="ot">=</span> <span class="fu">read.csv</span> (<span class="fu">url</span>(<span class="fu">paste0</span> (url1, url2)))</span>
<span id="cb210-7"><a href="logistic-regression.html#cb210-7" aria-hidden="true" tabindex="-1"></a><span class="co"># set up colors for plotting</span></span>
<span id="cb210-8"><a href="logistic-regression.html#cb210-8" aria-hidden="true" tabindex="-1"></a>devtools<span class="sc">::</span><span class="fu">source_url</span> (<span class="fu">paste0</span> (url1, <span class="st">&quot;/stats-class/master/data/colors.R&quot;</span>))</span>
<span id="cb210-9"><a href="logistic-regression.html#cb210-9" aria-hidden="true" tabindex="-1"></a><span class="co"># source functions</span></span>
<span id="cb210-10"><a href="logistic-regression.html#cb210-10" aria-hidden="true" tabindex="-1"></a>devtools<span class="sc">::</span><span class="fu">source_url</span> (<span class="fu">paste0</span> (url1, <span class="st">&quot;/stats-class/master/data/functions.R&quot;</span>))</span></code></pre></div>
<p>In chapter 6 we saw that there is a relationship between perceived height and the f0 of the stimulus. The final model we considered in Chapter 5 suggested that this relationship may vary in a gender-dependent manner. In addition to size, listeners reported the age group (adult or child) and gender (female or male) of the speaker.</p>
<p>In Figure <a href="logistic-regression.html#fig:F7-1">7.1</a>, we see average reported height for each speaker as a function of f0, separately for males and females. Generally, a lower f0 results in the perception of a taller speaker.</p>
<div class="figure"><span id="fig:F7-1"></span>
<img src="07_files/figure-html/F7-1-1.png" alt="(left) Average perceived height for each speaker as a function of average f0. (middle) The same relationship as the left but only males are plotted. (right) Only females are plotted." width="768" />
<p class="caption">
Figure 7.1: (left) Average perceived height for each speaker as a function of average f0. (middle) The same relationship as the left but only males are plotted. (right) Only females are plotted.
</p>
</div>
<p>Figure <a href="logistic-regression.html#fig:F7-2">7.2</a> shows the probability of observing a classification of ‘adult’ (i.e., ‘man’ or ‘woman’) for each speaker as a function of average f0. Clearly, the perception of size and the perception of adultness are related. This is not surprising as, in general, larger speaker are more likely to be adults. Although superficially they seem similar, we can’t directly apply the models we’ve been using so far to explain variation in the perception of adultness.</p>
<div class="figure"><span id="fig:F7-2"></span>
<img src="07_files/figure-html/F7-2-1.png" alt="(left) Average adultness classifications for each speaker as a function of average f0. (middle) The same relationship as the left but only males are plotted. (right) Only females are plotted." width="768" />
<p class="caption">
Figure 7.2: (left) Average adultness classifications for each speaker as a function of average f0. (middle) The same relationship as the left but only males are plotted. (right) Only females are plotted.
</p>
</div>
<p>When we modeled a normally distributed variable, we were predicting the mean but also the outcomes. What I mean by that is that the an observation that equals the mean is a perfectly reasonable prediction (the most likely one in fact!).</p>
<p>However, adultness is a binary variable with outcomes of 0 (child, “not adult”) and 1 (adult) only. What are we predicting using our line? The line in the figure above goes from 0 to 1 but only exactly 0 and 1 (and nothing in between) are possible outcomes. There is no 0.4 child outcome being predicted!</p>
<p>We can instead say that we are modeling the probability of observing a certain outcome, instead of the outcome itself. So, we could model the continuous probability (<span class="math inline">\(p\)</span>) of observing an ‘adult’ response using a line. However, this doesn’t work well because the prediction line will predict values outside of 0 and 1, which doesn’t work for probabilities.</p>
<p>I mentioned in the first chapter that our models are like spherical models for billiards balls: not exactly right but close enough for many purposes. If you <em>do</em> just model <span class="math inline">\(p\)</span> with a line like in Figure <a href="logistic-regression.html#fig:F7-2">7.2</a>, your model will likely <em>not</em> be close enough for many purposes, potentially leading to major problems.</p>
<p>For these reasons, we don’t actually use linear models with Gaussian (normally-distributed) errors when modeling dichotomous variables. Instead, we use logistic regression, which is specifically intended to model binary outcomes.</p>
<p>In this chapter, we will use a logistic regression model to investigate the perception of adultness from f0 acoustics, and the way that this may vary in a gender-dependent manner.</p>
<div id="dichotomous-variables-and-data" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Dichotomous variables and data</h3>
<p>Dichotomous variables are ones that can take on only one of two possible discrete outcomes. We can easily think of many examples of this kind of data, perhaps the most obvious being something that is wrong or right.</p>
<p>When you can only have two outcomes, you can arbitrarily set one to equal a ‘success’. Keep in mind this is not a judgment of any kind, and the other category could be chosen with no important change to your model (all coefficient signs flip, that’s it).</p>
<p>After we pick a category to be a success we then make all instances of that category equal to 1 and the other category equal to 0. This means we can take all observations of the variable and find the average to calculate the probability of a success. This is actually very obvious: If I am practicing basketball and add up all my made baskets (1 point each) and divide by the total number of shots (averaging), I will clearly get the percent (or probability) of baskets made overall.</p>
<p>There are two main distributions used to model dichotomous variables: the binomial distribution and the Bernoulli distribution.</p>
<ul>
<li><p>The binomial distribution models a batch of multiple dichotomous outcomes. This distribution has two parameters: the probability of a success (<span class="math inline">\(p\)</span>) and the number of trials (<span class="math inline">\(n\)</span>). So for example, if I took 5 free throws and made 3 (3/5=0.6), then I say the data is likely to come from a binomial distribution with a <span class="math inline">\(p\)</span> parameter of 0.6 and an <span class="math inline">\(n\)</span> parameter of 5. If I use this distribution, I am treating all 5 trials as a single observation.</p></li>
<li><p>The Bernoulli distribution models individual dichotomous outcomes. This distribution has only one parameter: the probability of a success (<span class="math inline">\(p\)</span>). In this case, if I took 5 free throws and made 3 (3/5=0.6), I would still describe the data using a <span class="math inline">\(p\)</span> parameter of 0.6. However, with this distribution, I would model each 5 trial as a separate observation so there is no <span class="math inline">\(n\)</span> parameter (it is always 1).</p></li>
</ul>
<p>Below we generate random binomial variables (R doesn’t specifically make Bernoulli variables). The <code>rbinom</code> function takes parameters in this order <code>number of observations, batch size, probability of success</code>. Below, I generate first a single Bernoulli variable (a <a href="https://en.wikipedia.org/wiki/Bernoulli_trial">Bernoulli trial</a>), and then ten variables with the same probability of success.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="logistic-regression.html#cb211-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a single trial, probability of 0.5</span></span>
<span id="cb211-2"><a href="logistic-regression.html#cb211-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rbinom</span> (<span class="dv">1</span>,<span class="dv">1</span>,.<span class="dv">5</span>)</span>
<span id="cb211-3"><a href="logistic-regression.html#cb211-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0</span></span>
<span id="cb211-4"><a href="logistic-regression.html#cb211-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb211-5"><a href="logistic-regression.html#cb211-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ten single trials, probability of 0.5</span></span>
<span id="cb211-6"><a href="logistic-regression.html#cb211-6" aria-hidden="true" tabindex="-1"></a><span class="fu">rbinom</span> (<span class="dv">10</span>,<span class="dv">1</span>,.<span class="dv">5</span>)</span>
<span id="cb211-7"><a href="logistic-regression.html#cb211-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] 0 1 1 1 1 1 1 0 0 0</span></span></code></pre></div>
<p>Below we compare the data generated by the Bernoulli and the binomial distributions. In the top row we get a single number, the total number of successes in the trials. We don’t get any information about what happened on any individual trial. In the bottom row we do get information about what happened on each individual trial.</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="logistic-regression.html#cb212-1" aria-hidden="true" tabindex="-1"></a><span class="co"># a single batch of 10 trials, probability of 0.5</span></span>
<span id="cb212-2"><a href="logistic-regression.html#cb212-2" aria-hidden="true" tabindex="-1"></a><span class="fu">rbinom</span> (<span class="dv">1</span>,<span class="dv">10</span>,.<span class="dv">5</span>)</span>
<span id="cb212-3"><a href="logistic-regression.html#cb212-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 6</span></span>
<span id="cb212-4"><a href="logistic-regression.html#cb212-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-5"><a href="logistic-regression.html#cb212-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ten individual trials, probability of 0.5</span></span>
<span id="cb212-6"><a href="logistic-regression.html#cb212-6" aria-hidden="true" tabindex="-1"></a><span class="fu">rbinom</span> (<span class="dv">10</span>,<span class="dv">1</span>,.<span class="dv">5</span>)</span>
<span id="cb212-7"><a href="logistic-regression.html#cb212-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] 0 1 0 0 1 1 1 1 1 1</span></span></code></pre></div>
<p>Unlike the normal distribution, these distributions do not directly generate data with values near their mean. Instead, they generate sequences of 1s and 0s, the average of which (as the number of observations approach infinity) is expected to <a href="https://en.wikipedia.org/wiki/Consistent_estimator">converge</a> on the <span class="math inline">\(p\)</span> parameter of the distribution that generated the data. For example, below I generate sequences of a dichotomous variable with a <span class="math inline">\(p\)</span> parameter of 0.5. In each case, the estimate of the probability of the distribution gets closer as the length of the sample gets longer.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="logistic-regression.html#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span> (<span class="dv">1</span>)</span>
<span id="cb213-2"><a href="logistic-regression.html#cb213-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (<span class="fu">rbinom</span> (<span class="dv">10</span>,<span class="dv">1</span>,.<span class="dv">5</span>)) <span class="co"># the mean of 10 observations</span></span>
<span id="cb213-3"><a href="logistic-regression.html#cb213-3" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.6</span></span>
<span id="cb213-4"><a href="logistic-regression.html#cb213-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (<span class="fu">rbinom</span> (<span class="dv">100</span>,<span class="dv">1</span>,.<span class="dv">5</span>))  <span class="co"># the mean of 100 observations</span></span>
<span id="cb213-5"><a href="logistic-regression.html#cb213-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.47</span></span>
<span id="cb213-6"><a href="logistic-regression.html#cb213-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (<span class="fu">rbinom</span> (<span class="dv">1000</span>,<span class="dv">1</span>,.<span class="dv">5</span>))  <span class="co"># the mean of 1000 observations</span></span>
<span id="cb213-7"><a href="logistic-regression.html#cb213-7" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.481</span></span>
<span id="cb213-8"><a href="logistic-regression.html#cb213-8" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span> (<span class="fu">rbinom</span> (<span class="dv">100000</span>,<span class="dv">1</span>,.<span class="dv">5</span>))  <span class="co"># the mean of 100000 observations</span></span>
<span id="cb213-9"><a href="logistic-regression.html#cb213-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.50009</span></span></code></pre></div>
<p>So, when we have a binary outcome variable and are talking about individual trials, we model it as being generated by a Bernoulli distribution. This distribution has a single parameter <span class="math inline">\(p\)</span> so that our data model is <span class="math inline">\(adult \sim \mathrm{Bernoulli} (p)\)</span>. Unlike normally distributed data, there are only two outcomes (0 and 1) and variation in <span class="math inline">\(p\)</span> is bound to be between 0 and 1. If we are modeling multiple Bernoulli trials as a single observation, we can use the binomial distribution and our data model would be <span class="math inline">\(adult \sim \mathrm{Binomial} (p, n)\)</span> where <span class="math inline">\(p\)</span> is again bound to be between 0 and 1, and <span class="math inline">\(n\)</span> must be a positive integer.</p>
</div>
</div>
<div id="generalizing-our-linear-models" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Generalizing our linear models</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Generalized_linear_model">generalized linear model</a> is the extension of the same general design used in ‘regular’ regression (featuring normally distributed errors) to models for other kinds of variables.</p>
<p>Our regression models consist of a bunch of component parts stuck together. Early on I introduced two general parts: the random components and the systematic component. Our model predicting perceived height from f0 is:</p>
<p><code>pheight ~ g0_s + (g0_s|subj) + (1|speaker)</code></p>
<p>We know that this model is basically this:</p>
<p><span class="math display" id="eq:71">\[\begin{equation}
\begin{split}
pheight_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{error}) \\
\mu_{[i]} = a_{[i]} + b_{[i]} * \mathrm{x}_{[i]}  \\ 
...
\end{split}
\tag{7.1}
\end{equation}\]</span></p>
<p>Which says we expect our variable to be normally distributed, and we expect variation in the mean to vary along a line. These two parts are called the <em>random component</em> and the <em>systematic component</em> respectively. The random component specifies how our data randomly varies given some parameter (<span class="math inline">\(\mu\)</span> for normal distributions). The systematic component predicts variation in the parameter of interest using shapes like lines and planes.</p>
<p>Generalized linear models use what are know as <em>link functions</em>. We haven’t dealt with this at all yet because the normal distribution does not need one. For normally distributed data we directly model variation in <span class="math inline">\(\mu\)</span> so there is no need for a link function. We could add a link function in between the random and systematic components as below:</p>
<p><span class="math display" id="eq:72">\[\begin{equation}
\begin{split}
pheight_{[i]} \sim \mathcal{N}(\mu_{[i]},\sigma_{error}) \\
\mu_{[i]} = f(\theta_{[i]}) \\
\theta_{[i]}  = a_{[i]} + b_{[i]} * \mathrm{x}_{[i]}  \\ 
...
\end{split}
\tag{7.2}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(f()\)</span> represents the link function. Regression models with normally distributed errors use what is called the <em>identity</em> link function, which basically means that they model a parameter that directly translates into the <span class="math inline">\(\mu\)</span> parameter (<span class="math inline">\(f(x)=x\)</span>). This is equivalent to modeling the <span class="math inline">\(\mu\)</span> parameter directly in our prediction equation.</p>
<p>Why can we directly model variation in <span class="math inline">\(\mu\)</span> with our prediction equation? The key is that normally distributed data is supposed to be continuous and extend from positive to negative infinity. Of course this is only in principle, but it still means that <em>in principle</em>, you <em>can</em> model <span class="math inline">\(\mu\)</span> using lines since this might actually vary continuously along them.</p>
<p>In contrast, the probability of an event occurring must be between 0 and 1, so it obviously does <em>not</em> extend to infinity. As a result, even if we <em>did</em> switch to the Bernoulli distribution we could not just do something like this:</p>
<p><span class="math display" id="eq:73">\[\begin{equation}
\begin{split}
adult_{[i]} \sim \mathrm{Bernoulli}(p_{[i]}) \\
p_{[i]} = \theta_{[i]} \\
\theta_{[i]}  = a_{[i]} + b_{[i]} * \mathrm{x}_{[i]}  \\ 
\end{split}
\tag{7.3}
\end{equation}\]</span></p>
<p>Because this would cause weird things like predictions with negative probabilities. Instead, we need to employ an actual link function in the second step above (e.g., <span class="math inline">\(f(\theta_{[i]})\)</span>), in order to relate <span class="math inline">\(p\)</span> into something we can model using lines.</p>
<div id="link-functions" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Link functions</h3>
<p>A link function allows you to <em>link</em> variation in a parameter to variation in straight lines. Modeling becomes a three step process like this:</p>
<ol style="list-style-type: decimal">
<li><p>Predict variation of some parameter along straight lines (or related shapes), for example <span class="math inline">\(\theta = a + b * \mathrm{x}\)</span>.</p></li>
<li><p>Transform the parameter using a link function (<span class="math inline">\(p = f(\theta)\)</span>).</p></li>
<li><p>Use the <em>transformed</em> parameter in data generating distribution (e.g., <span class="math inline">\(adult \sim \mathrm{Bernoulli}(p)\)</span>)</p></li>
</ol>
<p>For dichotomous variables, it’s common to predict outcomes in units called <em>logits</em> (step 3). Then, logits are then turned into probabilities (<span class="math inline">\(p\)</span>) using the <a href="https://en.wikipedia.org/wiki/Logistic_function#Mathematical_properties">logistic link function</a> (step 2). The parameter <span class="math inline">\(p\)</span> can then be used together with an appropriate probability distribution to model the data.</p>
</div>
<div id="logits" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Logits</h3>
<p>Logits are log-odds, the logarithm of the odds of a success. The odds of a success is defined as:</p>
<p><code>odds = total number of successes / total number of failures</code></p>
<p>Odds of 3/1 indicate that success is three times as likely as a failure. We can turn this into a probability with the following calculation:</p>
<p><code>probability = total number of successes / (total number of successes + total number of failures</code>)</p>
<p>So, odds of 3/1 imply a probability of 0.25 (3 / (3+1)). Odds are still bounded by zero so they don’t work for linear modeling, but if we take the <em>logarithm</em> of the odds we get a logit: a value that extends continuously from positive to negative infinity. This is because logarithms represent the space from 0 to 1 with values from <span class="math inline">\(-\infty\)</span> to 0, and values from 1 to <span class="math inline">\(+\infty\)</span> with values from 0 to <span class="math inline">\(+\infty\)</span>.</p>
<p>We can calculate the logit (<span class="math inline">\(z\)</span>) with either of the two equivalent calculations:</p>
<p><code>logit = log (probability of successes / probability of failures)</code></p>
<p><code>logit = log (probability of successes) - log (probability of failures)</code></p>
<p>I wrote a function that calculates logits from probabilities (<code>ptoz</code>) seen below. Note that I have a special case for situations where <span class="math inline">\(p\)</span> is equal to 0 or 1, where I arbitrarily change those to 0.99 and 0.01. Since the log(0)=<span class="math inline">\(-\infty\)</span>, the logit of probabilities of 0 and 1 and positive and negative infinity respectively. One way to think about this is is that to really know that something is 1 and not, for example 0.9999…., you would have to observe an infinite number of successes with no failures, leading to infinite odds (successes/failures) and log odds. Infinite logit values are not useful for us (and can’t be plotted), so the <code>ptoz</code> function sets extreme (but manageable) values for probabilities of 0 and 1.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="logistic-regression.html#cb214-1" aria-hidden="true" tabindex="-1"></a><span class="co"># changes probabilities (p) to logits</span></span>
<span id="cb214-2"><a href="logistic-regression.html#cb214-2" aria-hidden="true" tabindex="-1"></a>ptoz <span class="ot">=</span> <span class="cf">function</span> (p){</span>
<span id="cb214-3"><a href="logistic-regression.html#cb214-3" aria-hidden="true" tabindex="-1"></a>  p[p<span class="sc">==</span><span class="dv">1</span>] <span class="ot">=</span> .<span class="dv">99</span>  <span class="co"># if p=1, change to 0.99</span></span>
<span id="cb214-4"><a href="logistic-regression.html#cb214-4" aria-hidden="true" tabindex="-1"></a>  p[p<span class="sc">==</span><span class="dv">0</span>] <span class="ot">=</span> .<span class="dv">01</span>  <span class="co"># if p=0, change to 0.01 (i.e. 1-0.99)</span></span>
<span id="cb214-5"><a href="logistic-regression.html#cb214-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">log</span> (p) <span class="sc">-</span> <span class="fu">log</span>(<span class="dv">1</span><span class="sc">-</span>p)</span>
<span id="cb214-6"><a href="logistic-regression.html#cb214-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Our logistic regression models are going to predict <strong>logits</strong>. Our lines will describe continuous changes in logits. Our intercepts will describe shifts on the values of logits across several categories. Given a predicted value expressed in logits, we can then use the <em>logistic function</em> (sometimes called the <em>antilogit function</em>) to convert this value into a probability.</p>
<p>For this reason, the logistic function is said to be the link function for logistic regression: the logistic function links the prediction made by our model (in logits) to the parameter used in the data-generating function assumed by our model (<span class="math inline">\(p\)</span> in a Bernoulli distribution).</p>
</div>
<div id="the-logistic-link-function" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> The logistic link function</h3>
<p>The logistic link function looks like below, for values of <span class="math inline">\(z\)</span> ranging continuously from positive to negative infinity, where <span class="math inline">\(e\)</span> is the <a href="https://en.wikipedia.org/wiki/Natural_logarithm">mathematical constant used for natural logarithms</a>.</p>
<p><span class="math display" id="eq:74">\[
f(z) = \frac{1}{1 + e^{-z}}
\tag{7.4}
\]</span></p>
<p>I’ve written this simple R function that implements the logistic link:</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="logistic-regression.html#cb215-1" aria-hidden="true" tabindex="-1"></a><span class="co"># changes logits to probabilities.</span></span>
<span id="cb215-2"><a href="logistic-regression.html#cb215-2" aria-hidden="true" tabindex="-1"></a>ztop <span class="ot">=</span> <span class="cf">function</span> (z) <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>z)) </span></code></pre></div>
<p>The function may look complicated but its a bit simpler when you remember than any number raised to the power of zero is 1. So <span class="math inline">\(e^0=1\)</span>, meaning that when <span class="math inline">\(z=0\)</span>,
<span class="math display" id="eq:742">\[
\frac{1}{1 + e^{0}}=\frac{1}{1 + 1}=0.5
\tag{7.5}
\]</span></p>
<p>When <span class="math inline">\(z\)</span> is a positive value, <span class="math inline">\(e^{-z}\)</span> will be a fraction between 0 and 1, for example if <span class="math inline">\(z=3\)</span> then <span class="math inline">\(e^{-z}=e^{-(3)}=0.05\)</span>. This means that when <span class="math inline">\(z&lt;0\)</span>, we expect probabilities between 0.5 and 1, as in:</p>
<p><span class="math display" id="eq:75">\[
\frac{1}{1 + e^{-3}}=\frac{1}{1 + 0.05}=0.95
\tag{7.6}
\]</span></p>
<p>On the other hand, when <span class="math inline">\(z\)</span> is negative, <span class="math inline">\(e^{-z}\)</span> will be a positive number greater than 1, as in <span class="math inline">\(e^{-z}=e^{-(-3)}=e^{3}=20.1\)</span>. Thus, for negative values of <span class="math inline">\(z\)</span> we expect probabilities between 0 and 0.5 as in:</p>
<p><span class="math display" id="eq:76">\[
\frac{1}{1 + e^{3}}=\frac{1}{1 + 20.1}=0.05
\tag{7.7}
\]</span></p>
<p>In Figure <a href="logistic-regression.html#fig:F7-3">7.3</a> I draw a line with a slope of one and an intercept of 0 (i.e., y = x). We can imagine that this line defines expected changes in <em>logits</em> as a function of some predictor x. For example, where we might directly model change in perceived height (in inches) as a function of continuous variation in f0, we model the change in the <em>logit</em> of the probability of observing an adult response, as a function of f0.</p>
<p>In the middle panel, we’ve applied the logistic function to the line, resulting in a <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid curve</a>. If we apply the logit function to this sigmoid curve, the result is a line again. We can see that the logit and sigmoid functions/transformations are just in the inverse of each other. They can be applied to data to go back and forth between a probability or logit interpretation.</p>
<div class="figure"><span id="fig:F7-3"></span>
<img src="07_files/figure-html/F7-3-1.png" alt="(left) A plot of a line with a slope of 1 and intercept of 0. We can treat the y-axis as logits. (middle) The result of applying the logistic function to every point of the line in the left column. (right) Calculating the logit of each value specified on the curve in the middle turns our sigmoid curve back to a line." width="768" />
<p class="caption">
Figure 7.3: (left) A plot of a line with a slope of 1 and intercept of 0. We can treat the y-axis as logits. (middle) The result of applying the logistic function to every point of the line in the left column. (right) Calculating the logit of each value specified on the curve in the middle turns our sigmoid curve back to a line.
</p>
</div>
<p>In chapter 5 I mentioned that when you do a linear regression you model the data as normal distributions sliding along a line, generating normally distributed data along the line as they move. In logistic regression you model the data as a Bernoulli distribution sliding along the sigmoid curve above, generating 1s and 0s as it moves. This is accomplished by modeling variation in logits (<span class="math inline">\(z\)</span>) using our regression models, and only converting to a probability (<span class="math inline">\(p\)</span>) after prediction.</p>
<p>In other words, our model consists of the three elements seen below: the data generation (random component), the link function, and our prediction equation (systematic component).</p>
<p><span class="math display" id="eq:762">\[\begin{equation}
\begin{split}
adult \sim \mathrm{Bernoulli}(p) \\
p = \mathrm{logistic} (z) \\
z  = a + b * \mathrm{x}  \\ 
\end{split}
\tag{7.8}
\end{equation}\]</span></p>
<p>Keep in mind that we could put the prediction equation directly inside the logistic function:</p>
<p><span class="math display" id="eq:77">\[
p = \frac{1}{1 + e^{-(a + b*x)}}
\tag{7.9}
\]</span></p>
<p>And even put the output of that inside the Bernoulli distribution directly.</p>
<p><span class="math display" id="eq:78">\[
adult \sim \mathrm{Bernoulli}(\frac{1}{1 + e^{-(a + b*x)}}) \\
\tag{7.10}
\]</span></p>
<p>When you look at it this way, it shows that our link function really does <em>link</em> our prediction equation (a + b*x) and our data distribution (Bernoulli). The fomulation above is reminiscent of this formulation of our model for normally distributed data:</p>
<p><span class="math display" id="eq:79">\[
pheight \sim \mathrm{Normal}(a + b*x, \sigma) \\
\tag{7.11}
\]</span></p>
</div>
<div id="building-intuitions-about-logits" class="section level3" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Building intuitions about logits</h3>
<p>The logit (and logistic) functions are <em>non linear</em> functions. In the simplest sense this means that they take lines and make them into non lines, and vice versa. This can be seen below where the left and right panels present the equivalent relation in logits and probabilities.</p>
<p>Another consequences of the non-linearity of the transformation is that a given unit increase in probabilities does not equal a given unit increase in logits. The reverse is also true. This can clearly be seen in the figure below which has horizontal lines from 0.1 to 0.9, spaced every 0.1 units of probability. We see that this results in equally-spaced lines on the right, but not on the left.</p>
<div class="figure"><span id="fig:F7-4"></span>
<img src="07_files/figure-html/F7-4-1.png" alt="(left) A line relating some predictor to logits. (right) A sigmoid curve expressing the probability associated with each value along the line in the left. Horizontal lines are places every 0.1 probability units from 0.1 to 0.9 probability." width="768" />
<p class="caption">
Figure 7.4: (left) A line relating some predictor to logits. (right) A sigmoid curve expressing the probability associated with each value along the line in the left. Horizontal lines are places every 0.1 probability units from 0.1 to 0.9 probability.
</p>
</div>
<p>We can easily check what the logit values are for a set of probabilities using the <code>ptoz</code> function. The top row below contains a sequence of probabilities and the bottom row shows equivalent logits.</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="logistic-regression.html#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span> ( (<span class="fu">seq</span> (<span class="fl">0.1</span>,.<span class="dv">9</span>,.<span class="dv">1</span>)), </span>
<span id="cb216-2"><a href="logistic-regression.html#cb216-2" aria-hidden="true" tabindex="-1"></a>        <span class="fu">round</span> ( <span class="fu">ptoz</span> (<span class="fu">seq</span> (<span class="fl">0.1</span>,<span class="fl">0.9</span>,.<span class="dv">1</span>)) , <span class="dv">3</span>) )</span>
<span id="cb216-3"><a href="logistic-regression.html#cb216-3" aria-hidden="true" tabindex="-1"></a><span class="do">##        [,1]   [,2]   [,3]   [,4] [,5]  [,6]  [,7]  [,8]  [,9]</span></span>
<span id="cb216-4"><a href="logistic-regression.html#cb216-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]  0.100  0.200  0.300  0.400  0.5 0.600 0.700 0.800 0.900</span></span>
<span id="cb216-5"><a href="logistic-regression.html#cb216-5" aria-hidden="true" tabindex="-1"></a><span class="do">## [2,] -2.197 -1.386 -0.847 -0.405  0.0 0.405 0.847 1.386 2.197</span></span></code></pre></div>
<p>Notice that the difference between 0.5 and 0.6 is 0.4 logits, but the difference between 0.7 and 0.9 is about 0.8 logits. Meanwhile the difference between 0.9 and 1 is infinity!</p>
<p>Imagine that you keep track of you free throw practice in basketball. You sink 500/1000 free throws, giving you a 0.5 probability of success. Now imagine you take a further 100 and sink them all. Now your probability of success is 600/1100, meaning your amazing streak has increased your probability to 0.54. However, suppose that you had been a 900/1000 shooter, with a probability of 0.9. If you had the same streak of 100 baskets, you would only have increased your probability to 90.1 (1000/1100).</p>
<p>We can see that ‘the same’ increase results in a large increase in one probability (0.5 -&gt; 0.54, almost 10%) and a minuscule change in another (0.9 -&gt; 0.901, about 1%). Basically, as you approach 0 and 1 it gets harder to make large changes in your probabilities. The result of this is that a given logit difference will equal a large probability difference near 0 logits, and a smaller probability difference as the underlying logit values are further from 0. Here are some useful things to keep in mind when interpreting logits:</p>
<ul>
<li><p>50% is 0. Positive means more likely to be a success, negative means more likely to be a failure.</p></li>
<li><p>-3 and 3 are 4.7% and 95.2%. Basically -3 and 3 logits are useful bounds for “almost always” and almost never".</p></li>
<li><p>Since a logit of 3 translates to a <span class="math inline">\(p\)</span> of about 0.95, all of the space between +3 and infinity logits represents the probability space between 0.95 and 1, while logits between 0 and 3 represent the space from 0.5 to 0.95.</p></li>
<li><p>Logits far beyond 3 might not have much practical significance. A logit of 6 is 99.7%, a 4.5% increase over 3 logits. The logits have doubled but the probability has barely changed for most purposes. Also, it is very difficult to distinguish 95% and 99% in practice since by definition, you will be observing very few mistakes to distinguish the two!</p></li>
<li><p>Effects can be considered important or not based on how far they got you along -3 to 3 (or -4 to 4). Basically, anything in the +1 range is very likely to matter, while effects smaller than 0.2 or so are likely having only small effects on outcomes.</p></li>
</ul>
<p>Here is one final thing to keep in mind: you <strong>must</strong> consider your model effects as logits <strong>before</strong> transforming them into probabilities. This important constraint follows directly from the fact that a given logit difference can lead to varying differences for different probability values.</p>
<p>For example, imagine that we have an intercept of 1 and an effect of 2 for some group, expressed in logits. This results in an expected probability of:</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="logistic-regression.html#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ztop</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span>)</span>
<span id="cb217-2"><a href="logistic-regression.html#cb217-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.9525741</span></span></code></pre></div>
<p>For that group. In contrast, if we had first converted to logits and tried to add after:</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="logistic-regression.html#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ztop</span> (<span class="dv">1</span>) <span class="sc">+</span> <span class="fu">ztop</span>(<span class="dv">2</span>)</span>
<span id="cb218-2"><a href="logistic-regression.html#cb218-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 1.611856</span></span></code></pre></div>
<p>We get something that is not even a valid probability.</p>
</div>
</div>
<div id="logistic-regression-with-one-predictor" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Logistic regression with one predictor</h2>
<p>We’re going to fit a model to our data that predicts the perception of adultness using logistic regression.</p>
<div id="description-of-the-model-9" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Description of the model</h3>
<p>The model would be quite familiar to use if it were dealing with normally distributed data. The formula for the model we are using looks like this:</p>
<p><code>padult ~ g0_s*pfemale + (g0_s*pfemale|subj) + (1|speaker)</code></p>
<p>Where <code>padult</code> represents perceived adultness, and <code>pfemale</code> represents perceived female (more on these later). We’re going to use scaled log-f0 (<code>g0_s</code>) instead of just the centered version of the predictor. When we scale predictors we first center them and then divide by the standard deviation. This causes our predictor to have a standard deviation of 1 and a mean of 0.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="logistic-regression.html#cb219-1" aria-hidden="true" tabindex="-1"></a>g0_m <span class="ot">=</span> <span class="fu">mean</span> ( <span class="fu">log</span>(h95<span class="sc">$</span>f0) ) </span>
<span id="cb219-2"><a href="logistic-regression.html#cb219-2" aria-hidden="true" tabindex="-1"></a>g0_sd <span class="ot">=</span> <span class="fu">sd</span> ( <span class="fu">log</span>(h95<span class="sc">$</span>f0) )</span>
<span id="cb219-3"><a href="logistic-regression.html#cb219-3" aria-hidden="true" tabindex="-1"></a>h95<span class="sc">$</span>g0_s <span class="ot">=</span> (<span class="fu">log</span>( h95<span class="sc">$</span>f0 ) <span class="sc">-</span> g0_m) <span class="sc">/</span> g0_sd</span></code></pre></div>
<p>When we scale predictors, the slope coefficients will reflect changes in our dependent variable per 1 standard deviation difference in the predictor (rather than a 1 unit change). Scaling is very useful for logistic regression because it make most parameters have small values (&lt;10 or so) by making the range of any predictor mostly fall within -4 and 4. If you don’t scale your continuous predictors, you can end up with slope coefficients like 200 when the predictor has very large values. Since we know that 3 logits is already 95%, coefficient values as large as 200 are difficult to interpret quickly.</p>
<p>The formula above says tells our model to predict perceived adultness using scaled log f0 (<code>g0_s</code>), information about whether the speaker was identified as female or male, and the gender-dependent use of f0. It allows for random by-subject effects for all predictors and random intercepts for speaker.</p>
<p>Our full model specification is now:</p>
<p><span class="math display" id="eq:710">\[\begin{equation}
\begin{split}
\textrm{Likelihood:} \\
padult_{[i]} \sim \mathrm{Bernoulli}(p_{[i]}) \\
p_{[i]} = \mathrm{logistic} (z_{[i]}) \\
z_{[i]} = a_{[i]} + b_{[i]} * \mathrm{x}_{[i]}  \\ 
a_{[i]} = Intercept + pfemale_{[i]} + \alpha_{[\mathrm{subj}_{[i]}]}  +  \alpha_{pfemale[\mathrm{subj}_{[i]}]}+ \alpha_{[\mathrm{speaker}_{[i]}]}  \\
b_{[i]} =  g0\_c + g0\_s \colon pfemale_{[i]} + \beta_{[\mathrm{subj}_{[i]}]} + \beta_{g0\_s \colon pfemale[\mathrm{subj}_{[i]}]} \\ \\

\textrm{Priors:} \\
\alpha_{speaker} \sim \mathrm{Normal}(0,\sigma_{speaker}) \\ \\  

\begin{bmatrix} \alpha_{subj} \\ \alpha_{pfemale,subj} \\ \beta_{subj} \\ \beta_{g0\_s \colon pfemale,subj} \\ \end{bmatrix}    
\sim \mathrm{MVNormal} ( \begin{bmatrix} 0\\ 0 \\ 0 \\ 0 \\ \end{bmatrix}, \Sigma) \\ \\

Intercept \sim t(3, 0, 5) \\
g0\_c \sim t(3, 0, 3) \\
pfemale \sim t(3, 0, 3) \\
g0\_c \colon female \sim t(3, 0, 3) \\ \\

\sigma_{\alpha_{speaker}} \sim t(3, 0, 4) \\ 
\sigma_{\alpha_{subj}} \sim t(3, 0, 4) \\ 
\sigma_{\alpha_{pfemale,subj}} \sim t(3, 0, 4) \\ 
\sigma_{\beta_{g0\_s \colon pfemale, subj}} \sim t(3, 0, 4) \\ 
\sigma_{\beta_{subj}} \sim t(3, 0, 4) \\ 
R \sim \mathrm{LKJCorr} (2)
\end{split}
\tag{7.12}
\end{equation}\]</span></p>
<blockquote>
<p>We’re treating our adultness judgments (1 or 0 for adult and child) as coming from a Bernoulli distribution with a probability that varies trial to trial. The <em>logit of the probability</em> (z) varies along lines. The lines are specified by intercepts and slopes that vary trom trial to trial, and there is a single continuous predictor (g0_s). The intercept of these lines vary based on an overall intercept (the main effect), an overall effect for the perception of a female speaker (pfemale), subject-specific deviations from the mean, subject-specific deviations from the female effect, and speaker-specific deviations from the mean. The slope of these lines vary based on an overall slope (the main effect), subject-specific deviations from the average slope, and subject-specific deviation from the slope based on whether the speaker was identified as female or not.</p>
</blockquote>
<blockquote>
<p>The speaker intercepts were drawn from a normal distribution with a mean of zero and a standard deviation estimated from the data. The subject random effects were drawn from a normal distribution with means of 0 of zero and a covariance matrix estimated from the data. All other effects (e.g., the Intercept, g0_s, etc.) were treated as ‘fixed’ and drawn from prior distributions appropriate for their expected range of values (e.g., subj ~ t(3,0,3)).</p>
</blockquote>
<p>That model specification is pretty long, and our models are not even <em>that</em> complicated yet. Going forward I’m going to start skipping some or all of the model specification, and provide a verbal description of our models based on the <code>brm</code> function calls we use to fit them. We’ve already covered all the components of these models so you can always look back at previous chapters to see how to ‘translate’ model formulas to their model specifications.</p>
<p>For example, here is the function call we need to run the model described above:</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="logistic-regression.html#cb220-1" aria-hidden="true" tabindex="-1"></a>logistic_g0 <span class="ot">=</span></span>
<span id="cb220-2"><a href="logistic-regression.html#cb220-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span> (padult <span class="sc">~</span> g0_s<span class="sc">*</span>pfemale <span class="sc">+</span> (g0_s<span class="sc">*</span>pfemale<span class="sc">|</span>subj) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>speaker), <span class="at">data=</span>h95, </span>
<span id="cb220-3"><a href="logistic-regression.html#cb220-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">chains=</span><span class="dv">4</span>, <span class="at">cores=</span><span class="dv">4</span>, <span class="at">family=</span><span class="st">&quot;bernoulli&quot;</span>, </span>
<span id="cb220-4"><a href="logistic-regression.html#cb220-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">warmup=</span><span class="dv">1000</span>, <span class="at">iter =</span> <span class="dv">11000</span>, <span class="at">thin =</span> <span class="dv">10</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span> <span class="fl">0.95</span>), </span>
<span id="cb220-5"><a href="logistic-regression.html#cb220-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">prior =</span> <span class="fu">c</span>(<span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 5)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb220-6"><a href="logistic-regression.html#cb220-6" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb220-7"><a href="logistic-regression.html#cb220-7" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 4)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>),</span>
<span id="cb220-8"><a href="logistic-regression.html#cb220-8" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>)))</span></code></pre></div>
<p>Every piece of information in the model description above is predictable from this function call. The model structure is predictable from the model formula. The <code>family</code> parameter tells <code>brm</code> that our model is analyzing Bernoulli distributed data. When you set <code>family="bernoulli"</code>, <code>brm</code> knows to use the logistic link function. The priors can be understood from the call if we remember that:</p>
<ul>
<li><p><code>class = "Intercept"</code> sets the prior only for the intercept.</p></li>
<li><p><code>class = "b"</code> sets the prior only for all ‘fixed effect’ (i.e. what <code>brm</code> calls “population level”) predictors, other than the intercept.</p></li>
<li><p><code>class = "sd"</code> sets the prior for standard deviation parameters for our ‘random’ effects.</p></li>
<li><p><code>class = "corr"</code> sets the prior for correlation matrices needed to build the covariance matrix for our multivariate normal distributions (discussed in chapter 6).</p></li>
</ul>
</div>
<div id="fitting-the-model-8" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Fitting the model</h3>
<p>We’re going to fit the model outlined above, except I am just going to use a standard deviation of 3 for everything. These distributions have most of their mass between -9 and +9, which are relatively large values for logits. In general we expect factors and scaled continuous predictors to mostly have values under &lt;10, or for values under 10 to be most meaningful to us.</p>
<p>I’m also going to set up my <code>padult</code> and <code>pfemale</code> variables. We are going to arbitrarily call ‘adult’ responses 1 and ‘child’ responses 0. Below, I create a new vector called <code>padult</code> that equals 1 if the perceived group (<code>pgroup</code>) is woman (<code>w</code>) or man (<code>m</code>) and 0 if the perceived group is girl (<code>g</code>) or boy (<code>b</code>). This will be our depndent variable.</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="logistic-regression.html#cb221-1" aria-hidden="true" tabindex="-1"></a>h95<span class="sc">$</span>padult <span class="ot">=</span> <span class="fu">as.numeric</span>(h95<span class="sc">$</span>pgroup <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&#39;m&#39;</span>,<span class="st">&#39;w&#39;</span>))</span>
<span id="cb221-2"><a href="logistic-regression.html#cb221-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb221-3"><a href="logistic-regression.html#cb221-3" aria-hidden="true" tabindex="-1"></a><span class="fu">as.character</span> (h95<span class="sc">$</span>pgroup[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]) <span class="co"># original group label</span></span>
<span id="cb221-4"><a href="logistic-regression.html#cb221-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] &quot;g&quot; &quot;g&quot; &quot;b&quot; &quot;g&quot; &quot;w&quot; &quot;b&quot; &quot;b&quot; &quot;w&quot; &quot;b&quot; &quot;w&quot;</span></span>
<span id="cb221-5"><a href="logistic-regression.html#cb221-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb221-6"><a href="logistic-regression.html#cb221-6" aria-hidden="true" tabindex="-1"></a>h95<span class="sc">$</span>padult[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>] <span class="co"># 0,1 dependent variable</span></span>
<span id="cb221-7"><a href="logistic-regression.html#cb221-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] 0 0 0 0 1 0 0 1 0 1</span></span></code></pre></div>
<p>Our model also needs a predictor indicating perceived female gender. Rather than using a factor, we are going to directly sum code this variable ourselves. Since there is only two levels, we can set female responses (<code>g</code> or <code>w</code>) to 1 and male responses (<code>b</code> or <code>m</code>) to -1. The ‘slope’ associated with this predictor reflects the effect for perceived femaleness, while the negative of the slope reflects the effect for perceived maleness. This is how sum coding is implemented!</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="logistic-regression.html#cb222-1" aria-hidden="true" tabindex="-1"></a>h95<span class="sc">$</span>pfemale <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span>(<span class="fu">as.numeric</span>(h95<span class="sc">$</span>pgroup <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&#39;g&#39;</span>,<span class="st">&#39;w&#39;</span>)) <span class="sc">-</span> <span class="fl">0.5</span>)</span>
<span id="cb222-2"><a href="logistic-regression.html#cb222-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-3"><a href="logistic-regression.html#cb222-3" aria-hidden="true" tabindex="-1"></a><span class="fu">as.character</span> (h95<span class="sc">$</span>pgroup[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]) <span class="co"># original group label</span></span>
<span id="cb222-4"><a href="logistic-regression.html#cb222-4" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1] &quot;g&quot; &quot;g&quot; &quot;b&quot; &quot;g&quot; &quot;w&quot; &quot;b&quot; &quot;b&quot; &quot;w&quot; &quot;b&quot; &quot;w&quot;</span></span>
<span id="cb222-5"><a href="logistic-regression.html#cb222-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-6"><a href="logistic-regression.html#cb222-6" aria-hidden="true" tabindex="-1"></a>h95<span class="sc">$</span>pfemale[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>] <span class="co"># -1,1 predictor variable</span></span>
<span id="cb222-7"><a href="logistic-regression.html#cb222-7" aria-hidden="true" tabindex="-1"></a><span class="do">##  [1]  1  1 -1  1  1 -1 -1  1 -1  1</span></span></code></pre></div>
<p>We fit the model below:</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="logistic-regression.html#cb223-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model yourself, or</span></span>
<span id="cb223-2"><a href="logistic-regression.html#cb223-2" aria-hidden="true" tabindex="-1"></a><span class="co"># download pre-fit model from: </span></span>
<span id="cb223-3"><a href="logistic-regression.html#cb223-3" aria-hidden="true" tabindex="-1"></a><span class="co"># github.com/santiagobarreda/stats-class/tree/master/models</span></span>
<span id="cb223-4"><a href="logistic-regression.html#cb223-4" aria-hidden="true" tabindex="-1"></a><span class="co"># and load after placing in working directory</span></span>
<span id="cb223-5"><a href="logistic-regression.html#cb223-5" aria-hidden="true" tabindex="-1"></a><span class="co"># logistic_g0 = readRDS (&#39;7_logistic_g0.RDS&#39;)</span></span>
<span id="cb223-6"><a href="logistic-regression.html#cb223-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb223-7"><a href="logistic-regression.html#cb223-7" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span> (<span class="dv">1</span>)</span>
<span id="cb223-8"><a href="logistic-regression.html#cb223-8" aria-hidden="true" tabindex="-1"></a>logistic_g0 <span class="ot">=</span></span>
<span id="cb223-9"><a href="logistic-regression.html#cb223-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">brm</span> (padult <span class="sc">~</span> g0_s<span class="sc">*</span>pfemale <span class="sc">+</span> (g0_s<span class="sc">*</span>pfemale<span class="sc">|</span>subj) <span class="sc">+</span> (<span class="dv">1</span><span class="sc">|</span>speaker), <span class="at">data=</span>h95, </span>
<span id="cb223-10"><a href="logistic-regression.html#cb223-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">chains=</span><span class="dv">4</span>, <span class="at">cores=</span><span class="dv">4</span>, <span class="at">family=</span><span class="st">&quot;bernoulli&quot;</span>, </span>
<span id="cb223-11"><a href="logistic-regression.html#cb223-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">warmup=</span><span class="dv">1000</span>, <span class="at">iter =</span> <span class="dv">11000</span>, <span class="at">thin =</span> <span class="dv">10</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta =</span> <span class="fl">0.95</span>), </span>
<span id="cb223-12"><a href="logistic-regression.html#cb223-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">prior =</span> <span class="fu">c</span>(<span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;Intercept&quot;</span>),</span>
<span id="cb223-13"><a href="logistic-regression.html#cb223-13" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;b&quot;</span>),</span>
<span id="cb223-14"><a href="logistic-regression.html#cb223-14" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;student_t(3, 0, 3)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;sd&quot;</span>),</span>
<span id="cb223-15"><a href="logistic-regression.html#cb223-15" aria-hidden="true" tabindex="-1"></a>                 <span class="fu">set_prior</span>(<span class="st">&quot;lkj_corr_cholesky (2)&quot;</span>, <span class="at">class =</span> <span class="st">&quot;cor&quot;</span>)))</span>
<span id="cb223-16"><a href="logistic-regression.html#cb223-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb223-17"><a href="logistic-regression.html#cb223-17" aria-hidden="true" tabindex="-1"></a><span class="co"># save model</span></span>
<span id="cb223-18"><a href="logistic-regression.html#cb223-18" aria-hidden="true" tabindex="-1"></a><span class="fu">saveRDS</span> (logistic_g0, <span class="st">&#39;7_logistic_g0.RDS&#39;</span>)</span></code></pre></div>
</div>
<div id="interpreting-the-model-6" class="section level3" number="7.3.3">
<h3><span class="header-section-number">7.3.3</span> Interpreting the model</h3>
<p>The estimates and credible intervals in the model printout below are being expressed in logits, however, most of this model output is the same as it was for our Gaussian-error models. There are two noteworthy differences:</p>
<ol style="list-style-type: decimal">
<li>The top of the model now indicates <code>Family: bernoulli</code> and <code>Links: mu = logit</code>.</li>
<li>Notice the absence of the <code>Family-Specific</code> parameter section of the model where <code>sigma</code> (i.e., <span class="math inline">\(\sigma_{error}\)</span>) was usually found.</li>
</ol>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="logistic-regression.html#cb224-1" aria-hidden="true" tabindex="-1"></a>logistic_g0</span>
<span id="cb224-2"><a href="logistic-regression.html#cb224-2" aria-hidden="true" tabindex="-1"></a><span class="do">##  Family: bernoulli </span></span>
<span id="cb224-3"><a href="logistic-regression.html#cb224-3" aria-hidden="true" tabindex="-1"></a><span class="do">##   Links: mu = logit </span></span>
<span id="cb224-4"><a href="logistic-regression.html#cb224-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Formula: padult ~ g0_s * pfemale + (g0_s * pfemale | subj) + (1 | speaker) </span></span>
<span id="cb224-5"><a href="logistic-regression.html#cb224-5" aria-hidden="true" tabindex="-1"></a><span class="do">##    Data: h95 (Number of observations: 2780) </span></span>
<span id="cb224-6"><a href="logistic-regression.html#cb224-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 10;</span></span>
<span id="cb224-7"><a href="logistic-regression.html#cb224-7" aria-hidden="true" tabindex="-1"></a><span class="do">##          total post-warmup samples = 4000</span></span>
<span id="cb224-8"><a href="logistic-regression.html#cb224-8" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb224-9"><a href="logistic-regression.html#cb224-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Group-Level Effects: </span></span>
<span id="cb224-10"><a href="logistic-regression.html#cb224-10" aria-hidden="true" tabindex="-1"></a><span class="do">## ~speaker (Number of levels: 139) </span></span>
<span id="cb224-11"><a href="logistic-regression.html#cb224-11" aria-hidden="true" tabindex="-1"></a><span class="do">##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS</span></span>
<span id="cb224-12"><a href="logistic-regression.html#cb224-12" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(Intercept)     2.77      0.35     2.18     3.52 1.00     2842     3647</span></span>
<span id="cb224-13"><a href="logistic-regression.html#cb224-13" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb224-14"><a href="logistic-regression.html#cb224-14" aria-hidden="true" tabindex="-1"></a><span class="do">## ~subj (Number of levels: 10) </span></span>
<span id="cb224-15"><a href="logistic-regression.html#cb224-15" aria-hidden="true" tabindex="-1"></a><span class="do">##                             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS</span></span>
<span id="cb224-16"><a href="logistic-regression.html#cb224-16" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(Intercept)                   1.32      0.41     0.74     2.34 1.00     3947</span></span>
<span id="cb224-17"><a href="logistic-regression.html#cb224-17" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(g0_s)                        1.37      0.42     0.71     2.36 1.00     4027</span></span>
<span id="cb224-18"><a href="logistic-regression.html#cb224-18" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(pfemale)                     0.28      0.22     0.01     0.80 1.00     3934</span></span>
<span id="cb224-19"><a href="logistic-regression.html#cb224-19" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(g0_s:pfemale)                1.29      0.47     0.61     2.45 1.00     4124</span></span>
<span id="cb224-20"><a href="logistic-regression.html#cb224-20" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,g0_s)            -0.45      0.26    -0.85     0.13 1.00     3278</span></span>
<span id="cb224-21"><a href="logistic-regression.html#cb224-21" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,pfemale)          0.16      0.37    -0.60     0.80 1.00     3893</span></span>
<span id="cb224-22"><a href="logistic-regression.html#cb224-22" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(g0_s,pfemale)              -0.13      0.36    -0.75     0.61 1.00     3833</span></span>
<span id="cb224-23"><a href="logistic-regression.html#cb224-23" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,g0_s:pfemale)     0.10      0.30    -0.48     0.66 1.00     4077</span></span>
<span id="cb224-24"><a href="logistic-regression.html#cb224-24" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(g0_s,g0_s:pfemale)         -0.48      0.26    -0.88     0.13 1.00     3762</span></span>
<span id="cb224-25"><a href="logistic-regression.html#cb224-25" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(pfemale,g0_s:pfemale)      -0.01      0.36    -0.68     0.67 1.00     3818</span></span>
<span id="cb224-26"><a href="logistic-regression.html#cb224-26" aria-hidden="true" tabindex="-1"></a><span class="do">##                             Tail_ESS</span></span>
<span id="cb224-27"><a href="logistic-regression.html#cb224-27" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(Intercept)                   4013</span></span>
<span id="cb224-28"><a href="logistic-regression.html#cb224-28" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(g0_s)                        3797</span></span>
<span id="cb224-29"><a href="logistic-regression.html#cb224-29" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(pfemale)                     4056</span></span>
<span id="cb224-30"><a href="logistic-regression.html#cb224-30" aria-hidden="true" tabindex="-1"></a><span class="do">## sd(g0_s:pfemale)                3885</span></span>
<span id="cb224-31"><a href="logistic-regression.html#cb224-31" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,g0_s)             3952</span></span>
<span id="cb224-32"><a href="logistic-regression.html#cb224-32" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,pfemale)          3925</span></span>
<span id="cb224-33"><a href="logistic-regression.html#cb224-33" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(g0_s,pfemale)               4057</span></span>
<span id="cb224-34"><a href="logistic-regression.html#cb224-34" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(Intercept,g0_s:pfemale)     3808</span></span>
<span id="cb224-35"><a href="logistic-regression.html#cb224-35" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(g0_s,g0_s:pfemale)          3842</span></span>
<span id="cb224-36"><a href="logistic-regression.html#cb224-36" aria-hidden="true" tabindex="-1"></a><span class="do">## cor(pfemale,g0_s:pfemale)       3926</span></span>
<span id="cb224-37"><a href="logistic-regression.html#cb224-37" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb224-38"><a href="logistic-regression.html#cb224-38" aria-hidden="true" tabindex="-1"></a><span class="do">## Population-Level Effects: </span></span>
<span id="cb224-39"><a href="logistic-regression.html#cb224-39" aria-hidden="true" tabindex="-1"></a><span class="do">##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS</span></span>
<span id="cb224-40"><a href="logistic-regression.html#cb224-40" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept       -0.68      0.56    -1.78     0.46 1.00     2956     3572</span></span>
<span id="cb224-41"><a href="logistic-regression.html#cb224-41" aria-hidden="true" tabindex="-1"></a><span class="do">## g0_s            -3.29      0.59    -4.50    -2.17 1.00     2940     3404</span></span>
<span id="cb224-42"><a href="logistic-regression.html#cb224-42" aria-hidden="true" tabindex="-1"></a><span class="do">## pfemale          2.31      0.25     1.84     2.80 1.00     3297     3617</span></span>
<span id="cb224-43"><a href="logistic-regression.html#cb224-43" aria-hidden="true" tabindex="-1"></a><span class="do">## g0_s:pfemale     1.18      0.55     0.12     2.30 1.00     4232     4090</span></span>
<span id="cb224-44"><a href="logistic-regression.html#cb224-44" aria-hidden="true" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb224-45"><a href="logistic-regression.html#cb224-45" aria-hidden="true" tabindex="-1"></a><span class="do">## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS</span></span>
<span id="cb224-46"><a href="logistic-regression.html#cb224-46" aria-hidden="true" tabindex="-1"></a><span class="do">## and Tail_ESS are effective sample size measures, and Rhat is the potential</span></span>
<span id="cb224-47"><a href="logistic-regression.html#cb224-47" aria-hidden="true" tabindex="-1"></a><span class="do">## scale reduction factor on split chains (at convergence, Rhat = 1).</span></span></code></pre></div>
<p>Our model is basically three lines relating perceived adultness and f0: the overall (main effects) line, the line for when subjects indicated hearing a male speaker, and the line for for subjects indicated hearing a female speaker. We recover the parameters for these lines by adding the appropriate terms using the hypothesis function.</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="logistic-regression.html#cb225-1" aria-hidden="true" tabindex="-1"></a>logistic_g0_hypothesis <span class="ot">=</span> </span>
<span id="cb225-2"><a href="logistic-regression.html#cb225-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">hypothesis</span> (logistic_g0,</span>
<span id="cb225-3"><a href="logistic-regression.html#cb225-3" aria-hidden="true" tabindex="-1"></a>              <span class="at">hypothesis =</span> </span>
<span id="cb225-4"><a href="logistic-regression.html#cb225-4" aria-hidden="true" tabindex="-1"></a>                <span class="fu">c</span>(<span class="st">&quot;Intercept = 0&quot;</span>, <span class="co"># overall intercept</span></span>
<span id="cb225-5"><a href="logistic-regression.html#cb225-5" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Intercept + pfemale = 0&quot;</span>, <span class="co">#female intercept</span></span>
<span id="cb225-6"><a href="logistic-regression.html#cb225-6" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;Intercept - pfemale = 0&quot;</span>, <span class="co"># male intercept</span></span>
<span id="cb225-7"><a href="logistic-regression.html#cb225-7" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;g0_s = 0&quot;</span>, <span class="co"># overall slope</span></span>
<span id="cb225-8"><a href="logistic-regression.html#cb225-8" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;g0_s + g0_s:pfemale = 0&quot;</span>, <span class="co"># female slope</span></span>
<span id="cb225-9"><a href="logistic-regression.html#cb225-9" aria-hidden="true" tabindex="-1"></a>                  <span class="st">&quot;g0_s - g0_s:pfemale = 0&quot;</span>) <span class="co"># male slope</span></span>
<span id="cb225-10"><a href="logistic-regression.html#cb225-10" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb225-11"><a href="logistic-regression.html#cb225-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb225-12"><a href="logistic-regression.html#cb225-12" aria-hidden="true" tabindex="-1"></a>logistic_g0_hypothesis[[<span class="dv">1</span>]][,<span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb225-13"><a href="logistic-regression.html#cb225-13" aria-hidden="true" tabindex="-1"></a><span class="do">##     Estimate Est.Error   CI.Lower   CI.Upper</span></span>
<span id="cb225-14"><a href="logistic-regression.html#cb225-14" aria-hidden="true" tabindex="-1"></a><span class="do">## 1 -0.6751557 0.5571161 -1.7789047  0.4633089</span></span>
<span id="cb225-15"><a href="logistic-regression.html#cb225-15" aria-hidden="true" tabindex="-1"></a><span class="do">## 2  1.6316161 0.6212552  0.4057239  2.8991426</span></span>
<span id="cb225-16"><a href="logistic-regression.html#cb225-16" aria-hidden="true" tabindex="-1"></a><span class="do">## 3 -2.9819274 0.5970852 -4.1640600 -1.7486627</span></span>
<span id="cb225-17"><a href="logistic-regression.html#cb225-17" aria-hidden="true" tabindex="-1"></a><span class="do">## 4 -3.2941081 0.5917762 -4.5000468 -2.1679406</span></span>
<span id="cb225-18"><a href="logistic-regression.html#cb225-18" aria-hidden="true" tabindex="-1"></a><span class="do">## 5 -2.1140444 0.7215239 -3.5229921 -0.6624104</span></span>
<span id="cb225-19"><a href="logistic-regression.html#cb225-19" aria-hidden="true" tabindex="-1"></a><span class="do">## 6 -4.4741717 0.8851670 -6.3591675 -2.8303016</span></span></code></pre></div>
<p>We can use these parameters to plot lines predicting the logit of the probability of a male response given f0. Below, we see these lines overall, and separately for voices judged to be male or female. A comparison of each prediction line to the classifications of the voices in our model show that this model offers reasonable predictions of the trends in the data.</p>
<div class="figure"><span id="fig:F7-5"></span>
<img src="07_files/figure-html/F7-5-1.png" alt="(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted." width="768" />
<p class="caption">
Figure 7.5: (left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted.
</p>
</div>
<p>Below we see what happens when we apply the logistic transform on the lines above. The result is sigmoid curves that represent expected variation in the <span class="math inline">\(p\)</span> parameter of a Bernoulli distribution as a function of f0. This variation is along straight lines in the logit space but not in the probability space. Obviously, these curves are a much better fit for the data than the lines we originally used in Figure <a href="logistic-regression.html#fig:F7-2">7.2</a>.</p>
<div class="figure"><span id="fig:F7-6"></span>
<img src="07_files/figure-html/F7-6-1.png" alt="(left) Curve indicating the relationship between f0 and the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted." width="768" />
<p class="caption">
Figure 7.6: (left) Curve indicating the relationship between f0 and the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted.
</p>
</div>
</div>
</div>
<div id="answering-our-research-question" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Answering our research question</h2>
<p>In the beginning of the chapter, we considered a pretty broad research question: how is f0 used by listeners to determine adultness? How does this vary based on the perception of gender? In this final section we’re going to try to answer this question using the information presented in our model below, focusing mainly on the fixed effects.</p>
<p>Keep in mind, I don’t have a ‘right answer’ or correct interpretation of the data that I am working towards. This is just an example of the way you might approach understanding real data in order to see what you can learn about your research question.</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="logistic-regression.html#cb226-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fixef</span> (logistic_g0)</span>
<span id="cb226-2"><a href="logistic-regression.html#cb226-2" aria-hidden="true" tabindex="-1"></a><span class="do">##                Estimate Est.Error       Q2.5      Q97.5</span></span>
<span id="cb226-3"><a href="logistic-regression.html#cb226-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Intercept    -0.6751557 0.5571161 -1.7789047  0.4633089</span></span>
<span id="cb226-4"><a href="logistic-regression.html#cb226-4" aria-hidden="true" tabindex="-1"></a><span class="do">## g0_s         -3.2941081 0.5917762 -4.5000468 -2.1679406</span></span>
<span id="cb226-5"><a href="logistic-regression.html#cb226-5" aria-hidden="true" tabindex="-1"></a><span class="do">## pfemale       2.3067717 0.2466901  1.8402720  2.8007507</span></span>
<span id="cb226-6"><a href="logistic-regression.html#cb226-6" aria-hidden="true" tabindex="-1"></a><span class="do">## g0_s:pfemale  1.1800637 0.5494175  0.1188863  2.2988926</span></span></code></pre></div>
<p>We can consider the fixed effects more easily by plotting them as in the left panel in Figure <a href="logistic-regression.html#fig:F7-7">7.7</a>. The most important thing to remember when interpreting the coefficients of a logistic model is that positive coefficients push us towards an ‘adult’ response, negative values push us towards a ‘child’ response, and a predicted value of 0 means the outcome is 50/50. The right panel in Figure <a href="logistic-regression.html#fig:F7-7">7.7</a> shows us a summary of our data, with the ‘main effects’ classification line drawn on top of it.</p>
<p>The negative effect for f0 tells us that our line relating f0 to logits has a negative slope. Therefore, as f0 increases, we are <em>less</em> likely to observe an ‘adult’ response and <em>more</em> likely to observe a ‘child’ response.</p>
<p>The model intercept is the value of the line when x = 0. We can see this below as the value of y where our diagonal line crosses the vertical dotted line. Since we scaled our f0 predictor, our intercept corresponds to the expected logit value when given a mean log f0. So, the intercept of our model reflects the probability of observing a response of ‘adult’ at the average log f0. We can see that when scaled log f0 = 0, there is a small negative value, meaning we have a slight expectation of observing a ‘child’ response.</p>
<p>When we use models based on logits, we might actually be interested in the <em>x intercept</em> of our lines (or planes). The x intercept of our line is the value of our line along x where <em>y is equal to zero</em>. Why might we care about this? Well, when y = 0, that means that the probability of classification is 0.5. So, crossing the x intercept one way means you are more likely to see a success, and crossing the other way means you are more likely to see a failure. In other words, the x intercept of these models tells us about the location of the <em>category boundary</em> between our two categories, with respect to the continuous predictor.</p>
<div class="figure"><span id="fig:F7-7"></span>
<img src="07_files/figure-html/F7-7-1.png" alt="(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted." width="768" />
<p class="caption">
Figure 7.7: (left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted.
</p>
</div>
<p>The location of the x intercept is presented using a bold vertical line in the <a href="logistic-regression.html#fig:F7-7">7.7</a>. This is the location where our prediction line intersects with the x axis. We can find the x intercept by setting y=0 in our prediction equation and solving for (i.e. isolating) x, as seen below. For complicated prediction equations (and even for simple ones), I often rely on <a href="https://www.mathpapa.com/algebra-calculator.html">websites like this one</a>.</p>
<p><span class="math display" id="eq:711">\[
\begin{equation}
\begin{split}
y = a + b*x \\
0 = a + b*x \\
-a = b*x \\ 
-a/b = x
\end{split}
\tag{7.13}
\end{equation}
\]</span></p>
<p>We the above equation to calculate our category boundary between adult and child classifications. Remember that to do arithmetic operations on our parameters, we have to use the original samples and not the summaries. For example, based on the numbers in our printout above, we expect that the category boundary is at <code>-(a/b) = -(-0.67 / -3.31) = -0.20</code>. Instead, when I do the same calculation using our original samples below, we see that the boundary is actually nearer to <code>-0.22</code>.</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="logistic-regression.html#cb227-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get original samples</span></span>
<span id="cb227-2"><a href="logistic-regression.html#cb227-2" aria-hidden="true" tabindex="-1"></a>fixef_samples <span class="ot">=</span> <span class="fu">fixef</span> (logistic_g0, <span class="at">summary =</span> <span class="cn">FALSE</span>)</span>
<span id="cb227-3"><a href="logistic-regression.html#cb227-3" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate boundary = -a/b</span></span>
<span id="cb227-4"><a href="logistic-regression.html#cb227-4" aria-hidden="true" tabindex="-1"></a>boundary <span class="ot">=</span> <span class="sc">-</span>fixef_samples[,<span class="st">&quot;Intercept&quot;</span>] <span class="sc">/</span> fixef_samples[,<span class="st">&quot;g0_s&quot;</span>]</span>
<span id="cb227-5"><a href="logistic-regression.html#cb227-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb227-6"><a href="logistic-regression.html#cb227-6" aria-hidden="true" tabindex="-1"></a><span class="fu">posterior_summary</span> (boundary)</span>
<span id="cb227-7"><a href="logistic-regression.html#cb227-7" aria-hidden="true" tabindex="-1"></a><span class="do">##        Estimate Est.Error       Q2.5     Q97.5</span></span>
<span id="cb227-8"><a href="logistic-regression.html#cb227-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,] -0.2241527 0.2042887 -0.6675019 0.1332786</span></span></code></pre></div>
<p>The units above are not super useful. After fitting our model, we can convert our parameters back to ‘unscaled’ log-Hz and then even to ‘regular’ Hertz values. Below, we calculate the mean and standard deviations of the original variable. We then use these to <em>undo</em> our scaling operation by first multiplying by the standard deviation and then adding the mean (in that order!). We can then exponentiate these unscaled log-Hz values to get Hz ranges from the model parameters.</p>
<p>This is basically like, imagine your model was specified in centimeters. If you wanted to understand it in meters, it seems obvious that it would be ok to divide everything by 100. If the model were somehow radically different just because you divided the output by 100, that would obviously be a big problem for us. Scaling and unscaling predictors are these kinds of changes, they don’t really hurt our ability to make reliable inferences as long as we do the operation to all of the samples, and <em>then</em> summarize.</p>
<p>Below I unscale and exponentiate the data, before using the <code>posterior_summary</code> function to get a summary of the samples. Note that we can use this information to get a point estimate for the boundary, in addition to getting information about uncertainty in the estimate of the boundary (via the credible interval).</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="logistic-regression.html#cb228-1" aria-hidden="true" tabindex="-1"></a>h95_sd <span class="ot">=</span> <span class="fu">sd</span>(h95<span class="sc">$</span>g0)   <span class="co"># calculate data mean</span></span>
<span id="cb228-2"><a href="logistic-regression.html#cb228-2" aria-hidden="true" tabindex="-1"></a>h95_mean <span class="ot">=</span> <span class="fu">mean</span>(h95<span class="sc">$</span>g0) <span class="co"># calculate data sd</span></span>
<span id="cb228-3"><a href="logistic-regression.html#cb228-3" aria-hidden="true" tabindex="-1"></a>boundary_unscaled <span class="ot">=</span> boundary <span class="sc">*</span> h95_sd <span class="sc">+</span> h95_mean <span class="co"># unscale data</span></span>
<span id="cb228-4"><a href="logistic-regression.html#cb228-4" aria-hidden="true" tabindex="-1"></a>boundary_hz <span class="ot">=</span> <span class="fu">exp</span> (boundary_unscaled) <span class="co"># exponentiate to get hertz</span></span>
<span id="cb228-5"><a href="logistic-regression.html#cb228-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb228-6"><a href="logistic-regression.html#cb228-6" aria-hidden="true" tabindex="-1"></a><span class="fu">posterior_summary</span> (boundary_hz) <span class="co"># check out boundary statistics</span></span>
<span id="cb228-7"><a href="logistic-regression.html#cb228-7" aria-hidden="true" tabindex="-1"></a><span class="do">##      Estimate Est.Error     Q2.5    Q97.5</span></span>
<span id="cb228-8"><a href="logistic-regression.html#cb228-8" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,] 178.1333  10.55043 155.8174 197.7817</span></span></code></pre></div>
<p>In Figure <a href="logistic-regression.html#fig:F7-8">7.8</a> I present a histogram of the posterior distribution of the x-intercept of the line in our model. As we can see, our model suggests that this is likely between 160 and 200, with the mean being at around 180 Hz. We can see this distribution presented along with our data and ‘main effects’ line in the figure on the right below.</p>
<p>Notice that the plot below uses Hz labels rather than log-Hz. Our plots so far had ticks at -2, -2, 0 and 1 scaled log g0. I used the process above to calculate the Hz value corresponding to each of those scaled log-Hz values as shown below:</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="logistic-regression.html#cb229-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span> ( <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="sc">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>) <span class="sc">*</span> h95_sd <span class="sc">+</span> h95_mean )</span>
<span id="cb229-2"><a href="logistic-regression.html#cb229-2" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 104.7793 141.1276 190.0852 256.0263</span></span></code></pre></div>
<p>And then used these values to make the labels for the figure.</p>
<div class="figure"><span id="fig:F7-8"></span>
<img src="07_files/figure-html/F7-8-1.png" alt="(left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted." width="768" />
<p class="caption">
Figure 7.8: (left) Line indicating linear relationship between f0 and the logit of the probability of an adult response. (middle) Same as the left but only males are plotted. (middle) Same as the middle but only females are plotted.
</p>
</div>
<p>It can be useful to think about category boundaries when interpreting logits because the effects in our model can be interpreted as shifts in these boundaries. For example, consider Figure <a href="logistic-regression.html#fig:F7-9">7.9</a> which compares our data with the lines generated by our model.</p>
<p>Our model intercept estimate was -0.66, and the effect for <code>pfemale</code> was 2.3. That means that when subjects indicated hearing a female speaker, the lines representing their judgments were shifted up by 2.3 logits. Keep in mind this shift occurs when x=0, so if you haven’t centered your continuous predictors this is all more complicated, especially if your lines also have different slopes.</p>
<p>However, when a line with a fixed negative slope is shifted up it’s x-intercept must <em>increase</em> in value. This causes the category boundary to move to the <em>right</em> on our plots. We can see this below, the higher intercept for the teal line moves the adult/child boundary higher along f0 for speakers perceived as female. We can see the opposite effect for the male line (in red), where the effect of -2.3 (<code>-pfemale</code>) results in a lower line and a leftward move for the category boundary.</p>
<p>Our model slope was -3.3, with the <code>g0_s:pfemale</code> interaction equaling 1.18. This means that when speakers were identified as female, slopes were smaller in magnitude (-3.3 + 1.18) than when speakers were identified as male (-3.3 - 1.18). Increasing the magnitude of our slopes (positive or negative) without changing intercepts does not affect the location of the category boundary. Instead, it results in more ‘categorical’, less fuzzy classifications.</p>
<p>When lines differ in both slopes and intercepts, the effect on classification boundaries needs to be considered on a case by case basis. However in general it is quite straightforward, one only needs to imagine the effects on our lines and the locations where they will cross 0. Below, we can see that the intercept and slope differences between speakers identified as male and female both serve to increase the f0 threshold which a speaker must cross to be identified as an adult.</p>
<div class="figure"><span id="fig:F7-9"></span>
<img src="07_files/figure-html/F7-9-1.png" alt="Diagonal lines indicate the relationship between perceived adultness and f0, overall (black), and for women (teal) and men (coral). Bold vertical lines indicate x-axis intercept (category boundary) for each line." width="768" />
<p class="caption">
Figure 7.9: Diagonal lines indicate the relationship between perceived adultness and f0, overall (black), and for women (teal) and men (coral). Bold vertical lines indicate x-axis intercept (category boundary) for each line.
</p>
</div>
<p>In Figure <a href="logistic-regression.html#fig:F7-10">7.10</a> I present a final figure that might be the most useful to use if this work were being presented somewhere. It contains the data summarized as probabilities and the x-axis labeled with Hz, both of which help interpret the figure.</p>
<div class="figure"><span id="fig:F7-10"></span>
<img src="07_files/figure-html/F7-10-1.png" alt="Curves indicate the relationship between the probability of perceived adultness and f0, overall (black), and for women (teal) and men (coral). Bold vertical lines indicate category boundaries suggested by each line." width="768" />
<p class="caption">
Figure 7.10: Curves indicate the relationship between the probability of perceived adultness and f0, overall (black), and for women (teal) and men (coral). Bold vertical lines indicate category boundaries suggested by each line.
</p>
</div>
<p>We can calculate the gender-dependent boundaries in the same way we calculated the overall one:</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="logistic-regression.html#cb230-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate female boundary by reconstructing female slope and intercept</span></span>
<span id="cb230-2"><a href="logistic-regression.html#cb230-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and then finding -a/b</span></span>
<span id="cb230-3"><a href="logistic-regression.html#cb230-3" aria-hidden="true" tabindex="-1"></a>boundary_female <span class="ot">=</span> <span class="sc">-</span>(fixef_samples[,<span class="st">&quot;Intercept&quot;</span>]<span class="sc">+</span>fixef_samples[,<span class="st">&quot;pfemale&quot;</span>]) <span class="sc">/</span> </span>
<span id="cb230-4"><a href="logistic-regression.html#cb230-4" aria-hidden="true" tabindex="-1"></a>                   (fixef_samples[,<span class="st">&quot;g0_s&quot;</span>]<span class="sc">+</span>fixef_samples[,<span class="st">&quot;g0_s:pfemale&quot;</span>])</span>
<span id="cb230-5"><a href="logistic-regression.html#cb230-5" aria-hidden="true" tabindex="-1"></a><span class="co"># same for males</span></span>
<span id="cb230-6"><a href="logistic-regression.html#cb230-6" aria-hidden="true" tabindex="-1"></a>boundary_male <span class="ot">=</span> <span class="sc">-</span>(fixef_samples[,<span class="st">&quot;Intercept&quot;</span>]<span class="sc">-</span>fixef_samples[,<span class="st">&quot;pfemale&quot;</span>]) <span class="sc">/</span> </span>
<span id="cb230-7"><a href="logistic-regression.html#cb230-7" aria-hidden="true" tabindex="-1"></a>                 (fixef_samples[,<span class="st">&quot;g0_s&quot;</span>]<span class="sc">-</span>fixef_samples[,<span class="st">&quot;g0_s:pfemale&quot;</span>])</span>
<span id="cb230-8"><a href="logistic-regression.html#cb230-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-9"><a href="logistic-regression.html#cb230-9" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to Hz</span></span>
<span id="cb230-10"><a href="logistic-regression.html#cb230-10" aria-hidden="true" tabindex="-1"></a>boundary_female_hz <span class="ot">=</span> <span class="fu">exp</span> (boundary_female <span class="sc">*</span> h95_sd <span class="sc">+</span> h95_mean) </span>
<span id="cb230-11"><a href="logistic-regression.html#cb230-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-12"><a href="logistic-regression.html#cb230-12" aria-hidden="true" tabindex="-1"></a><span class="co"># convert to Hz</span></span>
<span id="cb230-13"><a href="logistic-regression.html#cb230-13" aria-hidden="true" tabindex="-1"></a>boundary_male_hz <span class="ot">=</span> <span class="fu">exp</span> (boundary_male <span class="sc">*</span> h95_sd <span class="sc">+</span> h95_mean) </span></code></pre></div>
<p>Below, I summarize the posterior distribution of these boundaries using the <code>posterior_summary</code> function, however I set <code>robust</code> to <code>TRUE</code>. This relies on medians rather than means to provide an estimate of the parameter and its variation. The reason for this is that since we are dividing by a variable that sometimes have very small values, some of our samples result in extremely deviant estimates of the category boundary (i.e., 50,000). So, using the <code>robust</code> option offers protection from these sorts of issues.</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="logistic-regression.html#cb231-1" aria-hidden="true" tabindex="-1"></a><span class="co"># overall boundary</span></span>
<span id="cb231-2"><a href="logistic-regression.html#cb231-2" aria-hidden="true" tabindex="-1"></a><span class="fu">posterior_summary</span> (boundary_hz, <span class="at">robust =</span> <span class="cn">TRUE</span>) </span>
<span id="cb231-3"><a href="logistic-regression.html#cb231-3" aria-hidden="true" tabindex="-1"></a><span class="do">##      Estimate Est.Error     Q2.5    Q97.5</span></span>
<span id="cb231-4"><a href="logistic-regression.html#cb231-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,] 178.8536  9.252209 155.8174 197.7817</span></span>
<span id="cb231-5"><a href="logistic-regression.html#cb231-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb231-6"><a href="logistic-regression.html#cb231-6" aria-hidden="true" tabindex="-1"></a><span class="co"># female boundary</span></span>
<span id="cb231-7"><a href="logistic-regression.html#cb231-7" aria-hidden="true" tabindex="-1"></a><span class="fu">posterior_summary</span> (boundary_female_hz, <span class="at">robust =</span> <span class="cn">TRUE</span>) </span>
<span id="cb231-8"><a href="logistic-regression.html#cb231-8" aria-hidden="true" tabindex="-1"></a><span class="do">##      Estimate Est.Error     Q2.5    Q97.5</span></span>
<span id="cb231-9"><a href="logistic-regression.html#cb231-9" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,]   238.83  18.20462 204.6917 326.4966</span></span>
<span id="cb231-10"><a href="logistic-regression.html#cb231-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb231-11"><a href="logistic-regression.html#cb231-11" aria-hidden="true" tabindex="-1"></a><span class="co"># male boundary</span></span>
<span id="cb231-12"><a href="logistic-regression.html#cb231-12" aria-hidden="true" tabindex="-1"></a><span class="fu">posterior_summary</span> (boundary_male_hz, <span class="at">robust =</span> <span class="cn">TRUE</span>) </span>
<span id="cb231-13"><a href="logistic-regression.html#cb231-13" aria-hidden="true" tabindex="-1"></a><span class="do">##      Estimate Est.Error     Q2.5    Q97.5</span></span>
<span id="cb231-14"><a href="logistic-regression.html#cb231-14" aria-hidden="true" tabindex="-1"></a><span class="do">## [1,] 155.5704  8.456514 131.9241 171.4507</span></span></code></pre></div>
<p>The boundaries, Figure <a href="logistic-regression.html#fig:F7-10">7.10</a>, and the fixed effects can be used to arrive at the following general conclusions. The overall boundary is at about 180 Hz, although this is not very useful: all adult females would be classified as children given this boundary (see the location of the teal points in Figure <a href="logistic-regression.html#fig:F7-10">7.10</a>).</p>
<p>Instead, listeners appear to use gender-dependent boundaries, a higher threshold for females and a lower one for males. Of course, subjects were not given information about speaker gender so if this were being used, it would have to be guessed from the stimulus just as adultness is being guessed.</p>
<p>Finally, in Figure <a href="logistic-regression.html#fig:F7-9">7.9</a> the teal line representing female classifications had a slope with a smaller magnitude than the red line representing the male classifications. As noted above this suggests ‘fuzzier’, less distinguishable categories. We can see in Figure <a href="logistic-regression.html#fig:F7-10">7.10</a> that boys and men are more separable, leading to a more ‘vertical’ sigmoid curve bentween the categories along the x axis. In contrast, girls and women largely overlap along f0 resulting in a more ‘horizontal’ sigmoid curve between them, reflecting the co-mingling of categories.</p>
</div>
<div id="plot-code-6" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Plot Code</h2>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="logistic-regression.html#cb232-1" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-2"><a href="logistic-regression.html#cb232-2" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 7.1</span></span>
<span id="cb232-3"><a href="logistic-regression.html#cb232-3" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-4"><a href="logistic-regression.html#cb232-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-5"><a href="logistic-regression.html#cb232-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">oma =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb232-6"><a href="logistic-regression.html#cb232-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-7"><a href="logistic-regression.html#cb232-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0, agg_males<span class="sc">$</span>pheight, <span class="at">cex =</span><span class="dv">2</span>, <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group], </span>
<span id="cb232-8"><a href="logistic-regression.html#cb232-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">4.5</span>,<span class="fl">5.75</span>),  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">45</span>,<span class="dv">75</span>),<span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb232-9"><a href="logistic-regression.html#cb232-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;Height (inches)&quot;</span>)</span>
<span id="cb232-10"><a href="logistic-regression.html#cb232-10" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb232-11"><a href="logistic-regression.html#cb232-11" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_females<span class="sc">$</span>g0, agg_females<span class="sc">$</span>pheight, <span class="at">cex =</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb232-12"><a href="logistic-regression.html#cb232-12" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group])</span>
<span id="cb232-13"><a href="logistic-regression.html#cb232-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-14"><a href="logistic-regression.html#cb232-14" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="fl">4.6</span>,<span class="dv">60</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Boys&quot;</span>,<span class="st">&quot;Girls&quot;</span>,<span class="st">&quot;Men&quot;</span>,<span class="st">&quot;Women&quot;</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">0</span>,</span>
<span id="cb232-15"><a href="logistic-regression.html#cb232-15" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> cols[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>], <span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pch=</span><span class="dv">1</span>,<span class="at">pt.cex=</span><span class="fl">1.5</span>)</span>
<span id="cb232-16"><a href="logistic-regression.html#cb232-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-17"><a href="logistic-regression.html#cb232-17" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (pheight <span class="sc">~</span> g0, <span class="at">data =</span> agg_males)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-18"><a href="logistic-regression.html#cb232-18" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (pheight <span class="sc">~</span> g0, <span class="at">data =</span> agg_females)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-19"><a href="logistic-regression.html#cb232-19" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (pheight <span class="sc">~</span> g0, <span class="at">data =</span> h95)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>, <span class="at">col=</span><span class="dv">1</span>)</span>
<span id="cb232-20"><a href="logistic-regression.html#cb232-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-21"><a href="logistic-regression.html#cb232-21" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0, agg_males<span class="sc">$</span>pheight, <span class="at">cex =</span><span class="dv">2</span>, <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group], </span>
<span id="cb232-22"><a href="logistic-regression.html#cb232-22" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">4.5</span>,<span class="fl">5.75</span>),  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">45</span>,<span class="dv">75</span>),<span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb232-23"><a href="logistic-regression.html#cb232-23" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;Height (inches)&quot;</span>)</span>
<span id="cb232-24"><a href="logistic-regression.html#cb232-24" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb232-25"><a href="logistic-regression.html#cb232-25" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (pheight <span class="sc">~</span> g0, <span class="at">data =</span> agg_females)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-26"><a href="logistic-regression.html#cb232-26" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (pheight <span class="sc">~</span> g0, <span class="at">data =</span> agg_males)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">col=</span>coral)</span>
<span id="cb232-27"><a href="logistic-regression.html#cb232-27" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (pheight <span class="sc">~</span> g0, <span class="at">data =</span> h95)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>, <span class="at">col=</span><span class="dv">1</span>)</span>
<span id="cb232-28"><a href="logistic-regression.html#cb232-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-29"><a href="logistic-regression.html#cb232-29" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_females<span class="sc">$</span>g0, agg_females<span class="sc">$</span>pheight, <span class="at">cex =</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">45</span>,<span class="dv">75</span>),</span>
<span id="cb232-30"><a href="logistic-regression.html#cb232-30" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group], <span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">4.5</span>,<span class="fl">5.75</span>),<span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb232-31"><a href="logistic-regression.html#cb232-31" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;Height (inches)&quot;</span>)</span>
<span id="cb232-32"><a href="logistic-regression.html#cb232-32" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb232-33"><a href="logistic-regression.html#cb232-33" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (pheight <span class="sc">~</span> g0, <span class="at">data =</span> agg_males)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-34"><a href="logistic-regression.html#cb232-34" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (pheight <span class="sc">~</span> g0, <span class="at">data =</span> agg_females)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">col=</span>teal)</span>
<span id="cb232-35"><a href="logistic-regression.html#cb232-35" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (pheight <span class="sc">~</span> g0, <span class="at">data =</span> h95)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>, <span class="at">col=</span><span class="dv">1</span>)</span>
<span id="cb232-36"><a href="logistic-regression.html#cb232-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-37"><a href="logistic-regression.html#cb232-37" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">1</span>, <span class="at">text =</span> <span class="st">&quot;f0 (log Hz)&quot;</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, <span class="at">cex =</span> <span class="fl">0.8</span>, <span class="at">line=</span><span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb232-38"><a href="logistic-regression.html#cb232-38" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">2</span>, <span class="at">text =</span> <span class="st">&quot;Height (inches)&quot;</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, <span class="at">cex =</span> <span class="fl">0.9</span>, <span class="at">line=</span><span class="dv">2</span>)</span>
<span id="cb232-39"><a href="logistic-regression.html#cb232-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-40"><a href="logistic-regression.html#cb232-40" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-41"><a href="logistic-regression.html#cb232-41" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 7.2</span></span>
<span id="cb232-42"><a href="logistic-regression.html#cb232-42" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-43"><a href="logistic-regression.html#cb232-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-44"><a href="logistic-regression.html#cb232-44" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">oma =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb232-45"><a href="logistic-regression.html#cb232-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-46"><a href="logistic-regression.html#cb232-46" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0, agg_males<span class="sc">$</span>padult, <span class="at">cex =</span><span class="dv">2</span>, <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group], </span>
<span id="cb232-47"><a href="logistic-regression.html#cb232-47" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">4.5</span>,<span class="fl">5.75</span>),  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb232-48"><a href="logistic-regression.html#cb232-48" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb232-49"><a href="logistic-regression.html#cb232-49" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_females<span class="sc">$</span>g0, agg_females<span class="sc">$</span>padult, <span class="at">cex =</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>,</span>
<span id="cb232-50"><a href="logistic-regression.html#cb232-50" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group])</span>
<span id="cb232-51"><a href="logistic-regression.html#cb232-51" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb232-52"><a href="logistic-regression.html#cb232-52" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (padult <span class="sc">~</span> g0, <span class="at">data =</span> agg_males)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-53"><a href="logistic-regression.html#cb232-53" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (padult <span class="sc">~</span> g0, <span class="at">data =</span> agg_females)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-54"><a href="logistic-regression.html#cb232-54" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (padult <span class="sc">~</span> g0, <span class="at">data =</span> h95)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>, <span class="at">col=</span><span class="dv">1</span>)</span>
<span id="cb232-55"><a href="logistic-regression.html#cb232-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-56"><a href="logistic-regression.html#cb232-56" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="fl">4.6</span>,<span class="fl">0.5</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Boys&quot;</span>,<span class="st">&quot;Girls&quot;</span>,<span class="st">&quot;Men&quot;</span>,<span class="st">&quot;Women&quot;</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">0</span>,</span>
<span id="cb232-57"><a href="logistic-regression.html#cb232-57" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> cols[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>], <span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pch=</span><span class="dv">1</span>,<span class="at">pt.cex=</span><span class="fl">1.5</span>)</span>
<span id="cb232-58"><a href="logistic-regression.html#cb232-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-59"><a href="logistic-regression.html#cb232-59" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0, agg_males<span class="sc">$</span>padult, <span class="at">cex =</span><span class="dv">2</span>, <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group], </span>
<span id="cb232-60"><a href="logistic-regression.html#cb232-60" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">4.5</span>,<span class="fl">5.75</span>),  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb232-61"><a href="logistic-regression.html#cb232-61" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb232-62"><a href="logistic-regression.html#cb232-62" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb232-63"><a href="logistic-regression.html#cb232-63" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (padult <span class="sc">~</span> g0, <span class="at">data =</span> agg_females)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-64"><a href="logistic-regression.html#cb232-64" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (padult <span class="sc">~</span> g0, <span class="at">data =</span> agg_males)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">col=</span>coral)</span>
<span id="cb232-65"><a href="logistic-regression.html#cb232-65" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (padult <span class="sc">~</span> g0, <span class="at">data =</span> h95)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>, <span class="at">col=</span><span class="dv">1</span>)</span>
<span id="cb232-66"><a href="logistic-regression.html#cb232-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-67"><a href="logistic-regression.html#cb232-67" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_females<span class="sc">$</span>g0, agg_females<span class="sc">$</span>padult, <span class="at">cex =</span><span class="dv">2</span>, <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb232-68"><a href="logistic-regression.html#cb232-68" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group], <span class="at">xlim=</span><span class="fu">c</span>(<span class="fl">4.5</span>,<span class="fl">5.75</span>),<span class="at">xlab =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb232-69"><a href="logistic-regression.html#cb232-69" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb232-70"><a href="logistic-regression.html#cb232-70" aria-hidden="true" tabindex="-1"></a><span class="fu">grid</span>()</span>
<span id="cb232-71"><a href="logistic-regression.html#cb232-71" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (padult <span class="sc">~</span> g0, <span class="at">data =</span> agg_males)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-72"><a href="logistic-regression.html#cb232-72" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (padult <span class="sc">~</span> g0, <span class="at">data =</span> agg_females)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>,<span class="at">col=</span>teal)</span>
<span id="cb232-73"><a href="logistic-regression.html#cb232-73" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span> (padult <span class="sc">~</span> g0, <span class="at">data =</span> h95)<span class="sc">$</span>coefficients, <span class="at">lwd=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">3</span>, <span class="at">col=</span><span class="dv">1</span>)</span>
<span id="cb232-74"><a href="logistic-regression.html#cb232-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-75"><a href="logistic-regression.html#cb232-75" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">1</span>, <span class="at">text =</span> <span class="st">&quot;f0 (log Hz)&quot;</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, <span class="at">cex =</span> <span class="fl">0.9</span>, <span class="at">line=</span><span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb232-76"><a href="logistic-regression.html#cb232-76" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">2</span>, <span class="at">text =</span> <span class="st">&quot;P(response=</span><span class="sc">\&quot;</span><span class="st">adult</span><span class="sc">\&quot;</span><span class="st">)&quot;</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, <span class="at">cex =</span> <span class="fl">0.9</span>, <span class="at">line=</span><span class="dv">2</span>)</span>
<span id="cb232-77"><a href="logistic-regression.html#cb232-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-78"><a href="logistic-regression.html#cb232-78" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-79"><a href="logistic-regression.html#cb232-79" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 7.3</span></span>
<span id="cb232-80"><a href="logistic-regression.html#cb232-80" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-81"><a href="logistic-regression.html#cb232-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-82"><a href="logistic-regression.html#cb232-82" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span> (<span class="sc">-</span><span class="dv">8</span>,<span class="dv">8</span>,.<span class="dv">01</span>)</span>
<span id="cb232-83"><a href="logistic-regression.html#cb232-83" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> x</span>
<span id="cb232-84"><a href="logistic-regression.html#cb232-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-85"><a href="logistic-regression.html#cb232-85" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">1</span>))</span>
<span id="cb232-86"><a href="logistic-regression.html#cb232-86" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (x,y, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span>deepgreen, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="at">main =</span> <span class="st">&quot;y = x&quot;</span>,</span>
<span id="cb232-87"><a href="logistic-regression.html#cb232-87" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab =</span> <span class="st">&quot;Predictor&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Logits&quot;</span>)</span>
<span id="cb232-88"><a href="logistic-regression.html#cb232-88" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">v=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">2</span>),<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-89"><a href="logistic-regression.html#cb232-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-90"><a href="logistic-regression.html#cb232-90" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (x,<span class="fu">ztop</span> (y), <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span>darkorange, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">7</span>,<span class="dv">7</span>), </span>
<span id="cb232-91"><a href="logistic-regression.html#cb232-91" aria-hidden="true" tabindex="-1"></a>      <span class="at">main =</span> <span class="st">&quot;y = logistic ( x )&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Predictor&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Probability&quot;</span>)</span>
<span id="cb232-92"><a href="logistic-regression.html#cb232-92" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,.<span class="dv">5</span>),<span class="at">v=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">2</span>),<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-93"><a href="logistic-regression.html#cb232-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-94"><a href="logistic-regression.html#cb232-94" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (x,<span class="fu">ptoz</span>(<span class="fu">ztop</span> (y)), <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">col=</span>lavender, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">7</span>,<span class="dv">7</span>), </span>
<span id="cb232-95"><a href="logistic-regression.html#cb232-95" aria-hidden="true" tabindex="-1"></a>      <span class="at">main =</span> <span class="st">&quot;y = logit ( logistic ( x ) )&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Predictor&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Logits&quot;</span>)</span>
<span id="cb232-96"><a href="logistic-regression.html#cb232-96" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">v=</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">2</span>),<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-97"><a href="logistic-regression.html#cb232-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-98"><a href="logistic-regression.html#cb232-98" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-99"><a href="logistic-regression.html#cb232-99" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 7.4</span></span>
<span id="cb232-100"><a href="logistic-regression.html#cb232-100" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-101"><a href="logistic-regression.html#cb232-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-102"><a href="logistic-regression.html#cb232-102" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">seq</span> (<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>,.<span class="dv">01</span>)</span>
<span id="cb232-103"><a href="logistic-regression.html#cb232-103" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> x</span>
<span id="cb232-104"><a href="logistic-regression.html#cb232-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-105"><a href="logistic-regression.html#cb232-105" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb232-106"><a href="logistic-regression.html#cb232-106" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (x,y, <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,<span class="at">lwd=</span><span class="dv">3</span>, <span class="at">col=</span>deepgreen, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>),<span class="at">xlab=</span><span class="st">&quot;Predictor (x)&quot;</span>,</span>
<span id="cb232-107"><a href="logistic-regression.html#cb232-107" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab =</span> <span class="st">&quot;Logits (log (p) - log(1-p))&quot;</span>)</span>
<span id="cb232-108"><a href="logistic-regression.html#cb232-108" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="fu">ptoz</span>(<span class="fu">seq</span>(<span class="fl">0.1</span>,<span class="fl">0.9</span>,.<span class="dv">1</span>)),<span class="at">v=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">9</span><span class="sc">:</span><span class="dv">9</span>),<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-109"><a href="logistic-regression.html#cb232-109" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h =</span> <span class="dv">0</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-110"><a href="logistic-regression.html#cb232-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-111"><a href="logistic-regression.html#cb232-111" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (x,<span class="fu">ztop</span> (y), <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,<span class="at">lwd=</span><span class="dv">3</span>, <span class="at">col=</span>darkorange, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>,<span class="dv">3</span>),</span>
<span id="cb232-112"><a href="logistic-regression.html#cb232-112" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab=</span><span class="st">&quot;Predictor (x)&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;Probability&quot;</span>)</span>
<span id="cb232-113"><a href="logistic-regression.html#cb232-113" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,.<span class="dv">1</span>),<span class="at">v=</span>(<span class="sc">-</span><span class="dv">9</span><span class="sc">:</span><span class="dv">9</span>),<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-114"><a href="logistic-regression.html#cb232-114" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h =</span> <span class="fl">0.5</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-115"><a href="logistic-regression.html#cb232-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-116"><a href="logistic-regression.html#cb232-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-117"><a href="logistic-regression.html#cb232-117" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-118"><a href="logistic-regression.html#cb232-118" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 7.5</span></span>
<span id="cb232-119"><a href="logistic-regression.html#cb232-119" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-120"><a href="logistic-regression.html#cb232-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-121"><a href="logistic-regression.html#cb232-121" aria-hidden="true" tabindex="-1"></a>cffs <span class="ot">=</span> logistic_g0_hypothesis[[<span class="dv">1</span>]][,<span class="dv">2</span>]</span>
<span id="cb232-122"><a href="logistic-regression.html#cb232-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-123"><a href="logistic-regression.html#cb232-123" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">oma =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb232-124"><a href="logistic-regression.html#cb232-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-125"><a href="logistic-regression.html#cb232-125" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_males<span class="sc">$</span>padult), <span class="at">cex =</span><span class="dv">2</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>),<span class="at">xlab=</span><span class="st">&quot;&quot;</span>, </span>
<span id="cb232-126"><a href="logistic-regression.html#cb232-126" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group],<span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s))</span>
<span id="cb232-127"><a href="logistic-regression.html#cb232-127" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-128"><a href="logistic-regression.html#cb232-128" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_females<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_females<span class="sc">$</span>padult), <span class="at">cex=</span><span class="dv">2</span>, </span>
<span id="cb232-129"><a href="logistic-regression.html#cb232-129" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-130"><a href="logistic-regression.html#cb232-130" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-131"><a href="logistic-regression.html#cb232-131" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-132"><a href="logistic-regression.html#cb232-132" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">3</span>] <span class="sc">+</span> cffs[<span class="dv">6</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-133"><a href="logistic-regression.html#cb232-133" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> coral, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-134"><a href="logistic-regression.html#cb232-134" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">2</span>] <span class="sc">+</span> cffs[<span class="dv">5</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-135"><a href="logistic-regression.html#cb232-135" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> teal, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-136"><a href="logistic-regression.html#cb232-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-137"><a href="logistic-regression.html#cb232-137" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="sc">-</span><span class="dv">2</span>,<span class="sc">-</span><span class="dv">1</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Boys&quot;</span>,<span class="st">&quot;Girls&quot;</span>,<span class="st">&quot;Men&quot;</span>,<span class="st">&quot;Women&quot;</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">0</span>,</span>
<span id="cb232-138"><a href="logistic-regression.html#cb232-138" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> cols[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>], <span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pch=</span><span class="dv">1</span>,<span class="at">pt.cex=</span><span class="fl">1.5</span>)</span>
<span id="cb232-139"><a href="logistic-regression.html#cb232-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-140"><a href="logistic-regression.html#cb232-140" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_males<span class="sc">$</span>padult), <span class="at">cex =</span><span class="dv">2</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>),<span class="at">xlab=</span><span class="st">&quot;&quot;</span>, </span>
<span id="cb232-141"><a href="logistic-regression.html#cb232-141" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s))</span>
<span id="cb232-142"><a href="logistic-regression.html#cb232-142" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-143"><a href="logistic-regression.html#cb232-143" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-144"><a href="logistic-regression.html#cb232-144" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-145"><a href="logistic-regression.html#cb232-145" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">3</span>] <span class="sc">+</span> cffs[<span class="dv">6</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-146"><a href="logistic-regression.html#cb232-146" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> coral, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-147"><a href="logistic-regression.html#cb232-147" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">2</span>] <span class="sc">+</span> cffs[<span class="dv">5</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-148"><a href="logistic-regression.html#cb232-148" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> teal, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-149"><a href="logistic-regression.html#cb232-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-150"><a href="logistic-regression.html#cb232-150" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_females<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_females<span class="sc">$</span>padult), <span class="at">cex=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>),<span class="at">xlab=</span><span class="st">&quot;&quot;</span>, </span>
<span id="cb232-151"><a href="logistic-regression.html#cb232-151" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s))</span>
<span id="cb232-152"><a href="logistic-regression.html#cb232-152" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-153"><a href="logistic-regression.html#cb232-153" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-154"><a href="logistic-regression.html#cb232-154" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-155"><a href="logistic-regression.html#cb232-155" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">3</span>] <span class="sc">+</span> cffs[<span class="dv">6</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-156"><a href="logistic-regression.html#cb232-156" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> coral, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-157"><a href="logistic-regression.html#cb232-157" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">2</span>] <span class="sc">+</span> cffs[<span class="dv">5</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-158"><a href="logistic-regression.html#cb232-158" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> teal, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-159"><a href="logistic-regression.html#cb232-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-160"><a href="logistic-regression.html#cb232-160" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">1</span>, <span class="at">text =</span> <span class="st">&quot;f0 (log Hz)&quot;</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, <span class="at">cex =</span> <span class="fl">0.8</span>, <span class="at">line=</span><span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb232-161"><a href="logistic-regression.html#cb232-161" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">2</span>, <span class="at">text =</span> <span class="st">&quot;Logit of P(response=</span><span class="sc">\&quot;</span><span class="st">male</span><span class="sc">\&quot;</span><span class="st">)&quot;</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-162"><a href="logistic-regression.html#cb232-162" aria-hidden="true" tabindex="-1"></a>       <span class="at">cex =</span> <span class="fl">0.9</span>, <span class="at">line=</span><span class="dv">2</span>)</span>
<span id="cb232-163"><a href="logistic-regression.html#cb232-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-164"><a href="logistic-regression.html#cb232-164" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-165"><a href="logistic-regression.html#cb232-165" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 7.6</span></span>
<span id="cb232-166"><a href="logistic-regression.html#cb232-166" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-167"><a href="logistic-regression.html#cb232-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-168"><a href="logistic-regression.html#cb232-168" aria-hidden="true" tabindex="-1"></a>cffs <span class="ot">=</span> logistic_g0_hypothesis[[<span class="dv">1</span>]][,<span class="dv">2</span>]</span>
<span id="cb232-169"><a href="logistic-regression.html#cb232-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-170"><a href="logistic-regression.html#cb232-170" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">oma =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb232-171"><a href="logistic-regression.html#cb232-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-172"><a href="logistic-regression.html#cb232-172" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0_s, (agg_males<span class="sc">$</span>padult), <span class="at">cex =</span><span class="dv">2</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,  </span>
<span id="cb232-173"><a href="logistic-regression.html#cb232-173" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">xlim =</span><span class="fu">range</span>(h95<span class="sc">$</span>g0_s))</span>
<span id="cb232-174"><a href="logistic-regression.html#cb232-174" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="fl">0.5</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-175"><a href="logistic-regression.html#cb232-175" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_females<span class="sc">$</span>g0_s, (agg_females<span class="sc">$</span>padult), <span class="at">cex=</span><span class="dv">2</span>, </span>
<span id="cb232-176"><a href="logistic-regression.html#cb232-176" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-177"><a href="logistic-regression.html#cb232-177" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-178"><a href="logistic-regression.html#cb232-178" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-179"><a href="logistic-regression.html#cb232-179" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">3</span>] <span class="sc">+</span> cffs[<span class="dv">6</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-180"><a href="logistic-regression.html#cb232-180" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> coral, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-181"><a href="logistic-regression.html#cb232-181" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">2</span>] <span class="sc">+</span> cffs[<span class="dv">5</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-182"><a href="logistic-regression.html#cb232-182" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> teal, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-183"><a href="logistic-regression.html#cb232-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-184"><a href="logistic-regression.html#cb232-184" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="sc">-</span><span class="dv">2</span>,<span class="fu">ztop</span>(<span class="sc">-</span><span class="dv">1</span>), <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Boys&quot;</span>,<span class="st">&quot;Girls&quot;</span>,<span class="st">&quot;Men&quot;</span>,<span class="st">&quot;Women&quot;</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">0</span>,</span>
<span id="cb232-185"><a href="logistic-regression.html#cb232-185" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> cols[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>], <span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pch=</span><span class="dv">1</span>,<span class="at">pt.cex=</span><span class="fl">1.5</span>)</span>
<span id="cb232-186"><a href="logistic-regression.html#cb232-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-187"><a href="logistic-regression.html#cb232-187" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0_s, (agg_males<span class="sc">$</span>padult), <span class="at">cex =</span><span class="dv">2</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,  </span>
<span id="cb232-188"><a href="logistic-regression.html#cb232-188" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>, <span class="at">xlim =</span><span class="fu">range</span>(h95<span class="sc">$</span>g0_s))</span>
<span id="cb232-189"><a href="logistic-regression.html#cb232-189" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="fl">0.5</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-190"><a href="logistic-regression.html#cb232-190" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-191"><a href="logistic-regression.html#cb232-191" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-192"><a href="logistic-regression.html#cb232-192" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">3</span>] <span class="sc">+</span> cffs[<span class="dv">6</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-193"><a href="logistic-regression.html#cb232-193" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> coral, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-194"><a href="logistic-regression.html#cb232-194" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">2</span>] <span class="sc">+</span> cffs[<span class="dv">5</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-195"><a href="logistic-regression.html#cb232-195" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> teal, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-196"><a href="logistic-regression.html#cb232-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-197"><a href="logistic-regression.html#cb232-197" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_females<span class="sc">$</span>g0_s, (agg_females<span class="sc">$</span>padult), <span class="at">cex=</span><span class="dv">2</span>,<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,  </span>
<span id="cb232-198"><a href="logistic-regression.html#cb232-198" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s))</span>
<span id="cb232-199"><a href="logistic-regression.html#cb232-199" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="fl">0.5</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-200"><a href="logistic-regression.html#cb232-200" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-201"><a href="logistic-regression.html#cb232-201" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-202"><a href="logistic-regression.html#cb232-202" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">3</span>] <span class="sc">+</span> cffs[<span class="dv">6</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-203"><a href="logistic-regression.html#cb232-203" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> coral, <span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-204"><a href="logistic-regression.html#cb232-204" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">2</span>] <span class="sc">+</span> cffs[<span class="dv">5</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-205"><a href="logistic-regression.html#cb232-205" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> teal, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-206"><a href="logistic-regression.html#cb232-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-207"><a href="logistic-regression.html#cb232-207" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">1</span>, <span class="at">text =</span> <span class="st">&quot;f0 (log Hz)&quot;</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, <span class="at">cex =</span> <span class="fl">0.8</span>, <span class="at">line=</span><span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb232-208"><a href="logistic-regression.html#cb232-208" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">2</span>, <span class="at">text =</span> <span class="st">&quot;P(response=</span><span class="sc">\&quot;</span><span class="st">male</span><span class="sc">\&quot;</span><span class="st">)&quot;</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, <span class="at">cex =</span> <span class="fl">0.9</span>,<span class="at">line=</span><span class="dv">2</span>)</span>
<span id="cb232-209"><a href="logistic-regression.html#cb232-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-210"><a href="logistic-regression.html#cb232-210" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-211"><a href="logistic-regression.html#cb232-211" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 7.7</span></span>
<span id="cb232-212"><a href="logistic-regression.html#cb232-212" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-213"><a href="logistic-regression.html#cb232-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-214"><a href="logistic-regression.html#cb232-214" aria-hidden="true" tabindex="-1"></a>cffs <span class="ot">=</span> logistic_g0_hypothesis[[<span class="dv">1</span>]][,<span class="dv">2</span>]</span>
<span id="cb232-215"><a href="logistic-regression.html#cb232-215" aria-hidden="true" tabindex="-1"></a>fixed_effects <span class="ot">=</span> <span class="fu">fixef</span>(logistic_g0)</span>
<span id="cb232-216"><a href="logistic-regression.html#cb232-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-217"><a href="logistic-regression.html#cb232-217" aria-hidden="true" tabindex="-1"></a><span class="fu">layout</span> (<span class="at">mat =</span> <span class="fu">t</span>(<span class="fu">as.matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>))))</span>
<span id="cb232-218"><a href="logistic-regression.html#cb232-218" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mar =</span> <span class="fu">c</span>(<span class="fl">4.2</span>,<span class="fl">4.5</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb232-219"><a href="logistic-regression.html#cb232-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-220"><a href="logistic-regression.html#cb232-220" aria-hidden="true" tabindex="-1"></a><span class="fu">brmplot</span> (fixed_effects, <span class="at">grid =</span> <span class="cn">TRUE</span>,<span class="at">cex.axis=</span><span class="fl">1.2</span>,<span class="at">cex.lab=</span><span class="fl">1.2</span>)</span>
<span id="cb232-221"><a href="logistic-regression.html#cb232-221" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-222"><a href="logistic-regression.html#cb232-222" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span> (<span class="at">side=</span><span class="dv">2</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, <span class="at">text =</span> <span class="st">&quot;Effect (logits)&quot;</span>, <span class="at">line=</span> <span class="sc">-</span><span class="fl">1.5</span>, <span class="at">cex=</span>.<span class="dv">9</span>)</span>
<span id="cb232-223"><a href="logistic-regression.html#cb232-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-224"><a href="logistic-regression.html#cb232-224" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_males<span class="sc">$</span>padult), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>),</span>
<span id="cb232-225"><a href="logistic-regression.html#cb232-225" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlab=</span><span class="st">&quot;f0 (scaled log Hz)&quot;</span>, <span class="at">cex.axis=</span><span class="fl">1.1</span>,<span class="at">cex.lab=</span><span class="fl">1.1</span>,</span>
<span id="cb232-226"><a href="logistic-regression.html#cb232-226" aria-hidden="true" tabindex="-1"></a>      <span class="at">type=</span><span class="st">&#39;n&#39;</span>,<span class="at">ylab=</span><span class="st">&quot;Logit of P(response=</span><span class="sc">\&quot;</span><span class="st">male</span><span class="sc">\&quot;</span><span class="st">)&quot;</span>,<span class="at">xlim =</span><span class="fu">range</span>(h95<span class="sc">$</span>g0_s))</span>
<span id="cb232-227"><a href="logistic-regression.html#cb232-227" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">v=</span><span class="dv">0</span>,<span class="at">lwd=</span><span class="dv">1</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-228"><a href="logistic-regression.html#cb232-228" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_males<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_males<span class="sc">$</span>padult), <span class="at">cex =</span><span class="dv">2</span>,  </span>
<span id="cb232-229"><a href="logistic-regression.html#cb232-229" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-230"><a href="logistic-regression.html#cb232-230" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_females<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_females<span class="sc">$</span>padult), <span class="at">cex=</span><span class="dv">2</span>, </span>
<span id="cb232-231"><a href="logistic-regression.html#cb232-231" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-232"><a href="logistic-regression.html#cb232-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-233"><a href="logistic-regression.html#cb232-233" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-234"><a href="logistic-regression.html#cb232-234" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">1</span>)</span>
<span id="cb232-235"><a href="logistic-regression.html#cb232-235" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="sc">-</span>(cffs[<span class="dv">1</span>]) <span class="sc">/</span> (cffs[<span class="dv">4</span>]); <span class="fu">abline</span> (<span class="at">v =</span> x, <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-236"><a href="logistic-regression.html#cb232-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-237"><a href="logistic-regression.html#cb232-237" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="sc">-</span><span class="dv">2</span>,<span class="sc">-</span><span class="dv">1</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Boys&quot;</span>,<span class="st">&quot;Girls&quot;</span>,<span class="st">&quot;Men&quot;</span>,<span class="st">&quot;Women&quot;</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">0</span>,</span>
<span id="cb232-238"><a href="logistic-regression.html#cb232-238" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> cols[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>], <span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pch=</span><span class="dv">1</span>,<span class="at">pt.cex=</span><span class="fl">1.5</span>)</span>
<span id="cb232-239"><a href="logistic-regression.html#cb232-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-240"><a href="logistic-regression.html#cb232-240" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-241"><a href="logistic-regression.html#cb232-241" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 7.8</span></span>
<span id="cb232-242"><a href="logistic-regression.html#cb232-242" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-243"><a href="logistic-regression.html#cb232-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-244"><a href="logistic-regression.html#cb232-244" aria-hidden="true" tabindex="-1"></a><span class="fu">layout</span> (<span class="at">mat =</span> <span class="fu">t</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)), <span class="at">widths =</span> <span class="fu">c</span>(.<span class="dv">35</span>, .<span class="dv">65</span>))</span>
<span id="cb232-245"><a href="logistic-regression.html#cb232-245" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="fl">4.5</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb232-246"><a href="logistic-regression.html#cb232-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-247"><a href="logistic-regression.html#cb232-247" aria-hidden="true" tabindex="-1"></a>den <span class="ot">=</span> <span class="fu">density</span>(boundary_hz)</span>
<span id="cb232-248"><a href="logistic-regression.html#cb232-248" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span> (boundary_hz, <span class="at">col=</span>lightpink, <span class="at">breaks =</span> <span class="dv">60</span>, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">freq =</span> <span class="cn">FALSE</span>)</span>
<span id="cb232-249"><a href="logistic-regression.html#cb232-249" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span> (den<span class="sc">$</span>x, den<span class="sc">$</span>y,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">1</span>)</span>
<span id="cb232-250"><a href="logistic-regression.html#cb232-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-251"><a href="logistic-regression.html#cb232-251" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_males<span class="sc">$</span>padult), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>),<span class="at">xlab=</span><span class="st">&quot;f0 (Hz)&quot;</span>, </span>
<span id="cb232-252"><a href="logistic-regression.html#cb232-252" aria-hidden="true" tabindex="-1"></a>      <span class="at">type=</span><span class="st">&#39;n&#39;</span>, <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">xaxt =</span> <span class="st">&#39;n&#39;</span>,</span>
<span id="cb232-253"><a href="logistic-regression.html#cb232-253" aria-hidden="true" tabindex="-1"></a>      <span class="at">ylab=</span><span class="st">&quot;Logit of P(response=</span><span class="sc">\&quot;</span><span class="st">male</span><span class="sc">\&quot;</span><span class="st">)&quot;</span>)</span>
<span id="cb232-254"><a href="logistic-regression.html#cb232-254" aria-hidden="true" tabindex="-1"></a>values <span class="ot">=</span> <span class="fu">round</span> (<span class="fu">exp</span> ( <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span><span class="fu">sd</span>(h95<span class="sc">$</span>g0)<span class="sc">+</span><span class="fu">mean</span>(h95<span class="sc">$</span>g0) ))</span>
<span id="cb232-255"><a href="logistic-regression.html#cb232-255" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span> (<span class="at">side =</span> <span class="dv">1</span>, <span class="at">at =</span> <span class="sc">-</span><span class="dv">2</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">labels =</span> values)</span>
<span id="cb232-256"><a href="logistic-regression.html#cb232-256" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">h=</span><span class="dv">0</span>,<span class="at">lwd=</span><span class="dv">1</span>)</span>
<span id="cb232-257"><a href="logistic-regression.html#cb232-257" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_males<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_males<span class="sc">$</span>padult), <span class="at">cex =</span><span class="dv">1</span>,  </span>
<span id="cb232-258"><a href="logistic-regression.html#cb232-258" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-259"><a href="logistic-regression.html#cb232-259" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_females<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_females<span class="sc">$</span>padult), <span class="at">cex=</span><span class="dv">1</span>, </span>
<span id="cb232-260"><a href="logistic-regression.html#cb232-260" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-261"><a href="logistic-regression.html#cb232-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-262"><a href="logistic-regression.html#cb232-262" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="sc">-</span><span class="fl">2.5</span>,<span class="sc">-</span><span class="fl">0.5</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Boys&quot;</span>,<span class="st">&quot;Girls&quot;</span>,<span class="st">&quot;Men&quot;</span>,<span class="st">&quot;Women&quot;</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">0</span>,</span>
<span id="cb232-263"><a href="logistic-regression.html#cb232-263" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> cols[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>], <span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pch=</span><span class="dv">1</span>,<span class="at">pt.cex=</span><span class="fl">1.5</span>)</span>
<span id="cb232-264"><a href="logistic-regression.html#cb232-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-265"><a href="logistic-regression.html#cb232-265" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-266"><a href="logistic-regression.html#cb232-266" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-267"><a href="logistic-regression.html#cb232-267" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="sc">-</span>(cffs[<span class="dv">1</span>]) <span class="sc">/</span> (cffs[<span class="dv">4</span>]); <span class="fu">abline</span> (<span class="at">v =</span> x, <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">1</span>)</span>
<span id="cb232-268"><a href="logistic-regression.html#cb232-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-269"><a href="logistic-regression.html#cb232-269" aria-hidden="true" tabindex="-1"></a>den <span class="ot">=</span> <span class="fu">density</span>(boundary)</span>
<span id="cb232-270"><a href="logistic-regression.html#cb232-270" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span> (den<span class="sc">$</span>x, den<span class="sc">$</span>y<span class="sc">/</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">1</span>)</span>
<span id="cb232-271"><a href="logistic-regression.html#cb232-271" aria-hidden="true" tabindex="-1"></a><span class="fu">polygon</span> (den<span class="sc">$</span>x, den<span class="sc">$</span>y<span class="sc">/</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">col=</span>lightpink)</span>
<span id="cb232-272"><a href="logistic-regression.html#cb232-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-273"><a href="logistic-regression.html#cb232-273" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-274"><a href="logistic-regression.html#cb232-274" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 7.9</span></span>
<span id="cb232-275"><a href="logistic-regression.html#cb232-275" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-276"><a href="logistic-regression.html#cb232-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-277"><a href="logistic-regression.html#cb232-277" aria-hidden="true" tabindex="-1"></a>cffs <span class="ot">=</span> logistic_g0_hypothesis[[<span class="dv">1</span>]][,<span class="dv">2</span>]</span>
<span id="cb232-278"><a href="logistic-regression.html#cb232-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-279"><a href="logistic-regression.html#cb232-279" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb232-280"><a href="logistic-regression.html#cb232-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-281"><a href="logistic-regression.html#cb232-281" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0_s,<span class="fu">ptoz</span>(agg_males<span class="sc">$</span>padult),<span class="at">xlab=</span><span class="st">&quot;f0 (Hz)&quot;</span>,<span class="at">type=</span><span class="st">&#39;n&#39;</span>, </span>
<span id="cb232-282"><a href="logistic-regression.html#cb232-282" aria-hidden="true" tabindex="-1"></a>      <span class="at">xlim=</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s),<span class="at">ylab=</span><span class="st">&quot;Logit of P(response=</span><span class="sc">\&quot;</span><span class="st">male</span><span class="sc">\&quot;</span><span class="st">)&quot;</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb232-283"><a href="logistic-regression.html#cb232-283" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">v=</span> <span class="dv">0</span>,<span class="at">h=</span><span class="dv">0</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-284"><a href="logistic-regression.html#cb232-284" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_males<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_males<span class="sc">$</span>padult), <span class="at">cex =</span><span class="dv">2</span>,  </span>
<span id="cb232-285"><a href="logistic-regression.html#cb232-285" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-286"><a href="logistic-regression.html#cb232-286" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_females<span class="sc">$</span>g0_s, <span class="fu">ptoz</span>(agg_females<span class="sc">$</span>padult), <span class="at">cex=</span><span class="dv">2</span>, </span>
<span id="cb232-287"><a href="logistic-regression.html#cb232-287" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-288"><a href="logistic-regression.html#cb232-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-289"><a href="logistic-regression.html#cb232-289" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="sc">-</span><span class="fl">2.5</span>,<span class="sc">-</span><span class="fl">0.5</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Boys&quot;</span>,<span class="st">&quot;Girls&quot;</span>,<span class="st">&quot;Men&quot;</span>,<span class="st">&quot;Women&quot;</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">0</span>,</span>
<span id="cb232-290"><a href="logistic-regression.html#cb232-290" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> cols[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>], <span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pch=</span><span class="dv">1</span>,<span class="at">pt.cex=</span><span class="fl">1.5</span>)</span>
<span id="cb232-291"><a href="logistic-regression.html#cb232-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-292"><a href="logistic-regression.html#cb232-292" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-293"><a href="logistic-regression.html#cb232-293" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-294"><a href="logistic-regression.html#cb232-294" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="sc">-</span>(cffs[<span class="dv">1</span>]) <span class="sc">/</span> (cffs[<span class="dv">4</span>]); <span class="fu">abline</span> (<span class="at">v =</span> x, <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-295"><a href="logistic-regression.html#cb232-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-296"><a href="logistic-regression.html#cb232-296" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">3</span>] <span class="sc">+</span> cffs[<span class="dv">6</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-297"><a href="logistic-regression.html#cb232-297" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> coral, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-298"><a href="logistic-regression.html#cb232-298" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="sc">-</span>(cffs[<span class="dv">3</span>]) <span class="sc">/</span> (cffs[<span class="dv">6</span>]); <span class="fu">abline</span> (<span class="at">v =</span> x, <span class="at">col =</span> coral, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-299"><a href="logistic-regression.html#cb232-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-300"><a href="logistic-regression.html#cb232-300" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( (cffs[<span class="dv">2</span>] <span class="sc">+</span> cffs[<span class="dv">5</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-301"><a href="logistic-regression.html#cb232-301" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> teal, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-302"><a href="logistic-regression.html#cb232-302" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="sc">-</span>(cffs[<span class="dv">2</span>]) <span class="sc">/</span> (cffs[<span class="dv">5</span>]); <span class="fu">abline</span> (<span class="at">v =</span> x, <span class="at">col =</span> teal, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-303"><a href="logistic-regression.html#cb232-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-304"><a href="logistic-regression.html#cb232-304" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-305"><a href="logistic-regression.html#cb232-305" aria-hidden="true" tabindex="-1"></a><span class="do">### Figure 7.10</span></span>
<span id="cb232-306"><a href="logistic-regression.html#cb232-306" aria-hidden="true" tabindex="-1"></a><span class="do">################################################################################</span></span>
<span id="cb232-307"><a href="logistic-regression.html#cb232-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-308"><a href="logistic-regression.html#cb232-308" aria-hidden="true" tabindex="-1"></a>cffs <span class="ot">=</span> logistic_g0_hypothesis[[<span class="dv">1</span>]][,<span class="dv">2</span>]</span>
<span id="cb232-309"><a href="logistic-regression.html#cb232-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-310"><a href="logistic-regression.html#cb232-310" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span> (<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb232-311"><a href="logistic-regression.html#cb232-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-312"><a href="logistic-regression.html#cb232-312" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span> (agg_males<span class="sc">$</span>g0_s, (agg_males<span class="sc">$</span>padult), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">xlab=</span><span class="st">&quot;f0 (Hz)&quot;</span>,<span class="at">type=</span><span class="st">&#39;n&#39;</span>,</span>
<span id="cb232-313"><a href="logistic-regression.html#cb232-313" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">xaxt =</span> <span class="st">&#39;n&#39;</span>, <span class="at">ylab =</span> <span class="st">&quot;P(response=</span><span class="sc">\&quot;</span><span class="st">male</span><span class="sc">\&quot;</span><span class="st">)&quot;</span>)</span>
<span id="cb232-314"><a href="logistic-regression.html#cb232-314" aria-hidden="true" tabindex="-1"></a>values <span class="ot">=</span> <span class="fu">round</span> (<span class="fu">exp</span> ( <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span><span class="fu">sd</span>(h95<span class="sc">$</span>g0)<span class="sc">+</span><span class="fu">mean</span>(h95<span class="sc">$</span>g0) ))</span>
<span id="cb232-315"><a href="logistic-regression.html#cb232-315" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span> (<span class="at">side =</span> <span class="dv">1</span>, <span class="at">at =</span> <span class="sc">-</span><span class="dv">2</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">labels =</span> values)</span>
<span id="cb232-316"><a href="logistic-regression.html#cb232-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-317"><a href="logistic-regression.html#cb232-317" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span> (<span class="sc">-</span><span class="fl">2.3</span>,<span class="fl">0.45</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Boys&quot;</span>,<span class="st">&quot;Girls&quot;</span>,<span class="st">&quot;Men&quot;</span>,<span class="st">&quot;Women&quot;</span>),<span class="at">lwd=</span><span class="dv">2</span>,<span class="at">lty=</span><span class="dv">0</span>,</span>
<span id="cb232-318"><a href="logistic-regression.html#cb232-318" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> cols[<span class="dv">3</span><span class="sc">:</span><span class="dv">6</span>], <span class="at">bty=</span><span class="st">&#39;n&#39;</span>,<span class="at">pch=</span><span class="dv">1</span>,<span class="at">pt.cex=</span><span class="fl">1.5</span>)</span>
<span id="cb232-319"><a href="logistic-regression.html#cb232-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-320"><a href="logistic-regression.html#cb232-320" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span> (<span class="at">v=</span> <span class="dv">0</span>,<span class="at">h=</span><span class="fl">0.5</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-321"><a href="logistic-regression.html#cb232-321" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_males<span class="sc">$</span>g0_s, (agg_males<span class="sc">$</span>padult), <span class="at">cex =</span><span class="dv">2</span>,  </span>
<span id="cb232-322"><a href="logistic-regression.html#cb232-322" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> cols[<span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-323"><a href="logistic-regression.html#cb232-323" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span> (agg_females<span class="sc">$</span>g0_s, (agg_females<span class="sc">$</span>padult), <span class="at">cex=</span><span class="dv">2</span>, </span>
<span id="cb232-324"><a href="logistic-regression.html#cb232-324" aria-hidden="true" tabindex="-1"></a>      <span class="at">col=</span>cols[<span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">6</span>)][agg_males<span class="sc">$</span>group],  <span class="at">pch=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-325"><a href="logistic-regression.html#cb232-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-326"><a href="logistic-regression.html#cb232-326" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">1</span>] <span class="sc">+</span> cffs[<span class="dv">4</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-327"><a href="logistic-regression.html#cb232-327" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-328"><a href="logistic-regression.html#cb232-328" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="sc">-</span>(cffs[<span class="dv">1</span>]) <span class="sc">/</span> (cffs[<span class="dv">4</span>]); <span class="fu">abline</span> (<span class="at">v =</span> x, <span class="at">col =</span> <span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-329"><a href="logistic-regression.html#cb232-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-330"><a href="logistic-regression.html#cb232-330" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">3</span>] <span class="sc">+</span> cffs[<span class="dv">6</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-331"><a href="logistic-regression.html#cb232-331" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> coral, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-332"><a href="logistic-regression.html#cb232-332" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="sc">-</span>(cffs[<span class="dv">3</span>]) <span class="sc">/</span> (cffs[<span class="dv">6</span>]); <span class="fu">abline</span> (<span class="at">v =</span> x, <span class="at">col =</span> coral, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb232-333"><a href="logistic-regression.html#cb232-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb232-334"><a href="logistic-regression.html#cb232-334" aria-hidden="true" tabindex="-1"></a><span class="fu">curve</span> ( <span class="fu">ztop</span>(cffs[<span class="dv">2</span>] <span class="sc">+</span> cffs[<span class="dv">5</span>]<span class="sc">*</span>x), <span class="at">xlim =</span><span class="fu">range</span> (h95<span class="sc">$</span>g0_s), <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb232-335"><a href="logistic-regression.html#cb232-335" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> teal, <span class="at">lwd=</span><span class="dv">2</span>, <span class="at">lty=</span><span class="dv">3</span>)</span>
<span id="cb232-336"><a href="logistic-regression.html#cb232-336" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="sc">-</span>(cffs[<span class="dv">2</span>]) <span class="sc">/</span> (cffs[<span class="dv">5</span>]); <span class="fu">abline</span> (<span class="at">v =</span> x, <span class="at">col =</span> teal, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>

</div>
</div>
<!-- Default Statcounter code for statsbook
https://santiagobarreda.github.io/stats-class/ -->
<script type="text/javascript">
var sc_project=12454226; 
var sc_invisible=1; 
var sc_security="a1959418"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/12454226/0/a1959418/1/"
alt="Web Analytics"></a></div></noscript>
<!-- End of Statcounter Code -->
            </section>

          </div>
        </div>
      </div>
<a href="random-slopes.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-anova-and-interpreting-complicated-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
