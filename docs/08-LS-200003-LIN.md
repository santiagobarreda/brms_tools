
# Bayesian ANOVA and interpreting complicated models

At this point we have enough model components to build very large and complicated models. Traditionally, models with many predictors have had three general problems:

  1) A model may return spurious values for the 'extra' predictors, leading to incorrect conclusions.
  2) The model may not fit/converge, meaning you can't get the model coefficients. 
  3) It can be difficult to interpret a model with *hundreds* of parameters.

In this chapter we're going to cover a Bayesian approach to working with large models. We're going to discuss how working with multilevel Bayesian models can naturally help us with problems (1) and (2) above, and we're going to discuss an easy way to approach the third problem also. 

## Data and research questions

We're going to change course slightly and focus on entirely new data, called `kb07`. The data is from this paper: 

  * [Kronmüller, E., & Barr, D. J. (2007). Perspective-free pragmatics: Broken precedents and the recovery-from-preemption hypothesis. Journal of Memory and Language, 56, 436--455.](https://github.com/dalejbarr/kronmueller-barr-2007)

This chapter will focus on the comparison of analyses for the `kb07` data presented in this paper:

  * [Bates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious mixed models. arXiv preprint arXiv:1506.04967](https://arxiv.org/abs/1506.04967)

The above paper discusses what kinds of random effects structures to include in our models. The paper focuses on use of the `lmer` function. I haven't discussed `lmer` very much, although it is extremely popular and extremely useful. Think of it like this: `lmer` is basically a 'special case' of `brm`. There is an equivalent `brm` model for every `lmer` model but not vice versa. However, generally speaking, `brm` and `lmer` should provide the same answers when fitting the same models. 

One major difference between `lmer` and `brms` is in the way the `lmer` estimates parameter values. Rather than provide a set of samples, `lmer` returns *point estimates* representing the *best* values of parameters. This can cause a problem when parameters are bounded, or grow without bound. For example, standard deviation parameters (e.g., $\sigma$) cannot be 0 or negative, but sometimes they are very, very small. So, when `lmer` tries to find the 'best' value, it can get closer, and closer, and closer to zero, leading to problems involving calculations with very small values (e.g., as $x$ approaches $0$, $1/x$ approaches $\infty$). 

In contrast, Bayesian models don't try to find the single 'best' value but instead collect a series of samples. As a result, the models are less likely to encounter problems when they try to estimate values that are very close to boundaries (they just 'bounce' of the boundary as they randomly walk!).

A second difference between the `lmer` estimation method and that of our Bayesian multilevel models is that `lmer` doesn't use prior probabilities for all of its parameters. This can cause some problems when estimating a large number of parameters without enough data. In contrast, `brms` applies 'shrinkage' to all its parameters (at least in principle), which can help avoid some of the problems encountered by `lmer`. 

Just to be clear, I hope to compare in this chapter is just two different estimation/fitting methods, rather than two different *philosophies*. The models fit by `lmer` and `brms` are much more similar than they are different. This chapter will simply serve to highlight how using Bayesian multilevel models can help us resolve some of the difficult issues that can present when we fit complicated models using approaches more similar to `lmer`, in addition to discussion how we can interpret models with large numbers of parameters.

### The experiment

We first load the `lme4` and `brms` packages, and the data. 


```r
library (lme4)
library (brms)
options (contrasts = c("contr.sum","cont.sum"))

url1 = "https://raw.githubusercontent.com/santiagobarreda"
# source colors
devtools::source_url (paste0 (url1, "/stats-class/master/data/colors.R"))
# source functions
devtools::source_url (paste0 (url1, "/stats-class/master/data/functions.R"))
```

The data comes from an experiment by Kronmüller and Barr (2007). The version of the data I'm using [is hosted here](https://github.com/dmbates/RePsychLing/tree/master/data). You need to download it, place it in your working directory, and then load it:


```r
load ("kb07_data.RDA")
```



Kronmüller and Barr ran an experiment where subjects had to select one of several visual objects on a monitor, with a cursor. 

Here's information about the experimental structure:

  * The dependent variable is reaction times measures in milliseconds (`RTtrunc`). 
  
  * 56 subjects responded to 32 items (n = 1790, 2 missing observations).
  
The experiment involved several manipulations. These are our *predictors*, because they are the things the authors did in order to affect reaction times. The manipulations were:

  * Speaker ($S$): Instructions presented by a new or old speaker (a factor with two levels: new and old).
  
  * Precedent ($P$): Instructions maintained or broke a precedent of reference (a factor with two levels: maintained and broke).
  
  * Cognitive Load ($C$): the task had to be performed without or with a cognitive load (a factor with two levels: with and without).
  
The experimental conditions were *crossed*. This means that all possible combinations were presented. Since we have 3 factors with two levels each we can call this a 2 x 2 x 2, crossed design. A 2x2x2 crossed design results in 8 (2x2x2=8) unique conditions (combination of factor levels). 

Since we're using sum coding, remember that we can only calculate 1 parameter for every 2-level factor. A model that included all main effects and interactions for our three factors would include the following terms:

  * Main effects: $Intercept, S, P, C$.
  
  * 2-way interactions: $SP, SC, PC$. 
  
  * 3-way interactions: $SPC$.   

Our model requires 8 parameters to represent the 8 combinations of the three factors. By using these predictors ($Intercept, S, P, C, SP, SC, PC, SPC$) rather than modeling group means directly, our model is able to estimate the independent effect of each predictor (main effects), in addition to any interaction effects. 

The model also includes three ‘random’ sources of variation: 

  * $Subject$: the individual subject/listener in the experiment.
  
  * $Item$: the item presented to the subject on the trial.
  
  * $Error$: the random trial to trial variation (i.e., $\sigma_{error}$).

## Deciding on a model

The authors manipulated listening conditions for a task and observed variation in reaction times as a result of the manipulations. The main goal of this experiment is to see what does (and does *not*) affect reaction times for this task. 

The most complicated possible answer to this question is that **everything** matters. You have unpredictable variation across all groups, meaning you need all eight fixed parameters (i.e., Int., S, P, C, SP, SC, PC, SPC) to distinguish the groups. Furthermore, the way that these main effects and interactions affect reaction times also varies *unpredictably* from subject to subject. And on top of all that, these effects also vary substantially and unpredictably from item to item. 

The statement above represents what's sometimes called the *maximal* model, the model containing all possible random slopes and intercepts. The *maximal* model for this experiment is:

```
RTtrunc ~  1 + S + P + C + SP + SC + PC + SPC + 
         ( 1 + S + P + C + SP + SC + PC + SPC | subj) + 
         ( 1 + S + P + C + SP + SC + PC + SPC | item)
```

This model estimates speaker and item-dependent effects for all predictors, basically a *lot* of `effect:subject` and `effect:item` interactions. We could compare this to a 'minimal' model so small it fits on one line:

```
RTtrunc ~  1 + S + P + C + SP + SC + PC + SPC + ( 1 | subj) + ( 1 | item)
```

This model does not contain *any* random effects other than intercepts. This means this model acts as if we do not have any systematic/substantial variation in our predictors by item or subject (random or otherwise). We could take this even further and make the model like this:

```
RTtrunc ~  1 + S + P + C + ( 1 | subj) + ( 1 | item)
```

Which basically says we expect only main effects and no interactions. We may know instinctively that it's a bad idea to start with the last model above. If you assume almost nothing matters how can you find out if it does? But is the opposite necessary, it is necessary to always use the maximal model?

There are at least two reasons why people may not use the maximal model as their final model:

  1) It may not be practical. These models can take a long time to fit since they are complicated. For example, since the random effects are treated as draws from a multivariate normal distribution (as discussed in chapter 6), we need to calculate correlations between all dimensions. This means calculating 8 standard deviations and 28 (!) correlations for the random slopes for subject, and then doing the same for the random slopes for item. When many of these standard deviations or correlations are near zero and/or there is not very much data, `lmer` can have problems converging on a solution. 
  
  2) It may not be necessary. If some predictors really have no effect on our outcomes, and especially when these interfere with our ability to fit or interpret our models, it may not be necessary to leave them in the model. 
  
In Bates et al., the authors outline a way to select a final model based on an initial 'maximal' model. The authors propose an:

> “iterative method that reduces model complexity to arrive at an optimal [linear mixed model] for this experiment. […] We qualify this procedure at the outset: We do not claim that this is the only way to proceed, but the strategy has consistently yielded satisfactory results for all data sets we have examined so far” p. 8. 

Notice that the authors are clear that this is not the best or only approach, but simply *an* approach, albeit one that is based on sound reasoning and with a proven track record. I'm not going to discuss the process outlined by the authors in detail. However, the general strategy can be summarized as:

  1) Fit the maximal model.
   
  2) Investigate which random effects likely don't matter. This is done based on which ones have very small variances and/or a small amount of independent variation between the dimensions of the random effects. 
  
  3) Fit a smaller model without the unimportant random effects. 

In this chapter, I'm going to present a slightly different, but largely analogous approach to dealing with large models, as described in this paper:

[Gelman, A. (2005). Analysis of variance—why it is more important than ever. Annals of statistics, 33(1), 1-53.](https://projecteuclid.org/euclid.aos/1112967698)

And in Chapter 22 of this book:

[Gelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge university press.](http://www.stat.columbia.edu/~gelman/arm/)

Basically, Gelman and colleagues suggest that we inspect the variance components (the $\sigma$ terms) of our multilevel models in order to figure out what does and doesn't matter. This is analogous to step 2 in the process outlined above. Once you have established which parameters are not a source of variance in your data, you can re-fit a smaller model, or you can sometimes just keep your maximal model. 

## Fitting the models

### The 'maximal' models

First we fit the maximal `brm` model


```r
# Fit the model yourself, or
# download pre-fit model from: 
# github.com/santiagobarreda/stats-class/tree/master/models
# and load after placing in working directory
# maximal_brms = readRDS ('8_maximal_brms.RDS')

set.seed (1)
maximal_brms =
  brm (RTtrunc ~ 1+S+P+C+SP+SC+PC+SPC + (1+S+P+C+SP+SC+PC+SPC|subj) +
                                        (1+S+P+C+SP+SC+PC+SPC|item), 
       data = kb07,  chains=4, cores=4, warmup=1000, iter = 3500, thin = 2,
       prior = c(set_prior("student_t(3, 2000, 500)", class = "Intercept"),
                 set_prior("student_t(3, 0, 500)", class = "b"),
                 set_prior("student_t(3, 0, 500)", class = "sd"),
                 set_prior("lkj_corr_cholesky (2)", class = "cor")))

# save model
#  saveRDS (maximal_brms, '8_maximal_brms.RDS')
```


And then the analogous, maximal `lmer` model. I got the parameter settings used for the model from [this webpage](https://github.com/dmbates/RePsychLing/blob/master/vignettes/KB.Rmd). 


```r
# Fit the model yourself, or
# download pre-fit model from: 
# github.com/santiagobarreda/stats-class/tree/master/models
# and load after placing in working directory
# maximal_lmer = readRDS ('8_maximal_lmer.RDS')

maximal_lmer <- lmer(RTtrunc ~ 1+S+P+C+SP+SC+PC+SPC + 
                                    (1+S+P+C+SP+SC+PC+SPC|subj) +
                                    (1+S+P+C+SP+SC+PC+SPC|item), 
                           kb07, REML=FALSE, start=thcvg$kb07$m0,
                           control=lmerControl(optimizer="Nelder_Mead",
                                               optCtrl=list(maxfun=50000L),
                                               check.conv.grad="ignore",
                                               check.conv.hess="ignore"),
                           verbose=TRUE)
# save model
#  saveRDS (maximal_lmer, '8_maximal_lmer.RDS')
```


### Fitting the 'final' models in lmer and brms

We can also fit the 'final' model as determined by Bates et al. using the selection process proposed by the authors. This model includes onle random by-subject intercepts, and random intercepts and $P$ effects for item. First, we fit the final model in `brms`:


```r
set.seed (1)
final_brms =
  brm (RTtrunc ~ 1+S+P+C+SP+SC+PC+SPC + (1|subj) + (1+P|item), 
       data = kb07,  chains=4, cores=4, warmup=1000, iter = 3500, thin = 2,
       prior = c(set_prior("student_t(3, 2000, 500)", class = "Intercept"),
                 set_prior("student_t(3, 0, 500)", class = "b"),
                 set_prior("student_t(3, 0, 500)", class = "sd"),
                 set_prior("lkj_corr_cholesky (2)", class = "cor")))

# save model
#saveRDS (final_brms, 'final_brms.RDS')
```


And then fit the final model in `lmer`:


```r
final_lmer <- lmer(RTtrunc ~ 1+S+P+C+SP+SC+PC+SPC +
                     (1|subj) + (1+P|item), kb07, REML=FALSE, 
                   control=lmerControl(optimizer="Nelder_Mead",
                                       optCtrl=list(maxfun=50000L),
                                       check.conv.grad="ignore",
                                       check.conv.hess="ignore"),
                   verbose=TRUE)

# save model
# saveRDS (final_lmer, 'final_lmer.RDS')
```


## Comparing the maximal and final models



Bates et al. compare the maximal and final models fit in `lmer` with the same models fit using STAN (the language `brms` uses to fit its models). We're going to make the same comparisons that Bates et al. make, and recreate some of the figures in their paper (with some modifications). Instead of using STAN directly, we're going to fit models in `brms` and compare these to the equivalent models fit in `lmer`.

In Figure \@ref(fig:F8-1) you can see a recreation of Figure 1 from Bates et al., which compares fixed-effect parameter estimates and intervals for the different models. Clearly, the four models provide essentially the same output for these predictors. That's reassuring! If results changed dramatically as a result of our model selection, we would need to think very carefully about the different models we're considering and what might have caused the changes.

<div class="figure">
<img src="08_files/figure-html/F8-1-1.png" alt="Comparison of means and confidence/credible intervals for 'fixed-effects' parameter estimates by the four models being considered." width="768" />
<p class="caption">(\#fig:F8-1)Comparison of means and confidence/credible intervals for 'fixed-effects' parameter estimates by the four models being considered.</p>
</div>

In Figure \@ref(fig:F8-2), I recreate Figure 2 from Bates et al., which compares the standard deviations estimates for the random effects included in the different models. It's worth stopping to think about what these standard deviations are. Recall that random effects like subject-specific random intercepts are modeled as coming from a normal distribution with a mean of 0 and a standard deviation based on the data, like this:

$$
\alpha_{[subj]} \sim \mathrm{Normal}(0, \sigma_{\alpha_{[subj]}})
(\#eq:801)
$$

So, the $\sigma_{\alpha_{[subj]}}$ reflects the variation in the subject-specific intercepts in the data. Similarly, we draw the subject-specific effects for $P$ (define) from a normal distribution with a standard deviation of $\sigma_{\alpha_{[subj]}}$. 

$$
\alpha_{P[subj]} \sim \mathrm{Normal}(0, \sigma_{\alpha_{P[subj]}})
(\#eq:802)
$$

These standard deviations directly reflect the amount of variation in these parameters in our model. Parameters that vary a lot reflect large effects on our data. These will be represented by large standard deviations. In contrast, parameters that do not vary much will be represented by small standard deviations do not have a large effect on our data (since they do not vary much).

So, the individual values of $\sigma_{\alpha_{[subj]}}$ and $\sigma_{\alpha_{P[subj]}}$ in our model will tell us whether the subject intercepts and the subject-specific effect for $P$ are important sources of variation in our model. We can take one look at Figure \@ref(fig:F8-2) and tell that there are really four dominant sources of variance in our random effects: the item-specific intercepts and effects for $P$, the subject-specific intercepts, and the random error ('sigma'). 

<div class="figure">
<img src="08_files/figure-html/F8-2-1.png" alt="Comparison of random effects standard deviation estimates for the frou models being considered." width="768" />
<p class="caption">(\#fig:F8-2)Comparison of random effects standard deviation estimates for the frou models being considered.</p>
</div>

As you can see, the models all provide very similar standard deviation estimates. Note that the `brm` models provide credible intervals for all parameters, while the `lmer` models only provide point estimates for these parameters. 

The lack of intervals on parameter estimates makes it difficult to 'rule out' parameters since they will *never* equal exactly zero. So, we will always have non-zero numbers for these parameters, and 'secretly' some of these are zero or nearly zero. 

We can use our credible intervals to figure out which variance components are unlikely to matter: Variance components whose credible intervals are concentrated near zero. There are several such components in the Figure below. In the *best case*, many of these components reflect a tiny amount of systematic variation in our outcomes. 

In Figure \@ref(fig:F8-3) we compare the correlations for the subject random effects, recreating Figure 3 from Bates et al. These are the correlations between the dimensions of the 8-dimensional variable representing the subject random effects in the maximal model. As I noted earlier, there are 28 of these correlations. Note that the plot below only contains information from the maximal model because the final models included only a single subject random effect (subject random intercepts), and so did not involve correlations. 

As with the standard deviations, `brms` gives us intervals for these parameters while `lmer` returns point estimates. We see that the `lmer` estimates vary substantially around 0 while the `brms` estimates are all close to zero, and have intervals that include zero. Here we see two advantages of our Bayesian model: 

  1) Credible intervals for all parameters let us accept values of zero as likely values for the correlations (this can't be done with point estimates).
  
  2) The prior on the covariance matrix pulls weakly-supported correlations to zero, thereby protecting against spurious results. 
  
Thus, we can actually use the results below to conclude that the most likely value for these correlations is zero or something close to zero. 
  
<div class="figure">
<img src="08_files/figure-html/F8-3-1.png" alt="Compraison of subject random effect correlation estimates for brm (green) and lmer (red)." width="768" />
<p class="caption">(\#fig:F8-3)Compraison of subject random effect correlation estimates for brm (green) and lmer (red).</p>
</div>

Figure \@ref(fig:F8-4) shows the correlations between the dimensions of the item random effects (a recreation of the other half of Figure 3 in Bates et al.). Recall that that final model included by-item intercepts and by-item effects for $P$, and so involved a single correlation parameter (Int:P in the figure below). 

Notice that the `lmer` estimates for the maximal model all vary around zero. There is nothing qualitatively different about the correlation that is found to actually be 'real' as opposed to the other ones. In fact, the value of the Int:P correlation is about the same magnitude across both `lmer` models.  

The fact that the correlation mainted in the final model looks about the same as the ones that are omitted from the final model is a problem for us, when we rely on `lmer`. In the absence of intervals, it is difficult to judge parameters based on magnitudes alone. The method outlined in Bates et al. is a proposed solution to this problem. 

However, note that our Bayesian models *do* let us distinguish the correlations that matter from the ones that don't. Even in the maximal model, the correlation that is included in the final model *is* qualitatively different from the others: its credible interval is substantially narrower and does not include 0. Furthermore, the correlations that do *not* represent reliable variation in the data are pulled toward zero, allowing us to establish that these are likely not important to our understanding of the data. 

So, in effect, our maximal Bayesian model give us the same information as the final model, without having to ever refit the model. It allows us to rule out the same variance components, and provides the same parameter estimates and intervals for the fixed effects. 

<div class="figure">
<img src="08_files/figure-html/F8-4-1.png" alt="Compraison of item random effect correlation estimates for the maximal brm (green) and lmer (red) models, and the final brm (blue)." width="768" />
<p class="caption">(\#fig:F8-4)Compraison of item random effect correlation estimates for the maximal brm (green) and lmer (red) models, and the final brm (blue).</p>
</div>

Bates et al. conclude that:

> “The Bayesian analysis shows two important things. First, the estimates of the fixed effects in the lme4 model and the Bayesian model are nearly identical. This shows that the ‘maximal’ lmm fit using lme4 is essentially equivalent to fitting a Bayesian lmm with regularizing priors of the sort described above.” (p. 13) 

and that:

> “Second, the relevant variance component parameters that were identified above using principal components analysis (pca) and likelihood ratio tests (lrts) are exactly the parameters that clearly dominate in the Bayesian analysis.” (p. 13) 

As a result of this general equivalence, fitting a maximal model using `brms` and focusing on the relative magnitudes of individual variance components can be a way to do model selection without necessarily 'selecting' your model. 


## Bayesian Analysis of Variance

Here's what Gelman and Hill have to say about the analysis of variance using multilevel models:

> "When moving to multilevel modeling, the key idea we want to take from the analysis of variance is the estimation of the importance of different batches of predictors (“components of variation” in ANOVA terminology). As usual, we focus on estimation rather than testing: instead of testing the null hypothesis that a variance component is zero, we estimate the standard deviation of the corresponding batch of coefficients. If this standard deviation is estimated to be small, then the source of variation is minor—we do not worry about whether it is exactly zero. In the social science and public health examples that we focus on, it can be a useful research goal to identify important sources of variation, but it is rare that anything is truly zero." (p. 490)

> "the standard deviation of a set of coefficients gives a sense of their predictive importance in the model. An analysis-of-variance plot, which shows the relative scale of different variance components, can be a useful tool in understanding a model." (p. 492)

Gelman and Hill distinguish between two types of standard deviations for a random effect with $J$ levels (p. 459):

> "• The superpopulation standard deviation [$\sigma_{\alpha_{[subj]}}$], which represents the variation among the modeled probability distribution from which the [$\alpha_{[subj]}$] were drawn, is relevant for determining the uncertainty about the value of a new group not in the original set of J." 

> "• The finite-population standard deviation [$\mathrm{sd}(\alpha_{[subj]})$] of the particular J values [$\alpha_{[subj]}$] describes variation within the existing data." 

Earlier, the authors note that (p. 464):

> "The superpopulation and finite-population standard deviations are not two different statistical “estimators” of a common quantity; rather, they are two different quantities that can both be estimated from the multilevel model. We can get a point estimate and uncertainty intervals for both. In general, the point estimates of σ and s will be similar to each other, but s will have less uncertainty than σ. That is, the variation is more precisely estimated for the finite population than the superpopulation. This makes sense because we have more information about the units we observe than the full population from which they are sampled."



We can extract the superpopulation standard deviations for `subj and` `item` using the `VarCorr` function and the code below. The first bit 



<div class="figure">
<img src="08_files/figure-html/F8-5-1.png" alt="TBD" width="768" />
<p class="caption">(\#fig:F8-5)TBD</p>
</div>













<div class="figure">
<img src="08_files/figure-html/F8-6-1.png" alt="TBD" width="768" />
<p class="caption">(\#fig:F8-6)TBD</p>
</div>


## Applying a BANOVA to our height perception models
  
text


## Plot Code




```r
################################################################################
### Figure 8.1
################################################################################

par (mfrow = c(1,1), mar = c(4,4,1,1))

shift = -.3
brmplot (xs = (1:7)+shift, fe_f[8:2,], horizontal = TRUE, col = deepgreen,
         xlim = c(.5,7.5), xlab = 'Predictors', ylab = 'RT (msec)',labels="")
abline (h = 0, lty=3)

shift = -.15
points ((7:1)+shift, sumf[[10]][2:8,1], col=coral,lwd=2,cex=1.5,pch=0)
arrows ((7:1)+shift, sumf[[10]][2:8,1]+sumf[[10]][2:8,2]*2,(7:1)+shift, 
        sumf[[10]][2:8,1]-sumf[[10]][2:8,2]*2, lwd=2,col=coral,length=0)

shift = +.0
brmplot (xs = (1:7)+shift, fe_m[8:2,], horizontal=TRUE, add=TRUE, 
         col=skyblue)
abline (h = 0, lty=1)

shift = +.15
points ((7:1)+shift, summ[[10]][2:8,1], pch=0, col=teal,lwd=2,cex=1.5)
arrows ((7:1)+shift, summ[[10]][2:8,1]+summ[[10]][2:8,2]*2, (7:1)+shift,pch=0, 
        summ[[10]][2:8,1]-summ[[10]][2:8,2]*2, lwd=2,col=teal,length=0)

legend (1, -150, bty='n', pch=c(16,0,16,0), pt.cex=1.5,
        col=c(deepgreen,coral,skyblue,teal),lwd=2, 
        legend = c("Maximal brm","Maximal lmer","Final brm","Final lmer"))

################################################################################
### Figure 8.2
################################################################################

par (mfrow = c(1,1), mar = c(7,4,1,1))

labels = paste0 (c("",rep("Item:",8),rep("Subj:",8)), rownames(vars_m))

brmplot (xs = (1:17), vars_m, horizontal = TRUE, col = deepgreen, xlab = '', 
         yaxs='i', ylab='RT (msec)',ylim = c(0,750),las=2, labels = labels)
grid()
points ((1:17), lmer_vars_m, pch=0, col=coral,lwd=3,cex=2)
points (c(1,7,9,17)+.3, lmer_vars_f, pch=0, col=teal,lwd=3,cex=2)

brmplot (xs = c(1,7,9,17)+.3, vars_final, horizontal = TRUE, col = skyblue, 
         add = TRUE)
abline (h=0)
legend (11, 600, bty='n', pch=c(16,0,16,0), pt.cex=1.5,
        col=c(deepgreen,coral,skyblue,teal),pt.lwd=2, 
        legend = c("Maximal brm","Maximal lmer","Final brm","Final lmer"))

################################################################################
### Figure 8.3
################################################################################

subjrhom = attr(VarCorr(maximal_lmer)[[1]],'correlation')

par (mfrow = c(1,1), mar = c(5,5.5,3,1))

corsuse = c(2:8,(3:8)+8,(4:8)+16,(5:8)+24,(6:8)+32,(7:8)+40,(8)+48)
blabels = c("SPC","PC","SC","SP","C","P","S","Int") 
labs = expand.grid(blabels,blabels)[corsuse,]
labs = paste(labs[,1],labs[,2],sep=" : ")

corrs_ms = VarCorr(maximal_brms)$subj$cor
corrs_ms = rbind (corrs_ms[7:1,,8], corrs_ms[6:1,,7], corrs_ms[5:1,,6],
               corrs_ms[4:1,,5], corrs_ms[3:1,,4], corrs_ms[2:1,,3], 
               corrs_ms[1,,2])

brmplot (corrs_ms, ylim = c(-1,1), main = "Subject Correlation Estimates",
         yaxs="i", cex.main=1, las=2, ylab = "Correlation", labels = labs,
         col=deepgreen)
abline (h = 0, lty = 3)
points (28:1, subjrhom[corsuse], pch=0, col=coral,lwd=2,cex=1.5)


################################################################################
### Figure 8.4
################################################################################

brmplot (corrs_mi, ylim = c(-1,1), main = "Subject Correlation Estimates",
         yaxs="i", las=2, labels = labs, col = deepgreen)
abline (h = 0, lty = 3)
points (28:1, itemrhom[corsuse], pch=0, col=coral,lwd=2,cex=1.5)

points (27+.4,corrs_fi[1], cex = 1.5, col = skyblue, pch=16)
segments (27+.4, corrs_fi[3], 27+.4, corrs_fi[4], lwd=2, col=skyblue)
points (27+.4, itemrhof[2], col=teal,lwd=3, pch = 0, cex = 1.5)

################################################################################
### Figure 8.5
################################################################################

par (mfrow = c(1,2), mar = c(5,4,3,1))
brmplot (subj_ranefs_is, yaxs='i', ylim = c(0,550),col=lavender,
         main="Fixed Effects", cex.main=1.2,las=2)
brmplot (item_ranefs_is, yaxs='i', ylim = c(0,550),col=skyblue,
         main="Fixed Effects", cex.main=1.2,las=2)

################################################################################
### Figure 8.6
################################################################################

par (mfrow = c(1,3), mar = c(4,1,3,0), oma = c(4,5,0,1))
brmplot (xs = (8:1)-0.1, fixefs_fs[-2,], yaxs = 'i', ylim = c(0,880),
         main="Fixed Effects", cex.main=1.2,cex.axis=1.3,cex.lab=1.3,las=2)
grid()
mtext (side = 2, outer = TRUE, text = "RT (ms)", line=3)
brmplot (xs = (8:1)+0.1, item_ranefs_fs, col = 2, yaxs = 'i', ylim = c(0,880),
         main="Item Random Effects", cex.main=1.2,yaxt="n",cex.axis=1.3,las=2)
grid()
brmplot (xs = (8:1)+0.1, subj_ranefs_fs, col = 4, yaxs = 'i', ylim = c(0,880),
         main="Subject Random Effects",cex.main=1.2,yaxt="n",cex.axis=1.3,las=2)
grid()
```







In the example above, there is no practical difference between the results for the maximal and final `brm` models. However, there may be a practical difference for your data. Some things. to think about:

  * If a model is dominated by meaningless predictors or if you encounter problems with fitting or interpretation of a model because it is too large, then it may be useful to think about which predictors might need to be omitted. 
  
  * Some 'maximal' models may be too complicated to be supported by the data available.
  
  * There may not be a need to 'overcomplicated' the model. If some effects are not interesting or if there are just too many 


Further, we arrive at the model by removing the variance components that we estimate to be zero. However, we must estimate them in order to know that they are near zero, and then we must explain this to the reader. We can relay largely the same information by presenting a "Bayesian Analysis of Variance" as recommended by Gelman, and focusing on the relative magnitude of different variance components.


However, keeping the larger model can let you have your cake and eat it too. It lets you use the same single model to: 1) Say what is important to understand your data, and 2) Say what is *not* important in understanding your data. 
